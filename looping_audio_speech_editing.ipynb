{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for GUI of speech editing of audio files (with looping options for convenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from streamreader_audio_only import stream\n",
    "import os\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "\n",
    "CURR_DIR = os.getcwd()\n",
    "\n",
    "from inference.tts.spec_denoiser import SpecDenoiserInfer\n",
    "from utils.commons.hparams import set_hparams\n",
    "import torchaudio\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import pyaudio\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from g2p_en.expand import normalize_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .wav file should have a samplerate of 16000 and be mono. Use the following function if necessary to put your .wav file in the correct format and save it to your specified output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "def correct_format_for_wav(input_wav_path, output_path):\n",
    "    converted_audio = AudioSegment.from_wav(input_wav_path)\n",
    "    converted_audio = converted_audio.set_frame_rate(16000).set_channels(1)\n",
    "    converted_audio.export(output_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user needs to set up these\n",
    "SAVE_AUDIO_DIR = CURR_DIR + \"\\\\saved_audio\"\n",
    "if not os.path.exists(SAVE_AUDIO_DIR):\n",
    "    os.mkdir(SAVE_AUDIO_DIR)\n",
    "# Set this to true if you want to save all segments of audio and not just the flagged ones\n",
    "SAVE_ALL_AUDIO = False\n",
    "binary_data_directory = \".\\\\data\\\\processed\\\\binary\\\\libritts\"\n",
    "Espeak_dll_directory = \"C:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "\n",
    "EspeakWrapper.set_library(Espeak_dll_directory)\n",
    "whisperX_phoneme_model_directory = \"..\\\\whisperX-main\\\\facebook\"\n",
    "whisperX_transcription_model_directory = None\n",
    "data_queue = queue.Queue()\n",
    "our_model_ckpt_path='checkpoints/spec_denoiser/model_ckpt_steps_568000.ckpt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some more hyperparameters the user may want to set. Be careful with most of these\n",
    "silero_sensitivity = 0.2  # higher = more likely to detect silence\n",
    "req_num_pauses = 2\n",
    "min_segs_to_keep = 1\n",
    "req_end_long = True\n",
    "# not sure if these are correct (other than sample_rate)\n",
    "sample_rate = 16000\n",
    "hop_length = 160\n",
    "segment_length = 30\n",
    "segment_length = segment_length * hop_length * 4\n",
    "SILERO_MIN_LENGTH_LONG_SILENCE = 200\n",
    "# force the audio to go through even if not enough silences are formed if more than this many chunks are appended\n",
    "MAX_ALLOWED_CHUNKS = 3\n",
    "# the max amount of time you expect it to take to transcribe and run inference\n",
    "processing_buffer = 1.5\n",
    "\n",
    "# The data acquisition process will stop after this number of steps.\n",
    "# This eliminates the need of process synchronization and makes this\n",
    "# tutorial simple.\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot mel with alignment tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/audio/main/generated/torchaudio.pipelines.Wav2Vec2ASRBundle.html\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "\n",
    "model_trellis = bundle.get_model().to(DEVICE)\n",
    "labels_trellis = bundle.get_labels()\n",
    "dictionary = {c: i for i, c in enumerate(labels_trellis)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel_with_align(waveform, transcript, output_path):\n",
    "\n",
    "    # Edit transcript into correct format\n",
    "    transcript = normalize_numbers(transcript)\n",
    "    transcript = transcript.upper()\n",
    "    for punc in [\".\", \"?\", \"!\", \"...\", \",\", \":\", \";\"]:\n",
    "        transcript = transcript.replace(punc, \"\")\n",
    "    transcript = transcript.replace(\" \", \"|\")\n",
    "    if not transcript.startswith(\"|\"):\n",
    "        transcript = \"|\" + transcript\n",
    "    if not transcript.endswith(\"|\"):\n",
    "        transcript = transcript + \"|\"\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        emissions, _ = model_trellis(waveform.to(DEVICE))\n",
    "        emissions = torch.log_softmax(emissions, dim=-1)\n",
    "\n",
    "    emission = emissions[0].cpu().detach()\n",
    "\n",
    "    tokens = [dictionary.get(c, 0) for c in transcript]\n",
    "\n",
    "    def get_trellis(emission, tokens, blank_id=0):\n",
    "        num_frame = emission.size(0)\n",
    "        num_tokens = len(tokens)\n",
    "\n",
    "        trellis = torch.zeros((num_frame, num_tokens))\n",
    "        trellis[1:, 0] = torch.cumsum(emission[1:, blank_id], 0)\n",
    "        trellis[0, 1:] = -float(\"inf\")\n",
    "        trellis[-num_tokens + 1 :, 0] = float(\"inf\")\n",
    "\n",
    "        for t in range(num_frame - 1):\n",
    "            trellis[t + 1, 1:] = torch.maximum(\n",
    "                # Score for staying at the same token\n",
    "                trellis[t, 1:] + emission[t, blank_id],\n",
    "                # Score for changing to the next token\n",
    "                trellis[t, :-1] + emission[t, tokens[1:]],\n",
    "            )\n",
    "        return trellis\n",
    "\n",
    "    trellis = get_trellis(emission, tokens)\n",
    "\n",
    "    @dataclass\n",
    "    class Point:\n",
    "        token_index: int\n",
    "        time_index: int\n",
    "        score: float\n",
    "\n",
    "    def backtrack(trellis, emission, tokens, blank_id=0):\n",
    "        t, j = trellis.size(0) - 1, trellis.size(1) - 1\n",
    "\n",
    "        path = [Point(j, t, emission[t, blank_id].exp().item())]\n",
    "        while j > 0:\n",
    "            # Should not happen but just in case\n",
    "            assert t > 0\n",
    "\n",
    "            # 1. Figure out if the current position was stay or change\n",
    "            # Frame-wise score of stay vs change\n",
    "            p_stay = emission[t - 1, blank_id]\n",
    "            p_change = emission[t - 1, tokens[j]]\n",
    "\n",
    "            # Context-aware score for stay vs change\n",
    "            stayed = trellis[t - 1, j] + p_stay\n",
    "            changed = trellis[t - 1, j - 1] + p_change\n",
    "\n",
    "            # Update position\n",
    "            t -= 1\n",
    "            if changed > stayed:\n",
    "                j -= 1\n",
    "\n",
    "            # Store the path with frame-wise probability.\n",
    "            prob = (p_change if changed > stayed else p_stay).exp().item()\n",
    "            path.append(Point(j, t, prob))\n",
    "        # Now j == 0, which means, it reached the SoS.\n",
    "        # Fill up the rest for the sake of visualization\n",
    "        while t > 0:\n",
    "            prob = emission[t - 1, blank_id].exp().item()\n",
    "            path.append(Point(j, t - 1, prob))\n",
    "            t -= 1\n",
    "\n",
    "        return path[::-1]\n",
    "\n",
    "    path = backtrack(trellis, emission, tokens)\n",
    "\n",
    "    # Merge the labels\n",
    "    @dataclass\n",
    "    class Segment:\n",
    "        label: str\n",
    "        start: int\n",
    "        end: int\n",
    "        score: float\n",
    "\n",
    "        def __repr__(self):\n",
    "            return (\n",
    "                f\"{self.label}\\t({self.score:4.2f}): [{self.start:5d}, {self.end:5d})\"\n",
    "            )\n",
    "\n",
    "        @property\n",
    "        def length(self):\n",
    "            return self.end - self.start\n",
    "\n",
    "    def merge_repeats(path):\n",
    "        i1, i2 = 0, 0\n",
    "        segments = []\n",
    "        while i1 < len(path):\n",
    "            while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "                i2 += 1\n",
    "            score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
    "            segments.append(\n",
    "                Segment(\n",
    "                    transcript[path[i1].token_index],\n",
    "                    path[i1].time_index,\n",
    "                    path[i2 - 1].time_index + 1,\n",
    "                    score,\n",
    "                )\n",
    "            )\n",
    "            i1 = i2\n",
    "        return segments\n",
    "\n",
    "    segments = merge_repeats(path)\n",
    "\n",
    "    # Merge words\n",
    "    def merge_words(segments, separator=\"|\"):\n",
    "        words = []\n",
    "        i1, i2 = 0, 0\n",
    "        while i1 < len(segments):\n",
    "            if i2 >= len(segments) or segments[i2].label == separator:\n",
    "                if i1 != i2:\n",
    "                    segs = segments[i1:i2]\n",
    "                    word = \"\".join([seg.label for seg in segs])\n",
    "                    score = sum(seg.score * seg.length for seg in segs) / sum(\n",
    "                        seg.length for seg in segs\n",
    "                    )\n",
    "                    words.append(\n",
    "                        Segment(word, segments[i1].start, segments[i2 - 1].end, score)\n",
    "                    )\n",
    "                i1 = i2 + 1\n",
    "                i2 = i1\n",
    "            else:\n",
    "                i2 += 1\n",
    "        return words\n",
    "\n",
    "    word_segments = merge_words(segments)\n",
    "\n",
    "    def plot_alignments(\n",
    "        trellis, segments, word_segments, waveform, sample_rate=bundle.sample_rate\n",
    "    ):\n",
    "\n",
    "        fig2, ax2 = plt.subplots()\n",
    "\n",
    "        # The original waveform\n",
    "        ratio = waveform.size(0) / sample_rate / trellis.size(0)\n",
    "        ax2.specgram(waveform, Fs=sample_rate)\n",
    "        for word in word_segments:\n",
    "            x0 = ratio * word.start\n",
    "            x1 = ratio * word.end\n",
    "            ax2.axvspan(x0, x1, facecolor=\"none\", edgecolor=\"white\", hatch=\"/\")\n",
    "            # ax2.annotate(f\"{word.score:.2f}\", (x0, sample_rate * 0.51), annotation_clip=False)\n",
    "\n",
    "        for seg in segments:\n",
    "            if seg.label != \"|\":\n",
    "                ax2.annotate(\n",
    "                    seg.label,\n",
    "                    (seg.start * ratio, sample_rate * 0.55),\n",
    "                    annotation_clip=False,\n",
    "                )\n",
    "        ax2.set_xlabel(\"time [second]\")\n",
    "        ax2.set_yticks([])\n",
    "        fig2.tight_layout()\n",
    "        # plt.ioff()\n",
    "        fig2.savefig(output_path)\n",
    "        plt.close(fig2)\n",
    "\n",
    "    plot_alignments(trellis, segments, word_segments, waveform[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tkinter GUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GUIViewerApp:\n",
    "    def __init__(self, root):\n",
    "        # print(\"__init__ GUI\")\n",
    "        global key_phrases_dict_orig\n",
    "        self.root = root\n",
    "        self.root.title(\"Transcription and spectrograms live viewer\")\n",
    "\n",
    "        self.num_background_loop = 1\n",
    "        \n",
    "        self.start_time = 0.0\n",
    "        self.stream_delay = 0.0\n",
    "        self.update_dict_flag = False\n",
    "\n",
    "\n",
    "        # Event to stop threads\n",
    "        self.stop_event = threading.Event()\n",
    "\n",
    "        # Add a Close button\n",
    "        self.close_button = tk.Button(root, text=\"Close\", command=self.close_window)\n",
    "        self.close_button.pack(side=\"top\")\n",
    "\n",
    "        # Pause/Resume button\n",
    "        self.is_paused = False\n",
    "        self.pause_button = tk.Button(\n",
    "            root,\n",
    "            text=\"Pause after current (background) loop\",\n",
    "            command=self.toggle_pause_resume,\n",
    "        )\n",
    "        self.pause_button.pack()\n",
    "\n",
    "        # Label for transcript\n",
    "        self.transcript_label = tk.Label(root, text=\"...\", font=(\"Arial\", 14))\n",
    "        self.transcript_label.pack(pady=10)\n",
    "\n",
    "        # Canvas for spectrogram\n",
    "        self.spectrogram_canvas = tk.Label(root)\n",
    "        self.spectrogram_canvas.pack(side=\"left\")\n",
    "\n",
    "        self.spectrogram_canvas.pack()\n",
    "\n",
    "        self.running = True\n",
    "        self.rows = []\n",
    "\n",
    "        # Create a frame to hold the labels and buttons (fixed part)\n",
    "        self.fixed_frame = tk.Frame(root)\n",
    "        self.fixed_frame.pack(side=\"top\", fill=\"x\", padx=10, pady=10)\n",
    "\n",
    "        # Buttons to add/remove rows\n",
    "        self.add_button = tk.Button(\n",
    "            self.fixed_frame, text=\"Add Row\", command=self.add_row\n",
    "        )\n",
    "        self.add_button.grid(row=1, column=0, padx=5, pady=10)\n",
    "\n",
    "        self.remove_button = tk.Button(\n",
    "            self.fixed_frame, text=\"Remove Row\", command=self.remove_row\n",
    "        )\n",
    "        self.remove_button.grid(row=1, column=1, padx=5, pady=10)\n",
    "\n",
    "        self.update_button = tk.Button(\n",
    "            self.fixed_frame, text=\"Update Dictionary\", command=self.update_dict\n",
    "        )\n",
    "        self.update_button.grid(row=1, column=2, padx=5, pady=10)\n",
    "\n",
    "        # Add labels for the two columns\n",
    "        self.label1 = tk.Label(\n",
    "            self.fixed_frame, text=\"Original Phrase |\", font=(\"Arial\", 10, \"bold\")\n",
    "        )\n",
    "        self.label2 = tk.Label(\n",
    "            self.fixed_frame, text=\"| Replacement Phrase\", font=(\"Arial\", 10, \"bold\")\n",
    "        )\n",
    "\n",
    "        self.label1.grid(row=0, column=0, padx=5, pady=5)\n",
    "        self.label2.grid(row=0, column=1, padx=5, pady=5)\n",
    "\n",
    "        # Create a canvas and a vertical scrollbar for scrolling the rows (scrollable part)\n",
    "        self.row_canvas = tk.Canvas(root, height=200, width=200)\n",
    "        self.scrollbar = tk.Scrollbar(\n",
    "            root, orient=\"vertical\", command=self.row_canvas.yview\n",
    "        )\n",
    "        self.row_frame = tk.Frame(self.row_canvas)\n",
    "\n",
    "        self.row_frame.bind(\n",
    "            \"<Configure>\",\n",
    "            lambda e: self.row_canvas.configure(\n",
    "                scrollregion=self.row_canvas.bbox(\"all\")\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.row_canvas.create_window((0, 0), window=self.row_frame, anchor=\"nw\")\n",
    "        self.row_canvas.configure(yscrollcommand=self.scrollbar.set)\n",
    "        self.row_canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        self.scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "        self.row_canvas.bind_all(\"<MouseWheel>\", self._on_mouse_wheel)\n",
    "\n",
    "        # Add rows based on the initial dictionary\n",
    "        for key, value in key_phrases_dict_orig.items():\n",
    "            self.add_row(key, value)\n",
    "\n",
    "        # Start threads\n",
    "        self.start_threads()\n",
    "\n",
    "\n",
    "\n",
    "    def toggle_pause_resume(self):\n",
    "        if self.is_paused:\n",
    "            self.pause_button.config(text=\"Pause after current (background) loop\")\n",
    "            self.is_paused = False\n",
    "        else:\n",
    "            self.pause_button.config(text=\"Resume (background) looping\")\n",
    "            self.is_paused = True\n",
    "\n",
    "    def start_threads(self):\n",
    "        # print(\"GUI start_threads\")\n",
    "        # Start the main task in a separate thread\n",
    "        self.main_thread = threading.Thread(\n",
    "            target=main_function,\n",
    "            args=(\n",
    "                audio_src,\n",
    "                inp_device,\n",
    "                self,\n",
    "            ),\n",
    "            daemon=True,\n",
    "        )\n",
    "        self.main_thread.start()\n",
    "\n",
    "        # Start worker thread\n",
    "        self.worker_thread = threading.Thread(\n",
    "            target=worker_task, args=(self,), daemon=True\n",
    "        )\n",
    "        self.worker_thread.start()\n",
    "\n",
    "\n",
    "    def stop_threads(self):\n",
    "        # print(\"GUI stop_threads\")\n",
    "        # Signal threads to stop\n",
    "        self.stop_event.set()\n",
    "\n",
    "        # Join threads to ensure they have stopped\n",
    "        self.main_thread.join(timeout=2)\n",
    "        # print(\"main_thread joined\")\n",
    "        self.worker_thread.join(timeout=2)\n",
    "        # print(\"worker_thread joined\")\n",
    "\n",
    "\n",
    "    def add_row(self, key=\"\", value=\"\"):\n",
    "        row = len(self.rows)\n",
    "        entry1 = tk.Entry(self.row_frame, width=20)\n",
    "        entry2 = tk.Entry(self.row_frame, width=20)\n",
    "\n",
    "        entry1.grid(row=row, column=0, padx=5, pady=5)\n",
    "        entry2.grid(row=row, column=1, padx=5, pady=5)\n",
    "\n",
    "        entry1.insert(0, key)\n",
    "        entry2.insert(0, value)\n",
    "\n",
    "        self.rows.append((entry1, entry2))\n",
    "\n",
    "        self.row_frame.update_idletasks()\n",
    "        self.row_canvas.config(scrollregion=self.row_canvas.bbox(\"all\"))\n",
    "\n",
    "    def remove_row(self):\n",
    "        if self.rows:\n",
    "            entry1, entry2 = self.rows.pop()\n",
    "            entry1.grid_forget()\n",
    "            entry2.grid_forget()\n",
    "\n",
    "            self.row_frame.update_idletasks()\n",
    "            self.row_canvas.config(scrollregion=self.row_canvas.bbox(\"all\"))\n",
    "\n",
    "    def update_dict(self):\n",
    "        self.update_dict_flag = True\n",
    "\n",
    "    def _on_mouse_wheel(self, event):\n",
    "        self.row_canvas.yview_scroll(int(-1 * (event.delta / 120)), \"units\")\n",
    "\n",
    "    def update_gui(self, transcript, mel_path):\n",
    "        \n",
    "        \n",
    "        self.transcript_label.config(\n",
    "            text=f\"{transcript}\\n Current background loop number: {self.num_background_loop}\"\n",
    "        )\n",
    "\n",
    "        image = Image.open(mel_path)\n",
    "        self.image_reference = ImageTk.PhotoImage(image)\n",
    "        self.spectrogram_canvas.config(image=self.image_reference)\n",
    "        self.spectrogram_canvas.image = self.image_reference\n",
    "\n",
    "        self.root.after(0, self.root.update)\n",
    "\n",
    "\n",
    "    def clear_queue(self):\n",
    "        # print(\"GUI clear_queue\")\n",
    "        try:\n",
    "            while not data_queue.empty():\n",
    "                data_queue.get_nowait()  # Remove and discard the item\n",
    "                data_queue.task_done()\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "\n",
    "    def close_window(self):\n",
    "        print(\"**** Closing the GUI ****\")\n",
    "        self.running = False\n",
    "        self.stop_threads()\n",
    "        self.clear_queue()\n",
    "        self.root.quit()\n",
    "        self.root.destroy()\n",
    "        try:\n",
    "            sys.exit(0) # Exit the program completely (this will throw an ExitError, but it's innocuous)\n",
    "        except SystemExit:\n",
    "            pass\n",
    "\n",
    "    def no_op(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live speech editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\fbale/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "# Load the Silero VAD model and utilities\n",
    "silero_model, silero_utils = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=False)\n",
    "(get_speech_ts, _, _, _, _) = silero_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split wav audio on silence using silero. Can get labels for speech/silence, and timestamps\n",
    "def split_on_silence_with_padding(audio_segment, silero_model, min_silence_duration_ms=500, sampling_rate=16000):\n",
    "\n",
    "    wav = audio_segment\n",
    "\n",
    "    # Detect non-silent (speech) segments\n",
    "    speech_timestamps = get_speech_ts(\n",
    "        wav,\n",
    "        silero_model,\n",
    "        sampling_rate=sampling_rate,\n",
    "        min_silence_duration_ms=5,\n",
    "        window_size_samples=512,\n",
    "        threshold=silero_sensitivity,\n",
    "    )\n",
    "    \"\"\" \n",
    "    Other parameters that could be passed to Silero to improve things:\n",
    "    threshold: float = 0.5, \n",
    "    sampling_rate: int = 16000, \n",
    "    min_speech_duration_ms: int = 250, \n",
    "    min_silence_duration_ms: int = 100, \n",
    "    window_size_samples: int = 1536, \n",
    "    speech_pad_ms: int = 30, \n",
    "    return_seconds: bool = False, \n",
    "    visualize_probs: bool = False): \n",
    "    see https://github.com/snakers4/silero-vad/discussions/201\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_nonsilence = len(speech_timestamps)\n",
    "\n",
    "    if len(speech_timestamps) == 0:\n",
    "        return [number_of_nonsilence, [wav, \"silence\", False, [0, len(wav)]]]\n",
    "\n",
    "    # Prepare variables for splitting\n",
    "    chunks = []\n",
    "    prev_end = 0\n",
    "    threshold_samples = int((min_silence_duration_ms / 1000) * sampling_rate)\n",
    "    half_threshold_samples = threshold_samples // 2\n",
    "\n",
    "    for index, segment in enumerate(speech_timestamps):\n",
    "        curr_start = segment[\"start\"]\n",
    "        curr_end = segment[\"end\"]\n",
    "        flag_add_future_silence = False\n",
    "\n",
    "        # First let's deal with the endpoint of the segment\n",
    "        if (index < len(speech_timestamps) - 1) and (len(speech_timestamps) >= 2):\n",
    "            future_start = speech_timestamps[index + 1][\"start\"]\n",
    "        else:\n",
    "            future_start = len(wav)\n",
    "        future_silence_duration_samples = future_start - curr_end\n",
    "        if future_silence_duration_samples > threshold_samples:\n",
    "            extra_future_silence_samples = (\n",
    "                future_silence_duration_samples - threshold_samples\n",
    "            )\n",
    "            temp_prev_end = curr_end + half_threshold_samples\n",
    "            temp_next_prev_end = temp_prev_end + extra_future_silence_samples\n",
    "            flag_add_future_silence = True\n",
    "        else:\n",
    "            temp_prev_end = (\n",
    "                curr_end + future_start\n",
    "            ) // 2  # Take midpoint between current end and future start as padding\n",
    "\n",
    "        # Now let's deal with the startpoint of the segment\n",
    "        silence_duration_samples = curr_start - prev_end\n",
    "\n",
    "        # Apart from the very first segment, for all the other segments the distance from the previous segment is always less than threshold / 2\n",
    "        # because of the way we deal with the future endpoints\n",
    "        if silence_duration_samples > threshold_samples:\n",
    "            extra_silence_duration_samples = (silence_duration_samples - threshold_samples)\n",
    "\n",
    "            if extra_silence_duration_samples > 0:\n",
    "                # This is going to be a silent chunk\n",
    "                chunks.append(\n",
    "                    [\n",
    "                        wav[prev_end : curr_start - half_threshold_samples],\n",
    "                        \"silence\",\n",
    "                        False,\n",
    "                        [prev_end, curr_start - half_threshold_samples],\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            # Now append current segment\n",
    "            chunks.append(\n",
    "                [\n",
    "                    wav[curr_start - half_threshold_samples : temp_prev_end],\n",
    "                    \"speech\",\n",
    "                    flag_add_future_silence,\n",
    "                    [curr_start - half_threshold_samples, temp_prev_end],\n",
    "                ]\n",
    "            )\n",
    "            # set the new prev_end\n",
    "            prev_end = temp_prev_end\n",
    "\n",
    "            if flag_add_future_silence:\n",
    "                # Add future silence\n",
    "                chunks.append(\n",
    "                    [\n",
    "                        wav[temp_prev_end:temp_next_prev_end],\n",
    "                        \"silence\",\n",
    "                        False,\n",
    "                        [temp_prev_end, temp_next_prev_end],\n",
    "                    ]\n",
    "                )\n",
    "                # set the new prev_end\n",
    "                prev_end = temp_next_prev_end\n",
    "\n",
    "        else:\n",
    "            chunks.append(\n",
    "                [\n",
    "                    wav[prev_end:temp_prev_end],\n",
    "                    \"speech\",\n",
    "                    flag_add_future_silence,\n",
    "                    [prev_end, temp_prev_end],\n",
    "                ]\n",
    "            )\n",
    "            # set the new prev_end\n",
    "            prev_end = temp_prev_end\n",
    "\n",
    "            if flag_add_future_silence:\n",
    "                # Add future silence\n",
    "                chunks.append(\n",
    "                    [\n",
    "                        wav[temp_prev_end:temp_next_prev_end],\n",
    "                        \"silence\",\n",
    "                        False,\n",
    "                        [temp_prev_end, temp_next_prev_end],\n",
    "                    ]\n",
    "                )\n",
    "                # set the new prev_end\n",
    "                prev_end = temp_next_prev_end\n",
    "\n",
    "    # Deal with the very last segment (which should be a silence)\n",
    "    if prev_end < len(wav):\n",
    "        chunks.append(\n",
    "            [wav[prev_end:], \"last_segment_short_silence\", False, [prev_end, len(wav)]]\n",
    "        )\n",
    "\n",
    "    return [number_of_nonsilence, chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all the various models separately from inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building whisperX transcription model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building our model...\n",
      "LOAD DIFFUSION MODEL TIME: 0.7125575542449951\n",
      "| load 'model_gen' from 'pretrained/hifigan_hifitts\\model_ckpt_steps_2168000.ckpt'.\n",
      "Build Vocoder Time 1.2619240283966064\n",
      "Vocoder Device cuda\n",
      "Loaded the voice encoder model on cuda in 0.09 seconds.\n",
      "WHISPERX LOAD TIME = 5.142096519470215\n",
      "Warming up models...\n",
      "Mask loc buffer set to 10 frames, but there are only 14 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' This is a Librivox recording.',\n",
       " 'chunks': [{'text': ' This', 'timestamp': (0.0, 0.26)},\n",
       "  {'text': ' is', 'timestamp': (0.26, 1.52)},\n",
       "  {'text': ' a', 'timestamp': (1.52, 1.74)},\n",
       "  {'text': ' Librivox', 'timestamp': (1.74, 2.34)},\n",
       "  {'text': ' recording.', 'timestamp': (2.34, 2.76)}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Building whisperX transcription model...\")\n",
    "torch_dtype = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "# https://huggingface.co/models?pipeline_tag=text-to-speech&p=1&sort=trending\n",
    "model_id = \"distil-whisper/distil-large-v3\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(DEVICE)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "whisper = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "\n",
    "hparams = set_hparams(exp_name=\"spec_denoiser\")\n",
    "\n",
    "print(\"Building our model...\")\n",
    "our_model = SpecDenoiserInfer(\n",
    "    hparams,\n",
    "    binary_data_directory,\n",
    "    our_model_ckpt_path,\n",
    "    whisperX_model_directory=whisperX_phoneme_model_directory,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"Warming up models...\")\n",
    "our_model.example_run(\n",
    "    [\n",
    "        {\n",
    "            \"item_name\": \"\",\n",
    "            \"text\": \"this is a libri vox recording\",\n",
    "            \"edited_text\": \"this is a funny joke shows.\",\n",
    "            \"wav_fn_orig\": \"inference/audio_backup/1_space.wav\",\n",
    "            \"edited_region\": \"[4,6]\",\n",
    "            \"region\": \"[4,6]\",\n",
    "            \"mfa_textgrid\": \"\",\n",
    "        }\n",
    "    ],\n",
    "    use_MFA=False,\n",
    "    use_librosa=False,\n",
    "    save_wav_bool=False,\n",
    "    disp_wav=False,\n",
    "    mask_loc_buffer=10,\n",
    ")\n",
    "sample_audio, rate = torchaudio.load(\"inference/audio_backup/1_space.wav\")\n",
    "sample_audio = torchaudio.functional.resample(\n",
    "    sample_audio, orig_freq=rate, new_freq=sample_rate\n",
    ")[0].squeeze()\n",
    "get_speech_ts(\n",
    "    sample_audio,\n",
    "    silero_model,\n",
    "    sampling_rate=sample_rate,\n",
    "    min_silence_duration_ms=5,\n",
    "    window_size_samples=512,\n",
    "    threshold=silero_sensitivity,\n",
    ")\n",
    "whisper(\n",
    "    sample_audio.to(\"cpu\").numpy(),\n",
    "    chunk_length_s=30,\n",
    "    stride_length_s=5,\n",
    "    batch_size=1,\n",
    "    return_timestamps=\"word\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(inp_device, src, app):\n",
    "    global key_phrases_dict_orig\n",
    "    global key_phrases_dict\n",
    "    global key_phrases\n",
    "\n",
    "    long_pause_punc_list = [\". \", \"? \", \"! \", \"... \"]\n",
    "    pause_punc_list = [\". \", \"? \", \"! \", \"... \", \", \", \": \", \"; \"]\n",
    "    extended_pause_punc_list = pause_punc_list + [\" \"]\n",
    "\n",
    "    # Dictionary of the phrases to be replaced (the key is the phrase to be replaced, while the value is the replacement)\n",
    "    # TODO: Initial uppercases/lowercases?\n",
    "\n",
    "    print(f\"Segment Length in seconds: {segment_length/sample_rate}\")\n",
    "    print(f\"Will run for {NUM_ITER*segment_length/sample_rate} seconds\")\n",
    "\n",
    "    @torch.no_grad\n",
    "    def infer(app):\n",
    "        global key_phrases_dict_orig\n",
    "        global key_phrases_dict\n",
    "        global key_phrases\n",
    "        import time\n",
    "\n",
    "        # Counter for keeping track of saved wav files for playback (need to save separate wav files, otherwise permission errors)\n",
    "        COUNTER_FOR_SD = 0\n",
    "\n",
    "        # how many key phrases have been found in total\n",
    "        num_key_phrase = 0\n",
    "\n",
    "        # will hold the current transcript\n",
    "        transcript = \"\"\n",
    "        cur_transcript = \"\"\n",
    "\n",
    "        count_num_audios_saved = 0\n",
    "\n",
    "        transcription_inference_times = [0]\n",
    "        our_model_inference_times = [0]\n",
    "        total_iter_times = []  # This is not used here, so we can remove it.\n",
    "        num_its_before_transcription_locked = []\n",
    "        num_its_before_transcription_counter = 0\n",
    "\n",
    "        total_audio = np.empty([1])\n",
    "\n",
    "        CHUNKS_SO_FAR = torch.empty(0)\n",
    "        flag_start_timing_its_before_transcription = True\n",
    "        flag_at_least_one_transcription = False\n",
    "\n",
    "        print(\"**** Start of streaming ****\")\n",
    "        print(\"Key phrases dict:\", key_phrases_dict_orig)\n",
    "\n",
    "        for i in range(NUM_ITER):\n",
    "            # Check periodically to exit the loop if stop_event is set\n",
    "            if app.stop_event.wait(timeout=0.0001):\n",
    "                break\n",
    "\n",
    "            chunk = q.get()\n",
    "\n",
    "            # for some reason on the first get we wait for several audio chunks to come through. If its the first run, just burn through to the end\n",
    "            # if i==0:\n",
    "            #    while not q.qsize()==0:\n",
    "            #        chunk,chunk_time=q.get()\n",
    "            #    print(time.time()-chunk_time)\n",
    "            initial_audio_recieved_time = time.time()\n",
    "\n",
    "            CHUNKS_SO_FAR = torch.cat((CHUNKS_SO_FAR, chunk[:, 0]), 0)\n",
    "\n",
    "            # Use silero to compute the number of speech bits in the CHUNKS_SO_FAR\n",
    "            number_of_nonsilence, split_by_silence_chunks_list = (\n",
    "                split_on_silence_with_padding(\n",
    "                    CHUNKS_SO_FAR, silero_model, SILERO_MIN_LENGTH_LONG_SILENCE, 16000\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # If there are at least two speech bits or if there is only a speech bit followed by a long silence, then lock the first speech bit for transcription with Whisper\n",
    "            flag_send_to_whisper = False\n",
    "            if number_of_nonsilence > 0:\n",
    "                # Find the first speech bit\n",
    "                flag_found_first_speech_segment = False\n",
    "                index_split_regions = 0\n",
    "                preceding_silence = torch.empty(0)\n",
    "                while not flag_found_first_speech_segment:\n",
    "                    if split_by_silence_chunks_list[index_split_regions][1] != \"speech\":\n",
    "                        preceding_silence = torch.cat(\n",
    "                            (\n",
    "                                preceding_silence,\n",
    "                                split_by_silence_chunks_list[index_split_regions][0],\n",
    "                            )\n",
    "                        )\n",
    "                    if split_by_silence_chunks_list[index_split_regions][1] == \"speech\":\n",
    "                        flag_found_first_speech_segment = True\n",
    "                        segment_to_send_to_whisper = split_by_silence_chunks_list[\n",
    "                            index_split_regions\n",
    "                        ][0]\n",
    "                    index_split_regions += 1\n",
    "                # Check if the first speech bit can be sent to Whisper\n",
    "                # i.e. either there are at least two speech bits or\n",
    "                # there is a speech bit followed by a long silence\n",
    "                if (number_of_nonsilence > req_num_pauses) or (\n",
    "                    split_by_silence_chunks_list[index_split_regions - 1][2] == True\n",
    "                ):\n",
    "                    flag_send_to_whisper = True\n",
    "                    # Reset CHUNKS_SO_FAR to the remaining regions after the first speech region\n",
    "                    CHUNKS_SO_FAR = torch.empty(0)\n",
    "                    while index_split_regions in range(\n",
    "                        len(split_by_silence_chunks_list)\n",
    "                    ):\n",
    "                        CHUNKS_SO_FAR = torch.cat(\n",
    "                            (\n",
    "                                CHUNKS_SO_FAR,\n",
    "                                split_by_silence_chunks_list[index_split_regions][0],\n",
    "                            ),\n",
    "                            0,\n",
    "                        )\n",
    "                        index_split_regions += 1\n",
    "                elif num_its_before_transcription_counter >= MAX_ALLOWED_CHUNKS:\n",
    "                    flag_send_to_whisper = True\n",
    "                    # Reset CHUNKS_SO_FAR to the remaining regions after the first speech region\n",
    "                    CHUNKS_SO_FAR = torch.empty(0)\n",
    "                    while index_split_regions in range(\n",
    "                        len(split_by_silence_chunks_list)\n",
    "                    ):\n",
    "                        CHUNKS_SO_FAR = torch.cat(\n",
    "                            (\n",
    "                                CHUNKS_SO_FAR,\n",
    "                                split_by_silence_chunks_list[index_split_regions][0],\n",
    "                            ),\n",
    "                            0,\n",
    "                        )\n",
    "                        index_split_regions += 1\n",
    "                    print(\"Warning - Not enough pauses detected for optimal inference\")\n",
    "\n",
    "            # this is the case of all silence\n",
    "            elif num_its_before_transcription_counter >= MAX_ALLOWED_CHUNKS:\n",
    "                segment_to_send_to_whisper = CHUNKS_SO_FAR\n",
    "                total_audio = np.concatenate(\n",
    "                    (total_audio, CHUNKS_SO_FAR.numpy()), axis=0\n",
    "                )\n",
    "                #######################################################################################################\n",
    "                wavfile.write(\n",
    "                    f\"{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}.wav\",\n",
    "                    16000,\n",
    "                    CHUNKS_SO_FAR.numpy(),\n",
    "                )\n",
    "                processed_segment_path = f\"{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}.wav\"\n",
    "                audio_segment, _ = torchaudio.load(processed_segment_path)\n",
    "                mel_path = f\"{SAVE_AUDIO_DIR}/mel_flagged_{COUNTER_FOR_SD}.png\"\n",
    "                plot_mel_with_align(audio_segment, cur_transcript, mel_path)\n",
    "                data_queue.put([\"\", mel_path, CHUNKS_SO_FAR.numpy()])\n",
    "                COUNTER_FOR_SD += 1\n",
    "                count_num_audios_saved += 1\n",
    "                num_its_before_transcription_counter = 0\n",
    "                CHUNKS_SO_FAR = torch.empty(0)\n",
    "\n",
    "            num_its_before_transcription_counter += 1\n",
    "\n",
    "            if flag_start_timing_its_before_transcription:\n",
    "                time_its_before_transcription = time.time()\n",
    "                flag_start_timing_its_before_transcription = False\n",
    "\n",
    "            # If the current segment can be passed to whisper for transcription:\n",
    "            if flag_send_to_whisper:\n",
    "\n",
    "                # Get the new dictionary with all the punctuated variations of keys and values\n",
    "                key_phrases_dict = generate_punctuated_key_phrases_dict(\n",
    "                    key_phrases_dict_orig, extended_pause_punc_list\n",
    "                )\n",
    "                key_phrases = list(key_phrases_dict.keys())\n",
    "\n",
    "                def update_key_phrases_dict():\n",
    "                    global key_phrases_dict_orig\n",
    "                    global key_phrases_dict\n",
    "                    global key_phrases\n",
    "\n",
    "                    key_phrases_dict_orig.clear()  # Clear the dictionary before updating\n",
    "\n",
    "                    for entry1, entry2 in app.rows:\n",
    "                        key = entry1.get().strip()\n",
    "                        value = entry2.get().strip()\n",
    "                        if key:  # Only add non-empty keys\n",
    "                            key_phrases_dict_orig[key] = value\n",
    "\n",
    "                    key_phrases_dict = generate_punctuated_key_phrases_dict(\n",
    "                        key_phrases_dict_orig, extended_pause_punc_list\n",
    "                    )\n",
    "                    key_phrases = list(key_phrases_dict.keys())\n",
    "\n",
    "                    print(\n",
    "                        \"Updated dictionary:\", key_phrases_dict_orig\n",
    "                    )  # Print to console for demonstration\n",
    "\n",
    "                if app.update_dict_flag:\n",
    "                    update_key_phrases_dict()\n",
    "                    app.update_dict_flag = False\n",
    "\n",
    "                flag_at_least_one_transcription = True\n",
    "\n",
    "                num_its_before_transcription_locked.append(\n",
    "                    num_its_before_transcription_counter\n",
    "                )\n",
    "                num_its_before_transcription_counter = 0\n",
    "                total_iter_times.append(time.time() - time_its_before_transcription)\n",
    "                flag_start_timing_its_before_transcription = True\n",
    "\n",
    "                # Pass the locked audiosegment to Whisper for transcription\n",
    "                transcription_inference_time_start = time.time()\n",
    "                transcript = whisper(\n",
    "                    segment_to_send_to_whisper.numpy(),\n",
    "                    chunk_length_s=30,\n",
    "                    stride_length_s=5,\n",
    "                    batch_size=1,\n",
    "                    return_timestamps=\"word\",\n",
    "                )\n",
    "\n",
    "                transcription_inference_times.append(\n",
    "                    time.time() - transcription_inference_time_start\n",
    "                )\n",
    "                cur_transcript = (\n",
    "                    \" \" + \" \".join(transcript[\"text\"].lower().split()) + \" \"\n",
    "                )\n",
    "\n",
    "                # Look for flagged words in the transcript\n",
    "                if any([phrase in cur_transcript for phrase in key_phrases]):\n",
    "                    wavfile.write(\n",
    "                        f\"{SAVE_AUDIO_DIR}/flagged_{num_key_phrase}.wav\",\n",
    "                        sample_rate,\n",
    "                        segment_to_send_to_whisper.numpy(),\n",
    "                    )\n",
    "\n",
    "                    # Run our speech editing model\n",
    "                    our_model_inference_time_start = time.time()\n",
    "                    dataset_info = prep_inp_for_replacement_handle_multiple_key_phrases(\n",
    "                        cur_transcript, f\"{SAVE_AUDIO_DIR}/flagged_{num_key_phrase}.wav\"\n",
    "                    )\n",
    "                    result_wavs = our_model.example_run(\n",
    "                        dataset_info, False, False, False, False, 5\n",
    "                    )\n",
    "                    our_model_inference_times.append(\n",
    "                        time.time() - our_model_inference_time_start\n",
    "                    )\n",
    "\n",
    "                    wavfile.write(\n",
    "                        f\"{SAVE_AUDIO_DIR}/flagged_edited_{num_key_phrase}.wav\",\n",
    "                        sample_rate,\n",
    "                        result_wavs[0][1].astype(np.float32),\n",
    "                    )\n",
    "\n",
    "                    num_flagged_phrases = sum(\n",
    "                        x != 0\n",
    "                        for x in [\n",
    "                            phrase in cur_transcript.lower() for phrase in key_phrases\n",
    "                        ]\n",
    "                    )\n",
    "                    num_key_phrase += num_flagged_phrases\n",
    "\n",
    "                    if SAVE_ALL_AUDIO:\n",
    "                        # Note: here the original sampling rate is 22050 because it's the output of our whisperX model (which works at that sampling rate)\n",
    "                        total_audio = np.concatenate(\n",
    "                            (\n",
    "                                total_audio,\n",
    "                                preceding_silence.numpy(),\n",
    "                                torchaudio.functional.resample(\n",
    "                                    torch.tensor(result_wavs[0][1]),\n",
    "                                    orig_freq=22050,\n",
    "                                    new_freq=22050,\n",
    "                                )\n",
    "                                .to(\"cpu\")\n",
    "                                .numpy(),\n",
    "                            ),\n",
    "                            axis=0,\n",
    "                        )\n",
    "\n",
    "                    #######################################################################################################\n",
    "                    wavfile.write(\n",
    "                        f\"{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}_22050.wav\",\n",
    "                        22050,\n",
    "                        torchaudio.functional.resample(\n",
    "                            torch.tensor(result_wavs[0][1]),\n",
    "                            orig_freq=22050,\n",
    "                            new_freq=22050,\n",
    "                        )\n",
    "                        .to(\"cpu\")\n",
    "                        .numpy()\n",
    "                        .astype(np.float32),\n",
    "                    )\n",
    "                    processed_segment_path = (\n",
    "                        f\"{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}_22050.wav\"\n",
    "                    )\n",
    "                    audio_segment, _ = torchaudio.load(processed_segment_path)\n",
    "                    mel_path = f\"{SAVE_AUDIO_DIR}/mel_flagged_{COUNTER_FOR_SD}.png\"\n",
    "                    plot_mel_with_align(\n",
    "                        audio_segment, dataset_info[0][\"edited_text\"], mel_path\n",
    "                    )\n",
    "                    resampled_inferred_audio = (\n",
    "                        torchaudio.functional.resample(\n",
    "                            torch.tensor(result_wavs[0][1]),\n",
    "                            orig_freq=22050,\n",
    "                            new_freq=sample_rate,\n",
    "                        )\n",
    "                        .to(\"cpu\")\n",
    "                        .numpy()\n",
    "                    )\n",
    "\n",
    "\n",
    "                    data_queue.put(\n",
    "                        [\n",
    "                            dataset_info[0][\"edited_text_with_marked_words\"],\n",
    "                            mel_path,\n",
    "                            np.concatenate((preceding_silence.numpy(), resampled_inferred_audio), 0)\n",
    "                        ]\n",
    "                    )\n",
    "                    COUNTER_FOR_SD += 1\n",
    "                    #######################################################################################################\n",
    "\n",
    "                    count_num_audios_saved += 1\n",
    "\n",
    "                else:  # No flagged words\n",
    "                    total_audio = np.concatenate(\n",
    "                        (\n",
    "                            total_audio,\n",
    "                            preceding_silence.numpy(),\n",
    "                            segment_to_send_to_whisper.numpy(),\n",
    "                        ),\n",
    "                        axis=0,\n",
    "                    )\n",
    "\n",
    "                    #######################################################################################################\n",
    "                    wavfile.write(\n",
    "                        f\"{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}.wav\",\n",
    "                        16000,\n",
    "                        segment_to_send_to_whisper.numpy(),\n",
    "                    )\n",
    "                    processed_segment_path = (\n",
    "                        f\"{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}.wav\"\n",
    "                    )\n",
    "                    audio_segment, _ = torchaudio.load(processed_segment_path)\n",
    "                    mel_path = f\"{SAVE_AUDIO_DIR}/mel_flagged_{COUNTER_FOR_SD}.png\"\n",
    "                    plot_mel_with_align(audio_segment, cur_transcript, mel_path)\n",
    "                    data_queue.put(\n",
    "                        [\n",
    "                            cur_transcript,\n",
    "                            mel_path,\n",
    "                            np.concatenate(\n",
    "                                (\n",
    "                                    preceding_silence.numpy(),\n",
    "                                    segment_to_send_to_whisper.numpy(),\n",
    "                                ),\n",
    "                                0,\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    COUNTER_FOR_SD += 1\n",
    "                    #######################################################################################################\n",
    "\n",
    "                    count_num_audios_saved += 1\n",
    "\n",
    "        if SAVE_ALL_AUDIO:\n",
    "            wavfile.write(\n",
    "                f\"{SAVE_AUDIO_DIR}/complete.wav\", 22050, total_audio.astype(np.float32)\n",
    "            )\n",
    "\n",
    "        if not flag_at_least_one_transcription:\n",
    "            total_iter_times.append(time.time() - time_its_before_transcription)\n",
    "\n",
    "        print(\n",
    "            f\"Average Transcription Inference Time: {np.mean(transcription_inference_times)}. Maximum: {np.max(transcription_inference_times)} Transcription performed on {len(transcription_inference_times)-1} of {NUM_ITER} iterations\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Average Our Model Inference Time: {np.mean(our_model_inference_times)}. Maximum: {np.max(our_model_inference_times)} .Replacement performed on {len(our_model_inference_times)-1} of {NUM_ITER} iterations\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Average Total Iteration Time: {np.mean(total_iter_times)}. Maximum: {np.max(total_iter_times)}.\"\n",
    "        )\n",
    "        try:\n",
    "            print(\n",
    "                f\"Average Number of Iterations before transcription: {np.mean(num_its_before_transcription_locked)}. Maximum: {np.max(num_its_before_transcription_locked)}. Number of locked segments: {len(num_its_before_transcription_locked)}\"\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(f\"Total Audio Shape: {np.shape(total_audio)}\")\n",
    "\n",
    "        print(\"**** Iterations finished! ****\")\n",
    "\n",
    "    def generate_punctuated_key_phrases_dict(\n",
    "        key_phrases_dict, extended_pause_punc_list\n",
    "    ):\n",
    "        punct_key_phrases_dict = {}\n",
    "        key_phrases = list(key_phrases_dict.keys())\n",
    "        for phrase in key_phrases:\n",
    "            flag_starts_with_space = False\n",
    "            if not phrase.startswith(\" \"):\n",
    "                flag_starts_with_space = True\n",
    "\n",
    "            for p in extended_pause_punc_list:\n",
    "                if not phrase.endswith(p):\n",
    "                    if not flag_starts_with_space:\n",
    "                        punct_key_phrases_dict[phrase + p] = (\n",
    "                            key_phrases_dict[phrase] + p\n",
    "                        )\n",
    "                    else:\n",
    "                        punct_key_phrases_dict[\" \" + phrase + p] = (\n",
    "                            \" \" + key_phrases_dict[phrase] + p\n",
    "                        )\n",
    "        return punct_key_phrases_dict\n",
    "\n",
    "    def prep_inp_for_replacement_handle_multiple_key_phrases(\n",
    "        transcipt, file_name\n",
    "    ):  # ,silero_timestamps):\n",
    "        global key_phrases_dict\n",
    "        global key_phrases\n",
    "        # right now if there are multiple key phrases in the transcipt, this tells our model to infer those phrases and everything in between\n",
    "        # a smarter way to do this would be to make it so that our model supports editing multiple regions\n",
    "\n",
    "        dataset_info = [{}]\n",
    "        dataset_info[0][\n",
    "            \"item_name\"\n",
    "        ] = \"\"  # this should just be used for naming the output file\n",
    "        dataset_info[0][\"text\"] = \" \".join(\n",
    "            transcipt.lower().split()\n",
    "        )  # a transcription of the original text\n",
    "        for punc in pause_punc_list:\n",
    "            dataset_info[0][\"text\"] = dataset_info[0][\"text\"].replace(punc.strip(), \"\")\n",
    "        dataset_info[0][\n",
    "            \"wav_fn_orig\"\n",
    "        ] = file_name  # location of the .wav file to perform inference on\n",
    "\n",
    "        transcript_words = transcipt.lower().split()\n",
    "        key_phrase_words = [phrase.split() for phrase in key_phrases]\n",
    "\n",
    "        # right now if multiple key phrases exist in the transcript, we consider the last one appearing in key_phrases_dict as the one to be replaced, and replace the first occurance of it\n",
    "\n",
    "        num_ves = 0\n",
    "\n",
    "        phrase_info_dicts = []\n",
    "\n",
    "        change_in_region_length = 0\n",
    "        for phrase in key_phrase_words:\n",
    "            try:\n",
    "                word_reg_start = transcript_words.index(phrase[0])\n",
    "                word_reg_end = transcript_words.index(phrase[-1])\n",
    "                key_phrase = \" \" + \" \".join(phrase) + \" \"\n",
    "                phrase_info_dicts.append(\n",
    "                    {\"phrase\": key_phrase, \"start\": word_reg_start, \"end\": word_reg_end}\n",
    "                )\n",
    "                key = \" \" + \" \".join(phrase) + \" \"\n",
    "                change_in_region_length += len(key_phrases_dict[key].split()) - len(\n",
    "                    phrase\n",
    "                )\n",
    "                print(change_in_region_length)\n",
    "            except ValueError:\n",
    "                num_ves += 1\n",
    "        if num_ves >= len(key_phrase_words):\n",
    "            print(\n",
    "                \"Error: Attempting to replace a word that does not exist in the transcript\"\n",
    "            )\n",
    "            return 1\n",
    "\n",
    "        dataset_info[0][\"edited_text\"] = \" \" + \" \".join(transcipt.lower().split()) + \" \"\n",
    "        dataset_info[0][\"edited_text_with_marked_words\"] = (\n",
    "            \" \" + \" \".join(transcipt.lower().split()) + \" \"\n",
    "        )\n",
    "\n",
    "        for phrase in key_phrases:\n",
    "            replacement_phrase = key_phrases_dict[phrase]\n",
    "            dataset_info[0][\"edited_text_with_marked_words\"] = dataset_info[0][\n",
    "                \"edited_text_with_marked_words\"\n",
    "            ].replace(phrase, f\" |{replacement_phrase.strip()}| \")\n",
    "            dataset_info[0][\"edited_text\"] = dataset_info[0][\"edited_text\"].replace(\n",
    "                phrase, replacement_phrase\n",
    "            )\n",
    "\n",
    "        for punc in pause_punc_list:\n",
    "            dataset_info[0][\"edited_text\"] = dataset_info[0][\"edited_text\"].replace(\n",
    "                punc.strip(), \"\"\n",
    "            )\n",
    "            dataset_info[0][\"edited_text_with_marked_words\"] = dataset_info[0][\n",
    "                \"edited_text_with_marked_words\"\n",
    "            ].replace(punc.strip(), \"\")\n",
    "        dataset_info[0][\"edited_text\"] = dataset_info[0][\"edited_text\"].strip()\n",
    "        dataset_info[0][\"edited_text_with_marked_words\"] = dataset_info[0][\n",
    "            \"edited_text_with_marked_words\"\n",
    "        ].strip()\n",
    "\n",
    "        phrase_info_dicts = sorted(\n",
    "            phrase_info_dicts, key=lambda d: d[\"start\"]\n",
    "        )  # this could potentially fail, it assumes that the transcript and phrases are all so that the first occurance of the first word in the phrase is contained in the full phrase\n",
    "        word_reg_start = phrase_info_dicts[0][\"start\"]\n",
    "        word_reg_end = phrase_info_dicts[-1][\"end\"]\n",
    "\n",
    "        dataset_info[0][\n",
    "            \"region\"\n",
    "        ] = f\"[{word_reg_start+1},{word_reg_end+1}]\"  # the region to edit (counting the words that will be changed starting from 1)\n",
    "\n",
    "        dataset_info[0][\n",
    "            \"edited_region\"\n",
    "        ] = f\"[{word_reg_start+1},{word_reg_end+1+change_in_region_length}]\"  # word counts in the full edited text of the region which is to be inferred starting from one\n",
    "\n",
    "        dataset_info[0][\n",
    "            \"mfa_textgrid\"\n",
    "        ] = \"\"  # we still need to set this to some value even if we are not using MFA\n",
    "\n",
    "        if not os.path.exists(\"./transcripts\"):\n",
    "            os.makedirs(\"./transcripts\")\n",
    "        import json\n",
    "\n",
    "        with open(f\"./transcripts/dataset_info.txt\", \"a\") as f:\n",
    "            f.write(json.dumps(dataset_info[0]))\n",
    "            f.write(\"\\n\")\n",
    "        return dataset_info\n",
    "\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    manager = (\n",
    "        ctx.Manager()\n",
    "    )  # for some reason this fixes an issue I was having with multiprocessing https://discuss.pytorch.org/t/using-torch-tensor-over-multiprocessing-queue-process-fails/2847\n",
    "    q = manager.Queue()\n",
    "    p = ctx.Process(\n",
    "        target=stream,\n",
    "        args=(\n",
    "            q,\n",
    "            inp_device,\n",
    "            src,\n",
    "            segment_length,\n",
    "            sample_rate,\n",
    "            NUM_ITER,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # print(\"p started\")\n",
    "    p.start()\n",
    "    p_start_time = time.time()\n",
    "    infer(app)\n",
    "    # print(\"infer finished\")\n",
    "    p.join(timeout=1)\n",
    "    # print(\"p joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(audio_src, inp_device, app):\n",
    "\n",
    "    while not app.stop_event.is_set():\n",
    "        # Check if the GUI has been paused\n",
    "        if not app.is_paused:\n",
    "            print(\"****************\")\n",
    "            print(\"(Background) loop number\", app.num_background_loop)\n",
    "            print(\"****************\")\n",
    "            gc.collect()\n",
    "\n",
    "            data_queue = queue.Queue()\n",
    "            main(src=audio_src, inp_device=inp_device, app=app)\n",
    "\n",
    "            app.num_background_loop += 1\n",
    "        else:\n",
    "            time.sleep(0.0001)\n",
    "\n",
    "        # Check periodically to exit the loop if stop_event is set\n",
    "        if app.stop_event.wait(timeout=0.1):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_task(app):\n",
    "\n",
    "    FORMAT = pyaudio.paFloat32\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "\n",
    "    pya = pyaudio.PyAudio()\n",
    "\n",
    "    py_stream = pya.open(format=FORMAT, channels=CHANNELS, rate=RATE, output=True)\n",
    "\n",
    "    while not app.stop_event.is_set():\n",
    "        while not data_queue.empty():\n",
    "            (transcript, mel_path, audio_chunk) = data_queue.get()\n",
    "            \n",
    "\n",
    "            if app.running:\n",
    "                # Check if the root window is still active\n",
    "                if app.root.winfo_exists():\n",
    "                    # Schedule the GUI update on the main thread\n",
    "                    try:\n",
    "                        app.root.after(0, app.update_gui, transcript, mel_path)\n",
    "                    except RuntimeError:\n",
    "                        # The root window may have been destroyed in the meantime\n",
    "                        break\n",
    "\n",
    "                \n",
    "                py_stream.write(audio_chunk.tobytes())\n",
    "                # py_stream.write(audio_chunk)\n",
    "                data_queue.task_done()\n",
    "            else:\n",
    "                break\n",
    "        if not app.running:\n",
    "            break\n",
    "\n",
    "        while data_queue.empty():\n",
    "            time.sleep(0.0001)\n",
    "            if not app.running:\n",
    "                break\n",
    "        # Check periodically to exit the loop if stop_event is set\n",
    "        if app.stop_event.wait(timeout=0.1):\n",
    "            break\n",
    "\n",
    "    py_stream.stop_stream()\n",
    "    py_stream.close()\n",
    "    pya.terminate()\n",
    "\n",
    "\n",
    "    print(\"If it didn't close automatically, please manually close the GUI\")\n",
    "    try:\n",
    "        while not data_queue.empty():\n",
    "            data_queue.get_nowait()  # Remove and discard the item\n",
    "            data_queue.task_done()  # Mark the task as done\n",
    "    except data_queue.empty():\n",
    "        pass  # If the queue is already empty, just pass\n",
    "    # app.root.after(1, app.root.destroy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this!\n",
    "inp_device = None  # None for a file\n",
    "\n",
    "# The path to your .wav file (make sure the sample rate is 16000 and the audio is mono)\n",
    "audio_src = \"test_sample.wav\"\n",
    "\n",
    "# whether to save all audio and the initial dictionary (can be updated in gui but takes a long time before will take effect)\n",
    "SAVE_ALL_AUDIO = False\n",
    "\n",
    "# The number of iterations for each background loop; the length of the playback of a loop is NUM_ITER * segment_length / sample_rate\n",
    "NUM_ITER = 20\n",
    "\n",
    "# The words/phrases to be edited (the key is the word/phrase to be edited, the value is what it gets edited to )\n",
    "key_phrases_dict_orig = {\n",
    "    'blueprints': 'yellowprints',\n",
    "    'hand':'lend',\n",
    "    'things':'kings'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************\n",
      "(Background) loop number 1\n",
      "****************\n",
      "Segment Length in seconds: 1.2\n",
      "Will run for 24.0 seconds\n",
      "**** Start of streaming ****\n",
      "Key phrases dict: {'blueprints': 'yellowprints', 'hand': 'lend', 'things': 'kings'}\n",
      "**** Closing the GUI ****\n",
      "If it didn't close automatically, please manually close the GUI\n",
      "**** End of streaming ****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Transcription Inference Time: 0.0. Maximum: 0 Transcription performed on 0 of 20 iterations\n",
      "Average Our Model Inference Time: 0.0. Maximum: 0 .Replacement performed on 0 of 20 iterations\n",
      "Average Total Iteration Time: 0.0. Maximum: 0.0.\n",
      "Total Audio Shape: (1,)\n",
      "**** Iterations finished! ****\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    clear_output()\n",
    "    gc.collect()\n",
    "    tk_root = tk.Tk()\n",
    "    tk_app = GUIViewerApp(tk_root)\n",
    "    tk_root.mainloop()\n",
    "    print(\"**** End of streaming ****\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aligner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
