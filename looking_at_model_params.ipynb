{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bezem\\miniconda3\\envs\\aligner\\lib\\site-packages\\pyannote.audio-3.1.1-py3.10.egg\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modules.speech_editing.spec_denoiser.spec_denoiser import GaussianDiffusion\n",
    "from utils.commons.ckpt_utils import load_ckpt\n",
    "from utils.commons.hparams import set_hparams\n",
    "from data_gen.tts.base_preprocess import BasePreprocessor\n",
    "from modules.speech_editing.spec_denoiser.diffnet import DiffNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| load 'model' from 'checkpoints/spec_denoiser\\model_ckpt_steps_568000.ckpt'.\n"
     ]
    }
   ],
   "source": [
    "binary_data_directory='C:\\\\Users\\\\bezem\\\\Documents\\\\erdos_deep_learning\\\\Speech-Editing-Toolkit-stable\\\\data\\\\processed\\\\binary\\\\libritts'\n",
    "hparams=set_hparams(exp_name='spec_denoiser')\n",
    "preprocessor=BasePreprocessor()\n",
    "ph_encoder,word_encoder=preprocessor.load_dict(binary_data_directory)\n",
    "DIFF_DECODERS = {\n",
    "    'wavenet': lambda hp: DiffNet(hp['audio_num_mel_bins']),\n",
    "}\n",
    "model = GaussianDiffusion(\n",
    "            phone_encoder=ph_encoder,\n",
    "            out_dims=hparams['audio_num_mel_bins'], denoise_fn=DIFF_DECODERS[hparams['diff_decoder_type']](hparams),\n",
    "            timesteps=hparams['timesteps'], time_scale=hparams['timescale'],\n",
    "            loss_type=hparams['diff_loss_type'],\n",
    "            spec_min=hparams['spec_min'], spec_max=hparams['spec_max'],\n",
    "        )\n",
    "load_ckpt(model, hparams['work_dir'], 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names=[param[0] for param in model.named_parameters()] #see if we can tell what parameters corresponds to the Layer norms from their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps\n",
      "timescale\n",
      "betas\n",
      "alphas_cumprod\n",
      "alphas_cumprod_prev\n",
      "sqrt_alphas_cumprod\n",
      "sqrt_one_minus_alphas_cumprod\n",
      "log_one_minus_alphas_cumprod\n",
      "sqrt_recip_alphas_cumprod\n",
      "sqrt_recipm1_alphas_cumprod\n",
      "posterior_variance\n",
      "posterior_log_variance_clipped\n",
      "posterior_mean_coef1\n",
      "posterior_mean_coef2\n",
      "spec_min\n",
      "spec_max\n"
     ]
    }
   ],
   "source": [
    "#make sure none are hidden from param_names https://discuss.pytorch.org/t/model-named-parameters-will-lose-some-layer-modules/14588/3\n",
    "for k, v in model.state_dict().items():\n",
    "    if k not in param_names:\n",
    "        print(k)\n",
    "#looks like were good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denoise_fn.input_projection.weight\n",
      "denoise_fn.input_projection.bias\n",
      "denoise_fn.mlp.0.weight\n",
      "denoise_fn.mlp.0.bias\n",
      "denoise_fn.mlp.2.weight\n",
      "denoise_fn.mlp.2.bias\n",
      "denoise_fn.residual_layers.0.dilated_conv.weight\n",
      "denoise_fn.residual_layers.0.dilated_conv.bias\n",
      "denoise_fn.residual_layers.0.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.0.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.0.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.0.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.0.output_projection.weight\n",
      "denoise_fn.residual_layers.0.output_projection.bias\n",
      "denoise_fn.residual_layers.1.dilated_conv.weight\n",
      "denoise_fn.residual_layers.1.dilated_conv.bias\n",
      "denoise_fn.residual_layers.1.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.1.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.1.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.1.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.1.output_projection.weight\n",
      "denoise_fn.residual_layers.1.output_projection.bias\n",
      "denoise_fn.residual_layers.2.dilated_conv.weight\n",
      "denoise_fn.residual_layers.2.dilated_conv.bias\n",
      "denoise_fn.residual_layers.2.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.2.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.2.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.2.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.2.output_projection.weight\n",
      "denoise_fn.residual_layers.2.output_projection.bias\n",
      "denoise_fn.residual_layers.3.dilated_conv.weight\n",
      "denoise_fn.residual_layers.3.dilated_conv.bias\n",
      "denoise_fn.residual_layers.3.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.3.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.3.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.3.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.3.output_projection.weight\n",
      "denoise_fn.residual_layers.3.output_projection.bias\n",
      "denoise_fn.residual_layers.4.dilated_conv.weight\n",
      "denoise_fn.residual_layers.4.dilated_conv.bias\n",
      "denoise_fn.residual_layers.4.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.4.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.4.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.4.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.4.output_projection.weight\n",
      "denoise_fn.residual_layers.4.output_projection.bias\n",
      "denoise_fn.residual_layers.5.dilated_conv.weight\n",
      "denoise_fn.residual_layers.5.dilated_conv.bias\n",
      "denoise_fn.residual_layers.5.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.5.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.5.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.5.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.5.output_projection.weight\n",
      "denoise_fn.residual_layers.5.output_projection.bias\n",
      "denoise_fn.residual_layers.6.dilated_conv.weight\n",
      "denoise_fn.residual_layers.6.dilated_conv.bias\n",
      "denoise_fn.residual_layers.6.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.6.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.6.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.6.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.6.output_projection.weight\n",
      "denoise_fn.residual_layers.6.output_projection.bias\n",
      "denoise_fn.residual_layers.7.dilated_conv.weight\n",
      "denoise_fn.residual_layers.7.dilated_conv.bias\n",
      "denoise_fn.residual_layers.7.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.7.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.7.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.7.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.7.output_projection.weight\n",
      "denoise_fn.residual_layers.7.output_projection.bias\n",
      "denoise_fn.residual_layers.8.dilated_conv.weight\n",
      "denoise_fn.residual_layers.8.dilated_conv.bias\n",
      "denoise_fn.residual_layers.8.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.8.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.8.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.8.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.8.output_projection.weight\n",
      "denoise_fn.residual_layers.8.output_projection.bias\n",
      "denoise_fn.residual_layers.9.dilated_conv.weight\n",
      "denoise_fn.residual_layers.9.dilated_conv.bias\n",
      "denoise_fn.residual_layers.9.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.9.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.9.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.9.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.9.output_projection.weight\n",
      "denoise_fn.residual_layers.9.output_projection.bias\n",
      "denoise_fn.residual_layers.10.dilated_conv.weight\n",
      "denoise_fn.residual_layers.10.dilated_conv.bias\n",
      "denoise_fn.residual_layers.10.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.10.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.10.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.10.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.10.output_projection.weight\n",
      "denoise_fn.residual_layers.10.output_projection.bias\n",
      "denoise_fn.residual_layers.11.dilated_conv.weight\n",
      "denoise_fn.residual_layers.11.dilated_conv.bias\n",
      "denoise_fn.residual_layers.11.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.11.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.11.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.11.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.11.output_projection.weight\n",
      "denoise_fn.residual_layers.11.output_projection.bias\n",
      "denoise_fn.residual_layers.12.dilated_conv.weight\n",
      "denoise_fn.residual_layers.12.dilated_conv.bias\n",
      "denoise_fn.residual_layers.12.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.12.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.12.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.12.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.12.output_projection.weight\n",
      "denoise_fn.residual_layers.12.output_projection.bias\n",
      "denoise_fn.residual_layers.13.dilated_conv.weight\n",
      "denoise_fn.residual_layers.13.dilated_conv.bias\n",
      "denoise_fn.residual_layers.13.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.13.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.13.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.13.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.13.output_projection.weight\n",
      "denoise_fn.residual_layers.13.output_projection.bias\n",
      "denoise_fn.residual_layers.14.dilated_conv.weight\n",
      "denoise_fn.residual_layers.14.dilated_conv.bias\n",
      "denoise_fn.residual_layers.14.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.14.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.14.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.14.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.14.output_projection.weight\n",
      "denoise_fn.residual_layers.14.output_projection.bias\n",
      "denoise_fn.residual_layers.15.dilated_conv.weight\n",
      "denoise_fn.residual_layers.15.dilated_conv.bias\n",
      "denoise_fn.residual_layers.15.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.15.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.15.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.15.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.15.output_projection.weight\n",
      "denoise_fn.residual_layers.15.output_projection.bias\n",
      "denoise_fn.residual_layers.16.dilated_conv.weight\n",
      "denoise_fn.residual_layers.16.dilated_conv.bias\n",
      "denoise_fn.residual_layers.16.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.16.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.16.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.16.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.16.output_projection.weight\n",
      "denoise_fn.residual_layers.16.output_projection.bias\n",
      "denoise_fn.residual_layers.17.dilated_conv.weight\n",
      "denoise_fn.residual_layers.17.dilated_conv.bias\n",
      "denoise_fn.residual_layers.17.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.17.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.17.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.17.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.17.output_projection.weight\n",
      "denoise_fn.residual_layers.17.output_projection.bias\n",
      "denoise_fn.residual_layers.18.dilated_conv.weight\n",
      "denoise_fn.residual_layers.18.dilated_conv.bias\n",
      "denoise_fn.residual_layers.18.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.18.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.18.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.18.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.18.output_projection.weight\n",
      "denoise_fn.residual_layers.18.output_projection.bias\n",
      "denoise_fn.residual_layers.19.dilated_conv.weight\n",
      "denoise_fn.residual_layers.19.dilated_conv.bias\n",
      "denoise_fn.residual_layers.19.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.19.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.19.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.19.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.19.output_projection.weight\n",
      "denoise_fn.residual_layers.19.output_projection.bias\n",
      "denoise_fn.skip_projection.weight\n",
      "denoise_fn.skip_projection.bias\n",
      "denoise_fn.output_projection.weight\n",
      "denoise_fn.output_projection.bias\n",
      "fs.encoder.res_blocks.0.blocks.0.0.weight\n",
      "fs.encoder.res_blocks.0.blocks.0.0.bias\n",
      "fs.encoder.res_blocks.0.blocks.0.1.weight\n",
      "fs.encoder.res_blocks.0.blocks.0.1.bias\n",
      "fs.encoder.res_blocks.0.blocks.0.4.weight\n",
      "fs.encoder.res_blocks.0.blocks.0.4.bias\n",
      "fs.encoder.res_blocks.0.blocks.1.0.weight\n",
      "fs.encoder.res_blocks.0.blocks.1.0.bias\n",
      "fs.encoder.res_blocks.0.blocks.1.1.weight\n",
      "fs.encoder.res_blocks.0.blocks.1.1.bias\n",
      "fs.encoder.res_blocks.0.blocks.1.4.weight\n",
      "fs.encoder.res_blocks.0.blocks.1.4.bias\n",
      "fs.encoder.res_blocks.1.blocks.0.0.weight\n",
      "fs.encoder.res_blocks.1.blocks.0.0.bias\n",
      "fs.encoder.res_blocks.1.blocks.0.1.weight\n",
      "fs.encoder.res_blocks.1.blocks.0.1.bias\n",
      "fs.encoder.res_blocks.1.blocks.0.4.weight\n",
      "fs.encoder.res_blocks.1.blocks.0.4.bias\n",
      "fs.encoder.res_blocks.1.blocks.1.0.weight\n",
      "fs.encoder.res_blocks.1.blocks.1.0.bias\n",
      "fs.encoder.res_blocks.1.blocks.1.1.weight\n",
      "fs.encoder.res_blocks.1.blocks.1.1.bias\n",
      "fs.encoder.res_blocks.1.blocks.1.4.weight\n",
      "fs.encoder.res_blocks.1.blocks.1.4.bias\n",
      "fs.encoder.res_blocks.2.blocks.0.0.weight\n",
      "fs.encoder.res_blocks.2.blocks.0.0.bias\n",
      "fs.encoder.res_blocks.2.blocks.0.1.weight\n",
      "fs.encoder.res_blocks.2.blocks.0.1.bias\n",
      "fs.encoder.res_blocks.2.blocks.0.4.weight\n",
      "fs.encoder.res_blocks.2.blocks.0.4.bias\n",
      "fs.encoder.res_blocks.2.blocks.1.0.weight\n",
      "fs.encoder.res_blocks.2.blocks.1.0.bias\n",
      "fs.encoder.res_blocks.2.blocks.1.1.weight\n",
      "fs.encoder.res_blocks.2.blocks.1.1.bias\n",
      "fs.encoder.res_blocks.2.blocks.1.4.weight\n",
      "fs.encoder.res_blocks.2.blocks.1.4.bias\n",
      "fs.encoder.res_blocks.3.blocks.0.0.weight\n",
      "fs.encoder.res_blocks.3.blocks.0.0.bias\n",
      "fs.encoder.res_blocks.3.blocks.0.1.weight\n",
      "fs.encoder.res_blocks.3.blocks.0.1.bias\n",
      "fs.encoder.res_blocks.3.blocks.0.4.weight\n",
      "fs.encoder.res_blocks.3.blocks.0.4.bias\n",
      "fs.encoder.res_blocks.3.blocks.1.0.weight\n",
      "fs.encoder.res_blocks.3.blocks.1.0.bias\n",
      "fs.encoder.res_blocks.3.blocks.1.1.weight\n",
      "fs.encoder.res_blocks.3.blocks.1.1.bias\n",
      "fs.encoder.res_blocks.3.blocks.1.4.weight\n",
      "fs.encoder.res_blocks.3.blocks.1.4.bias\n",
      "fs.encoder.last_norm.weight\n",
      "fs.encoder.last_norm.bias\n",
      "fs.encoder.post_net1.weight\n",
      "fs.encoder.post_net1.bias\n",
      "fs.encoder.embed_tokens.weight\n",
      "fs.decoder.res_blocks.0.blocks.0.0.weight\n",
      "fs.decoder.res_blocks.0.blocks.0.0.bias\n",
      "fs.decoder.res_blocks.0.blocks.0.1.weight\n",
      "fs.decoder.res_blocks.0.blocks.0.1.bias\n",
      "fs.decoder.res_blocks.0.blocks.0.4.weight\n",
      "fs.decoder.res_blocks.0.blocks.0.4.bias\n",
      "fs.decoder.res_blocks.0.blocks.1.0.weight\n",
      "fs.decoder.res_blocks.0.blocks.1.0.bias\n",
      "fs.decoder.res_blocks.0.blocks.1.1.weight\n",
      "fs.decoder.res_blocks.0.blocks.1.1.bias\n",
      "fs.decoder.res_blocks.0.blocks.1.4.weight\n",
      "fs.decoder.res_blocks.0.blocks.1.4.bias\n",
      "fs.decoder.res_blocks.1.blocks.0.0.weight\n",
      "fs.decoder.res_blocks.1.blocks.0.0.bias\n",
      "fs.decoder.res_blocks.1.blocks.0.1.weight\n",
      "fs.decoder.res_blocks.1.blocks.0.1.bias\n",
      "fs.decoder.res_blocks.1.blocks.0.4.weight\n",
      "fs.decoder.res_blocks.1.blocks.0.4.bias\n",
      "fs.decoder.res_blocks.1.blocks.1.0.weight\n",
      "fs.decoder.res_blocks.1.blocks.1.0.bias\n",
      "fs.decoder.res_blocks.1.blocks.1.1.weight\n",
      "fs.decoder.res_blocks.1.blocks.1.1.bias\n",
      "fs.decoder.res_blocks.1.blocks.1.4.weight\n",
      "fs.decoder.res_blocks.1.blocks.1.4.bias\n",
      "fs.decoder.res_blocks.2.blocks.0.0.weight\n",
      "fs.decoder.res_blocks.2.blocks.0.0.bias\n",
      "fs.decoder.res_blocks.2.blocks.0.1.weight\n",
      "fs.decoder.res_blocks.2.blocks.0.1.bias\n",
      "fs.decoder.res_blocks.2.blocks.0.4.weight\n",
      "fs.decoder.res_blocks.2.blocks.0.4.bias\n",
      "fs.decoder.res_blocks.2.blocks.1.0.weight\n",
      "fs.decoder.res_blocks.2.blocks.1.0.bias\n",
      "fs.decoder.res_blocks.2.blocks.1.1.weight\n",
      "fs.decoder.res_blocks.2.blocks.1.1.bias\n",
      "fs.decoder.res_blocks.2.blocks.1.4.weight\n",
      "fs.decoder.res_blocks.2.blocks.1.4.bias\n",
      "fs.decoder.res_blocks.3.blocks.0.0.weight\n",
      "fs.decoder.res_blocks.3.blocks.0.0.bias\n",
      "fs.decoder.res_blocks.3.blocks.0.1.weight\n",
      "fs.decoder.res_blocks.3.blocks.0.1.bias\n",
      "fs.decoder.res_blocks.3.blocks.0.4.weight\n",
      "fs.decoder.res_blocks.3.blocks.0.4.bias\n",
      "fs.decoder.res_blocks.3.blocks.1.0.weight\n",
      "fs.decoder.res_blocks.3.blocks.1.0.bias\n",
      "fs.decoder.res_blocks.3.blocks.1.1.weight\n",
      "fs.decoder.res_blocks.3.blocks.1.1.bias\n",
      "fs.decoder.res_blocks.3.blocks.1.4.weight\n",
      "fs.decoder.res_blocks.3.blocks.1.4.bias\n",
      "fs.decoder.last_norm.weight\n",
      "fs.decoder.last_norm.bias\n",
      "fs.decoder.post_net1.weight\n",
      "fs.decoder.post_net1.bias\n",
      "fs.mel_out.weight\n",
      "fs.mel_out.bias\n",
      "fs.spk_embed_proj.weight\n",
      "fs.spk_embed_proj.bias\n",
      "fs.dur_embed.weight\n",
      "fs.dur_predictor.conv.0.0.weight\n",
      "fs.dur_predictor.conv.0.0.bias\n",
      "fs.dur_predictor.conv.0.2.weight\n",
      "fs.dur_predictor.conv.0.2.bias\n",
      "fs.dur_predictor.conv.1.0.weight\n",
      "fs.dur_predictor.conv.1.0.bias\n",
      "fs.dur_predictor.conv.1.2.weight\n",
      "fs.dur_predictor.conv.1.2.bias\n",
      "fs.dur_predictor.conv.2.0.weight\n",
      "fs.dur_predictor.conv.2.0.bias\n",
      "fs.dur_predictor.conv.2.2.weight\n",
      "fs.dur_predictor.conv.2.2.bias\n",
      "fs.dur_predictor.linear.0.weight\n",
      "fs.dur_predictor.linear.0.bias\n",
      "fs.pitch_embed.weight\n",
      "fs.pitch_predictor.conv.0.0.weight\n",
      "fs.pitch_predictor.conv.0.0.bias\n",
      "fs.pitch_predictor.conv.0.2.weight\n",
      "fs.pitch_predictor.conv.0.2.bias\n",
      "fs.pitch_predictor.conv.1.0.weight\n",
      "fs.pitch_predictor.conv.1.0.bias\n",
      "fs.pitch_predictor.conv.1.2.weight\n",
      "fs.pitch_predictor.conv.1.2.bias\n",
      "fs.pitch_predictor.conv.2.0.weight\n",
      "fs.pitch_predictor.conv.2.0.bias\n",
      "fs.pitch_predictor.conv.2.2.weight\n",
      "fs.pitch_predictor.conv.2.2.bias\n",
      "fs.pitch_predictor.conv.3.0.weight\n",
      "fs.pitch_predictor.conv.3.0.bias\n",
      "fs.pitch_predictor.conv.3.2.weight\n",
      "fs.pitch_predictor.conv.3.2.bias\n",
      "fs.pitch_predictor.conv.4.0.weight\n",
      "fs.pitch_predictor.conv.4.0.bias\n",
      "fs.pitch_predictor.conv.4.2.weight\n",
      "fs.pitch_predictor.conv.4.2.bias\n",
      "fs.pitch_predictor.linear.weight\n",
      "fs.pitch_predictor.linear.bias\n",
      "mel_encoder.encoder.0.weight\n",
      "mel_encoder.encoder.0.bias\n",
      "mel_encoder.encoder.2.weight\n",
      "mel_encoder.encoder.2.bias\n",
      "mel_encoder.fc_out.weight\n",
      "mel_encoder.fc_out.bias\n"
     ]
    }
   ],
   "source": [
    "for param in param_names:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering, e.g. the pitch_predictor, we have:\n",
    "```\n",
    "def __init__(self, idim, n_layers=5, n_chans=384, odim=2, kernel_size=5, dropout_rate=0.1):\n",
    "        super(PitchPredictor, self).__init__()\n",
    "        self.conv = torch.nn.ModuleList()\n",
    "        self.kernel_size = kernel_size\n",
    "        for idx in range(n_layers):\n",
    "            in_chans = idim if idx == 0 else n_chans\n",
    "            self.conv += [torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(in_chans, n_chans, kernel_size, padding=kernel_size // 2),\n",
    "                torch.nn.ReLU(),\n",
    "                LayerNorm(n_chans, dim=1),\n",
    "                torch.nn.Dropout(dropout_rate)\n",
    "            )]\n",
    "        self.linear = torch.nn.Linear(n_chans, odim)\n",
    "```\n",
    "So it makes sense to surmise that, in the list:\n",
    "\n",
    "fs.pitch_predictor.conv.0.0.weight\n",
    "\n",
    "fs.pitch_predictor.conv.0.0.bias\n",
    "\n",
    "fs.pitch_predictor.conv.0.2.weight\n",
    "\n",
    "fs.pitch_predictor.conv.0.2.bias\n",
    "\n",
    "0.0 is indicating the weights and bias for the Conv1d layer and 0.2 for the LayerNorm layer.\n",
    "\n",
    "To confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256, 5])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    if param[0]=='fs.pitch_predictor.conv.0.0.weight':\n",
    "        print(param[1].shape)\n",
    "    if param[0]=='fs.pitch_predictor.conv.0.0.bias':\n",
    "        print(param[1].shape)\n",
    "    if param[0]=='fs.pitch_predictor.conv.0.2.weight':\n",
    "        print(param[1].shape)\n",
    "    if param[0]=='fs.pitch_predictor.conv.0.2.bias':\n",
    "        print(param[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the conv1d documentation:\n",
    "\n",
    "Attributes: weight (Tensor): the learnable weights of the module of shape\n",
    "            $(\\text{out\\_channels},\n",
    "            \\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size})$.\n",
    "            The values of these weights are sampled from\n",
    "            $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where\n",
    "            $k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}$\n",
    "\n",
    "bias (Tensor):   the learnable bias of the module of shape\n",
    "            (out_channels). If :attr:`bias` is ``True``, then the values of these weights are\n",
    "            sampled from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where\n",
    "            $k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}$\n",
    "\n",
    "From the LayerNorm documentation: \n",
    "\n",
    "Attributes: weight: the learnable weights of the module of shape\n",
    "            $\\text{normalized\\_shape}$ when :attr:`elementwise_affine` is set to ``True``.\n",
    "            The values are initialized to 1.\n",
    "            \n",
    "bias:   the learnable bias of the module of shape\n",
    "                $\\text{normalized\\_shape}$ when :attr:`elementwise_affine` is set to ``True``.\n",
    "                The values are initialized to 0.\n",
    "\n",
    "So, this indeed seems to be their naming convention.\n",
    "\n",
    "Based on the AdaSpeech paper https://arxiv.org/pdf/2103.00993 (in particular section 2.2) and its implementation https://github.com/tuanh123789/AdaSpeech/tree/main  (see also figure 2 and section 2.3 of https://arxiv.org/pdf/2211.00585) it may be sufficient to fine-tune simply by retraining any LayerNorm parameters. AdaSpeech suggests fine-tuning these as a linear function of the speaker embedding, but if we are just fine-tuning on a single speaker, I don't see why we should make this restriction. We may also want to finetune the final linear output layers? And we should almost certainly fine tune the single linear layer spk_embed_proj which makes the utterance-level speaker projection.\n",
    "\n",
    "AdaSpeech uses FastSpeech2 as the model backbone. Our model also uses fastspeech2 as the model backbone for acoustic conditioning modeling. So here we share the same Transformer-based archicture. A difference, however, is that AdaSpeech also uses the mel-spectrogram decoder of fastspeech 2, which is composed of stacked feed-forward Transformer layers and thus has additional layer-norms which they fine tune. We instead have a Context-Aware Spectrogram Denoiser which, following DiffWave 2021, uses a non-causal WaveNet, whose mel spectrogram decoder consists of a 1x1 convolution layer and N convolution blocks with residual connections. If we used the Auxiliary Decoder + boundary predictor in addition to the spectrogram denoiser as in DiffSinger2022, we would also have these additional feed-forward Transformer layers and hence layer-norms to fine tune. In addition, we do not have the more fine-grained ''phoneme level acoustic conditioning modeling'' added to the adaspeech architecture, only utterance-level and speaker-level. Nonetheless, it is interesting to see if adaspeech's proposed fine tuning works if we are only fine tuning the acoustic conditioning modeling and not the spectrogram decoder (in adaspeech's ablation study they only consider with/without CLN and dont break it into components). Adding these other elements is an interesting avenue for future work. In particular, using fast speech's spectrogram decoder should only add in 4 more layer norms to fine tune, but changing the architecture of the model would involve training from scratch. We can also add back in FastSpeech2's energy predictor, which was removed in this model for some reason.\n",
    "\n",
    "It also probably makes sense to compare with the very simple \"fine tuning\" suggested by the code where one just adds a fixed value to the speaker projection for a given speaker-id. I'm guessing this isn't very effective, which is probably why they don't discuss its implementation anywhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffNet(\n",
      "  (input_projection): Conv1d(80, 256, kernel_size=(1,), stride=(1,))\n",
      "  (diffusion_embedding): SinusoidalPosEmb()\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): Mish()\n",
      "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (residual_layers): ModuleList(\n",
      "    (0-19): 20 x ResidualBlock(\n",
      "      (dilated_conv): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (diffusion_projection): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (conditioner_projection): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "      (output_projection): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (skip_projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "  (output_projection): Conv1d(256, 80, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "FastSpeech(\n",
      "  (encoder): TextConvEncoder(\n",
      "    (res_blocks): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): LambdaLayer()\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): LambdaLayer()\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ResidualBlock(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): LambdaLayer()\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ResidualBlock(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): LambdaLayer()\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (last_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_net1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (embed_tokens): Embedding(80, 256, padding_idx=0)\n",
      "  )\n",
      "  (decoder): ConvBlocks(\n",
      "    (res_blocks): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): LambdaLayer()\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): LambdaLayer()\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ResidualBlock(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): LambdaLayer()\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ResidualBlock(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): LambdaLayer()\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (last_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_net1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (mel_out): Linear(in_features=256, out_features=80, bias=True)\n",
      "  (spk_embed_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (dur_embed): Embedding(2000, 256, padding_idx=0)\n",
      "  (dur_predictor): DurationPredictor(\n",
      "    (conv): ModuleList(\n",
      "      (0-2): 3 x Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (1): ReLU()\n",
      "        (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    )\n",
      "  )\n",
      "  (length_regulator): LengthRegulator()\n",
      "  (pitch_embed): Embedding(300, 256, padding_idx=0)\n",
      "  (pitch_predictor): PitchPredictor(\n",
      "    (conv): ModuleList(\n",
      "      (0-4): 5 x Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (1): ReLU()\n",
      "        (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "MelEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=80, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for module in model.children():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are layer norms in the fastspeech encoder and decoder, the DurationPredictor, and the PitchPredictor. We can identify which of the weights correspond to each LayerNorm weight and bias and fine tune them (as well as the linear speaker embeding projection layer). We can probably use the existing training code, although it may be specific to the datasets they use, so we need to figure out how to format our new speaker dataset in the correct way.\n",
    "\n",
    "It may also be worth learning a bit more about what each of the 4 components which contain layer norms are actually doing,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder: (4 blocks, layer norm on each twice plus final layer norm = 9 layer norms, 18 parameters, size 256 each)\n",
    "fs.encoder.res_blocks.0.blocks.0.0.weight,\n",
    "fs.encoder.res_blocks.0.blocks.0.0.bias,\n",
    "fs.encoder.res_blocks.0.blocks.1.0.weight,\n",
    "fs.encoder.res_blocks.0.blocks.1.0.bias,\n",
    "fs.encoder.res_blocks.1.blocks.0.0.weight,\n",
    "fs.encoder.res_blocks.1.blocks.0.0.bias,\n",
    "fs.encoder.res_blocks.1.blocks.1.0.weight,\n",
    "fs.encoder.res_blocks.1.blocks.1.0.bias,\n",
    "fs.encoder.res_blocks.2.blocks.0.0.weight,\n",
    "fs.encoder.res_blocks.2.blocks.0.0.bias,\n",
    "fs.encoder.res_blocks.2.blocks.1.0.weight,\n",
    "fs.encoder.res_blocks.2.blocks.1.0.bias,\n",
    "fs.encoder.res_blocks.3.blocks.0.0.weight,\n",
    "fs.encoder.res_blocks.3.blocks.0.0.bias,\n",
    "fs.encoder.res_blocks.3.blocks.1.0.weight,\n",
    "fs.encoder.res_blocks.3.blocks.1.0.bias,\n",
    "fs.encoder.last_norm.weight,\n",
    "fs.encoder.last_norm.bias\n",
    "\n",
    "Decoder: (4 blocks, layer norm on each twice plus final layer norm = 9 layer norms, 18 parameters, size 256 each)\n",
    "fs.decoder.res_blocks.0.blocks.0.0.weight,\n",
    "fs.decoder.res_blocks.0.blocks.0.0.bias,\n",
    "fs.decoder.res_blocks.0.blocks.1.0.weight,\n",
    "fs.decoder.res_blocks.0.blocks.1.0.bias,\n",
    "fs.decoder.res_blocks.1.blocks.0.0.weight,\n",
    "fs.decoder.res_blocks.1.blocks.0.0.bias,\n",
    "fs.decoder.res_blocks.1.blocks.1.0.weight,\n",
    "fs.decoder.res_blocks.1.blocks.1.0.bias,\n",
    "fs.decoder.res_blocks.2.blocks.0.0.weight,\n",
    "fs.decoder.res_blocks.2.blocks.0.0.bias,\n",
    "fs.decoder.res_blocks.2.blocks.1.0.weight,\n",
    "fs.decoder.res_blocks.2.blocks.1.0.bias,\n",
    "fs.decoder.res_blocks.3.blocks.0.0.weight,\n",
    "fs.decoder.res_blocks.3.blocks.0.0.bias,\n",
    "fs.decoder.res_blocks.3.blocks.1.0.weight,\n",
    "fs.decoder.res_blocks.3.blocks.1.0.bias,\n",
    "fs.decoder.last_norm.weight,\n",
    "fs.decoder.last_norm.bias\n",
    "\n",
    "Duration Predictor:(3 layer norms, 6 parameters size 256 each)\n",
    "fs.dur_predictor.conv.0.2.weight,\n",
    "fs.dur_predictor.conv.0.2.bias,\n",
    "fs.dur_predictor.conv.1.2.weight,\n",
    "fs.dur_predictor.conv.1.2.bias,\n",
    "fs.dur_predictor.conv.2.2.weight,\n",
    "fs.dur_predictor.conv.2.2.bias\n",
    "\n",
    "Pitch Predictor:(5 layer norms, 10 parameters size 256 each)\n",
    "fs.pitch_predictor.conv.0.2.weight,\n",
    "fs.pitch_predictor.conv.0.2.bias,\n",
    "fs.pitch_predictor.conv.1.2.weight,\n",
    "fs.pitch_predictor.conv.1.2.bias,\n",
    "fs.pitch_predictor.conv.2.2.weight,\n",
    "fs.pitch_predictor.conv.2.2.bias,\n",
    "fs.pitch_predictor.conv.3.2.weight,\n",
    "fs.pitch_predictor.conv.3.2.bias,\n",
    "fs.pitch_predictor.conv.4.2.weight,\n",
    "fs.pitch_predictor.conv.4.2.bias\n",
    "\n",
    "\n",
    "Speaker embedding weight and bias (size 256 by 256 and 256 respectively):\n",
    "fs.spk_embed_proj.weight,\n",
    "fs.spk_embed_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    if param[0]=='fs.spk_embed_proj.weight':\n",
    "        print(param[1].shape)\n",
    "    if param[0]=='fs.spk_embed_proj.bias':\n",
    "        print(param[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer_norms = [\n",
    "    \"fs.encoder.res_blocks.0.blocks.0.0.weight\",\n",
    "    \"fs.encoder.res_blocks.0.blocks.0.0.bias\",\n",
    "    \"fs.encoder.res_blocks.0.blocks.1.0.weight\",\n",
    "    \"fs.encoder.res_blocks.0.blocks.1.0.bias\",\n",
    "    \"fs.encoder.res_blocks.1.blocks.0.0.weight\",\n",
    "    \"fs.encoder.res_blocks.1.blocks.0.0.bias\",\n",
    "    \"fs.encoder.res_blocks.1.blocks.1.0.weight\",\n",
    "    \"fs.encoder.res_blocks.1.blocks.1.0.bias\",\n",
    "    \"fs.encoder.res_blocks.2.blocks.0.0.weight\",\n",
    "    \"fs.encoder.res_blocks.2.blocks.0.0.bias\",\n",
    "    \"fs.encoder.res_blocks.2.blocks.1.0.weight\",\n",
    "    \"fs.encoder.res_blocks.2.blocks.1.0.bias\",\n",
    "    \"fs.encoder.res_blocks.3.blocks.0.0.weight\",\n",
    "    \"fs.encoder.res_blocks.3.blocks.0.0.bias\",\n",
    "    \"fs.encoder.res_blocks.3.blocks.1.0.weight\",\n",
    "    \"fs.encoder.res_blocks.3.blocks.1.0.bias\",\n",
    "    \"fs.encoder.last_norm.weight\",\n",
    "    \"fs.encoder.last_norm.bias\"\n",
    "]\n",
    "\n",
    "decoder_layer_norms = [\n",
    "    \"fs.decoder.res_blocks.0.blocks.0.0.weight\",\n",
    "    \"fs.decoder.res_blocks.0.blocks.0.0.bias\",\n",
    "    \"fs.decoder.res_blocks.0.blocks.1.0.weight\",\n",
    "    \"fs.decoder.res_blocks.0.blocks.1.0.bias\",\n",
    "    \"fs.decoder.res_blocks.1.blocks.0.0.weight\",\n",
    "    \"fs.decoder.res_blocks.1.blocks.0.0.bias\",\n",
    "    \"fs.decoder.res_blocks.1.blocks.1.0.weight\",\n",
    "    \"fs.decoder.res_blocks.1.blocks.1.0.bias\",\n",
    "    \"fs.decoder.res_blocks.2.blocks.0.0.weight\",\n",
    "    \"fs.decoder.res_blocks.2.blocks.0.0.bias\",\n",
    "    \"fs.decoder.res_blocks.2.blocks.1.0.weight\",\n",
    "    \"fs.decoder.res_blocks.2.blocks.1.0.bias\",\n",
    "    \"fs.decoder.res_blocks.3.blocks.0.0.weight\",\n",
    "    \"fs.decoder.res_blocks.3.blocks.0.0.bias\",\n",
    "    \"fs.decoder.res_blocks.3.blocks.1.0.weight\",\n",
    "    \"fs.decoder.res_blocks.3.blocks.1.0.bias\",\n",
    "    \"fs.decoder.last_norm.weight\",\n",
    "    \"fs.decoder.last_norm.bias\"\n",
    "]\n",
    "\n",
    "dur_predictor_layer_norms = [\n",
    "    \"fs.dur_predictor.conv.0.2.weight\",\n",
    "    \"fs.dur_predictor.conv.0.2.bias\",\n",
    "    \"fs.dur_predictor.conv.1.2.weight\",\n",
    "    \"fs.dur_predictor.conv.1.2.bias\",\n",
    "    \"fs.dur_predictor.conv.2.2.weight\",\n",
    "    \"fs.dur_predictor.conv.2.2.bias\"\n",
    "]\n",
    "\n",
    "pitch_predictor_layer_norms = [\n",
    "    \"fs.pitch_predictor.conv.0.2.weight\",\n",
    "    \"fs.pitch_predictor.conv.0.2.bias\",\n",
    "    \"fs.pitch_predictor.conv.1.2.weight\",\n",
    "    \"fs.pitch_predictor.conv.1.2.bias\",\n",
    "    \"fs.pitch_predictor.conv.2.2.weight\",\n",
    "    \"fs.pitch_predictor.conv.2.2.bias\",\n",
    "    \"fs.pitch_predictor.conv.3.2.weight\",\n",
    "    \"fs.pitch_predictor.conv.3.2.bias\",\n",
    "    \"fs.pitch_predictor.conv.4.2.weight\",\n",
    "    \"fs.pitch_predictor.conv.4.2.bias\"\n",
    "]\n",
    "\n",
    "speaker_embedding_weights = [\n",
    "    \"fs.spk_embed_proj.weight\",\n",
    "    \"fs.spk_embed_proj.bias\"\n",
    "]\n",
    "\n",
    "all_weights_to_fine_tune_names=encoder_layer_norms+decoder_layer_norms+dur_predictor_layer_norms+pitch_predictor_layer_norms+speaker_embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fs.encoder.res_blocks.0.blocks.0.0.weight\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.0.blocks.0.0.bias\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.0.blocks.1.0.weight\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.0.blocks.1.0.bias\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.1.blocks.0.0.weight\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.1.blocks.0.0.bias\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.1.blocks.1.0.weight\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.1.blocks.1.0.bias\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.2.blocks.0.0.weight\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.2.blocks.0.0.bias\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.2.blocks.1.0.weight\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.2.blocks.1.0.bias\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.3.blocks.0.0.weight\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.3.blocks.0.0.bias\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.3.blocks.1.0.weight\n",
      "torch.Size([256])\n",
      "fs.encoder.res_blocks.3.blocks.1.0.bias\n",
      "torch.Size([256])\n",
      "fs.encoder.last_norm.weight\n",
      "torch.Size([256])\n",
      "fs.encoder.last_norm.bias\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.0.blocks.0.0.weight\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.0.blocks.0.0.bias\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.0.blocks.1.0.weight\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.0.blocks.1.0.bias\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.1.blocks.0.0.weight\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.1.blocks.0.0.bias\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.1.blocks.1.0.weight\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.1.blocks.1.0.bias\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.2.blocks.0.0.weight\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.2.blocks.0.0.bias\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.2.blocks.1.0.weight\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.2.blocks.1.0.bias\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.3.blocks.0.0.weight\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.3.blocks.0.0.bias\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.3.blocks.1.0.weight\n",
      "torch.Size([256])\n",
      "fs.decoder.res_blocks.3.blocks.1.0.bias\n",
      "torch.Size([256])\n",
      "fs.decoder.last_norm.weight\n",
      "torch.Size([256])\n",
      "fs.decoder.last_norm.bias\n",
      "torch.Size([256])\n",
      "fs.spk_embed_proj.weight\n",
      "torch.Size([256, 256])\n",
      "fs.spk_embed_proj.bias\n",
      "torch.Size([256])\n",
      "fs.dur_predictor.conv.0.2.weight\n",
      "torch.Size([256])\n",
      "fs.dur_predictor.conv.0.2.bias\n",
      "torch.Size([256])\n",
      "fs.dur_predictor.conv.1.2.weight\n",
      "torch.Size([256])\n",
      "fs.dur_predictor.conv.1.2.bias\n",
      "torch.Size([256])\n",
      "fs.dur_predictor.conv.2.2.weight\n",
      "torch.Size([256])\n",
      "fs.dur_predictor.conv.2.2.bias\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.0.2.weight\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.0.2.bias\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.1.2.weight\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.1.2.bias\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.2.2.weight\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.2.2.bias\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.3.2.weight\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.3.2.bias\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.4.2.weight\n",
      "torch.Size([256])\n",
      "fs.pitch_predictor.conv.4.2.bias\n",
      "torch.Size([256])\n",
      "54\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "all_weights_to_fine_tune_weights=[]\n",
    "for param in model.named_parameters():\n",
    "    if param[0] in all_weights_to_fine_tune_names:\n",
    "        print(param[0])\n",
    "        print(param[1].size())\n",
    "        all_weights_to_fine_tune_weights.append(param[1])\n",
    "print(len(all_weights_to_fine_tune_weights))\n",
    "print(18+18+6+10+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fine tune $256*53+256*256=79104$ paramaters. There are 31560867 in the model, so we are only tuning about .25% of them. Even knowing we are not tuning the denoiser, which has 15086416 paramaters, we are only tuning .48% of the remaining parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31560867\n"
     ]
    }
   ],
   "source": [
    "total_num_params=0\n",
    "for param in model.named_parameters():\n",
    "    size_this_param=1\n",
    "    for dim_size in param[1].size():\n",
    "        size_this_param=size_this_param*dim_size\n",
    "    total_num_params+=size_this_param\n",
    "print(total_num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15086416\n"
     ]
    }
   ],
   "source": [
    "total_num_diff_params=0\n",
    "for param in model.named_parameters():\n",
    "    if param[0].startswith('denoise_fn'):\n",
    "        size_this_param=1\n",
    "        for dim_size in param[1].size():\n",
    "            size_this_param=size_this_param*dim_size\n",
    "        total_num_diff_params+=size_this_param\n",
    "print(total_num_diff_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire pipeline of the model goes as follows: \n",
    "\n",
    "wav, transcript, edited text, and edited word regions as input\n",
    "\n",
    "wav converted to mel spectrogram \n",
    "\n",
    "text and edited text is converted to CMU phonemes via vanilla g2p_en\n",
    "\n",
    "phoneme alignment on original audio performed with mfa or whisperx - we improved how short silences are handled. input is just wav. produces mel2ph\n",
    "\n",
    "f0 (fundamental frequency) over time and uv (unvoiced) markers over time on original audio determined via vanilla parselmouth. input is just wav\n",
    "utterance-level speaker embedding is determined via vanilla resemblyzer. input is just wav\n",
    "\n",
    "the edited phonemes (alone!) are passed to FastSpeechEncoder, each phoneme is assigned a 256 dimensional vector. Somehow must relate the phonemes to one another? The result is encoder_out\n",
    "\n",
    "The utterance-level speaker embedding (256) from resemblyzer (alone!) is sent to spk_embed_proj, which is a linear layer producing another 256-dimensional vector. If a speaker_id is used, a single fixed 256 dim vector is added to this. The result is style_embed\n",
    "\n",
    "dur_inp = style_embed+encoder_out. This and a masked mel2ph are sent to the forward duration predictor. the masking is determined by the edited text and edited word regions\n",
    "\n",
    "edited_mel2ph, edited_f0, edited_uv, and ref_mels are computed from the output of the duration predictor and from the edited word regions. We improve this computation using the new hyperparameter \"mask_loc_buffer\". edited_mel2ph is from sticking the predicted mel2ph from the duration predicter in between the two mel2ph ends. edited_f0, edited_uv, and ref_mels are just the original f0, uv, and mel with zeros put in the unknown region which will be edited\n",
    "\n",
    "edited_f0,edited_uv,style_embed, edited_mel2ph, and encoder_out are used sent to the pitch prediction model, and the result with encoder_out and edited_mel2ph is input into the decoder and then the linear \"mel out\" layer to determine decoder_inp. So for each mel bin, we have an associated f0,phoneme, and decoder output associated to that phoneme.\n",
    "\n",
    "decoder_inp is summed with the result of passing the ref_mels (alone) to the MelEncoder to get cond\n",
    "\n",
    "cond is passed to the spectrogram denoiser to get the inferred spectrogram for the edited region, called output\n",
    "\n",
    "Output is concatinated with the original mel spectrogram to get the full spectogram\n",
    "\n",
    "the result is passed to a vocoder to produce the resulting wav.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses are: The sample reconstruction loss (mean absolute error between true and inferred spectrogram), the structural similarity index (SSIM) loss arXiv:2202.13066 (another measure of similarity between true and inferred spectrogram), and reconstruction loss for pitch and duration predictor, which are just the l2 lost between the true and inferred phoneme duration and pitch over time. For the former the ground truth is as determined by MFA and for the latter by parselmouth. To train, they randomly mask 80% of the phoenemes in a given utterance and try to reconstruct them. It is unclear if any of the libritts dataset is set aside for validation in the paper, or if the entire thing is used for training. It seems looking at spec_denoiser.yaml that for validation maybe only 30% of phonemes are randomly masked, though, since infer_mask_ratio: 0.30.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following, we see that changing the hparam use_spk_id to true and trying to load are existing model, ''fs.spk_id_proj.weight'' is missing from the state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ckpt not found in checkpoints/spec_denoiser_spk_id.\n"
     ]
    }
   ],
   "source": [
    "hparams_spk_id=set_hparams(exp_name='spec_denoiser_spk_id')\n",
    "preprocessor_spk_id=BasePreprocessor()\n",
    "ph_encoder_spk_id,word_encoder_spk_id=preprocessor_spk_id.load_dict(binary_data_directory)\n",
    "model_spk_id = GaussianDiffusion(\n",
    "            phone_encoder=ph_encoder_spk_id,\n",
    "            out_dims=hparams['audio_num_mel_bins'], denoise_fn=DIFF_DECODERS[hparams['diff_decoder_type']](hparams),\n",
    "            timesteps=hparams['timesteps'], time_scale=hparams['timescale'],\n",
    "            loss_type=hparams['diff_loss_type'],\n",
    "            spec_min=hparams['spec_min'], spec_max=hparams['spec_max'],\n",
    "        )\n",
    "load_ckpt(model_spk_id, hparams_spk_id['work_dir'], 'model', force=False, strict=False)\n",
    "param_names_spk_id=[param[0] for param in model_spk_id.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n",
      "324\n",
      "['fs.spk_id_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "print(len(param_names))\n",
    "print(len(param_names_spk_id))\n",
    "new_params=[param for param in param_names_spk_id if param not in param_names]\n",
    "print(new_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine tuning, we use the BG3Narrator dataset that Francesca put together. It consists of 2874 utterances and one speaker. 574 utterances are set aside for validation. The baseline is the model (with our adjustments) as-is. The validation losses are:\n",
    "\n",
    "From naive fine tune initial validation results:\n",
    "{'total_loss': 2.0832, 'l1_coarse': 0.2393, 'ssim_coarse': 0.2545, 'pdur': 0.1128, 'wdur': 0.1465, 'uv': 1.1229, 'f0': 0.2072}\n",
    "\n",
    "From adafinetune initial validation results:\n",
    "{'total_loss': 2.3501, 'l1_coarse': 0.3107, 'ssim_coarse': 0.3218, 'pdur': 0.0941, 'wdur': 0.102, 'uv': 1.3279, 'f0': 0.1934}\n",
    "\n",
    "The ''best'' inferences can be found as events.out.tfevents.1723669676.DESKTOP-CRD2FGJ.21240.0\n",
    "\n",
    "For fine-tuning, we add the hparam --naive_fine_tune.\n",
    "\n",
    "This assumes you have a model already trained with use_spk_id: false, so that fs.spk_id_proj.weight is missing from the state dict. It then initializes the paramaters from that model, adds fs.spk_id_proj.weight to the state dict, and only trains those weights (using their same training method). \n",
    "\n",
    "modify config.yaml of the original model to use_spk_id: true and num_spk to the number of speakers you are fine-tuning on. Also modify the data directory hparams accordingly, as well as val_check_interval and valid_infer_interval to be how often you want to validate and save the model parameters and max_updates to adjust how long to fine-tune for. Then run \n",
    "\n",
    "python tasks/run.py --config checkpoints/spec_denoiser/config.yaml --exp_name spec_denoiser --naive_fine_tune\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of utterances:\n",
      "2178\n",
      "Mean number of phonemes per utterance (including silent):\n",
      "51.16988062442608\n",
      "Max number of ph in an utterance:\n",
      "230\n",
      "Min number of ph in an utterance:\n",
      "4\n",
      "Total number of utterances:\n",
      "542\n",
      "Mean number of phonemes per utterance (including silent):\n",
      "54.798892988929886\n",
      "Max number of ph in an utterance:\n",
      "210\n",
      "Min number of ph in an utterance:\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ph_lengths = np.load('C:\\\\Users\\\\bezem\\\\Documents\\\\erdos_deep_learning\\\\Speech-Editing-Toolkit-stable-unedited\\\\Speech-Editing-Toolkit-stable\\\\data\\\\binary\\\\NarratorBG3\\\\train_ph_lengths.npy')\n",
    "print('Total number of utterances:')\n",
    "print(len(ph_lengths))\n",
    "print('Mean number of phonemes per utterance (including silent):')\n",
    "print(np.mean(ph_lengths))\n",
    "print('Max number of ph in an utterance:')\n",
    "print(np.max(ph_lengths))\n",
    "print('Min number of ph in an utterance:')\n",
    "print(np.min(ph_lengths))\n",
    "val_ph_lengths = np.load('C:\\\\Users\\\\bezem\\\\Documents\\\\erdos_deep_learning\\\\Speech-Editing-Toolkit-stable-unedited\\\\Speech-Editing-Toolkit-stable\\\\data\\\\binary\\\\NarratorBG3\\\\valid_ph_lengths.npy')\n",
    "print('Total number of utterances:')\n",
    "print(len(val_ph_lengths))\n",
    "print('Mean number of phonemes per utterance (including silent):')\n",
    "print(np.mean(val_ph_lengths))\n",
    "print('Max number of ph in an utterance:')\n",
    "print(np.max(val_ph_lengths))\n",
    "print('Min number of ph in an utterance:')\n",
    "print(np.min(val_ph_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchaudio\n",
    "import os\n",
    "audio_files=[os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk('C:\\\\Users\\\\bezem\\\\Documents\\\\erdos_deep_learning\\\\Speech-Editing-Toolkit-stable-unedited\\Speech-Editing-Toolkit-stable\\\\data\\\\processed\\\\NarratorBG3\\\\wav_processed') for f in filenames] \n",
    "audio_lengths=[]\n",
    "for file in audio_files:\n",
    "    wav,rate=torchaudio.load(file)\n",
    "    audio_lengths.append(wav.shape[1]/rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean utterance length in seconds:\n",
      "4.255508420951308\n",
      "Max utterance length:\n",
      "20.57687074829932\n",
      "Min utterance length:\n",
      "0.2849886621315193\n"
     ]
    }
   ],
   "source": [
    "print('Mean utterance length in seconds:')\n",
    "print(np.mean(audio_lengths))\n",
    "print('Max utterance length:')\n",
    "print(np.max(audio_lengths))\n",
    "print('Min utterance length:')\n",
    "print(np.min(audio_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(\n",
    "            model_spk_id.parameters(),\n",
    "            lr=hparams['lr'],\n",
    "            betas=(hparams['optimizer_adam_beta1'], hparams['optimizer_adam_beta2']),\n",
    "            weight_decay=hparams['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in optimizer.param_groups:\n",
    "    for param in group:\n",
    "        if param[0] =='fs.spk_id_proj.weight':\n",
    "            print('Found!')\n",
    "#cannot find param from optimizer groups, names get lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found!\n"
     ]
    }
   ],
   "source": [
    "for param in model_spk_id.named_parameters():\n",
    "    if param[0] =='fs.spk_id_proj.weight':\n",
    "            print('Found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| load 'model' from 'C:\\Users\\bezem\\Documents\\erdos_deep_learning\\Speech-Editing-Toolkit-stable-unedited\\Speech-Editing-Toolkit-stable\\checkpoints\\spec_denoiser_naive_fine_tuning\\model_ckpt_steps_572000.ckpt'.\n"
     ]
    }
   ],
   "source": [
    "#a sanity check to make sure no other weights are being messed with \n",
    "model_spk_id_trained = GaussianDiffusion(\n",
    "            phone_encoder=ph_encoder_spk_id,\n",
    "            out_dims=hparams['audio_num_mel_bins'], denoise_fn=DIFF_DECODERS[hparams['diff_decoder_type']](hparams),\n",
    "            timesteps=hparams['timesteps'], time_scale=hparams['timescale'],\n",
    "            loss_type=hparams['diff_loss_type'],\n",
    "            spec_min=hparams['spec_min'], spec_max=hparams['spec_max'],\n",
    "        )\n",
    "load_ckpt(model_spk_id_trained, 'C:\\\\Users\\\\bezem\\\\Documents\\\\erdos_deep_learning\\\\Speech-Editing-Toolkit-stable-unedited\\\\Speech-Editing-Toolkit-stable\\\\checkpoints\\\\spec_denoiser_naive_fine_tuning', 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fs.spk_id_proj.weight\n"
     ]
    }
   ],
   "source": [
    "num_problem_params=0\n",
    "for param in model_spk_id_trained.named_parameters():\n",
    "    matching_params=[parameter for parameter in model.named_parameters() if param[0]==parameter[0]]\n",
    "    if len(matching_params)>1:\n",
    "        print('problem!')\n",
    "    elif len(matching_params)==1:\n",
    "        if any(value not in param[1] for value in matching_params[0][1]):\n",
    "            num_problem_params+=1\n",
    "            print('Problem!')\n",
    "            print(param[0])\n",
    "            print(matching_params[0][0])\n",
    "            print('Original:')\n",
    "            print(matching_params[0][1])\n",
    "            print('Changed:')\n",
    "            print(param[1])\n",
    "    else:\n",
    "        print(param[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the hparam --single_speaker_ada_fine_tune, which makes the same assumptions but also tunes all of the LayerNorm weights and biases identified above, as well as the speaker embedding weights and bias.\n",
    "\n",
    "modify config.yaml of the original model to use_spk_id: true (if you want to include a speaker-id paramater) and num_spk to the number of speakers you are fine-tuning on. Also modify the data directory hparams accordingly, as well as val_check_interval and valid_infer_interval to be how often you want to validate and save the model parameters and max_updates to adjust how long to fine-tune for. Then run \n",
    "\n",
    "python tasks/run.py --config checkpoints/spec_denoiser/config.yaml --exp_name spec_denoiser --single_speaker_ada_fine_tune\n",
    "\n",
    "Right now for both fine-tuning methods we save the whole model rather than just the changed weights for sanity checks - later can change to save only the changed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| load 'model' from 'C:\\Users\\bezem\\Documents\\erdos_deep_learning\\Speech-Editing-Toolkit-stable-unedited\\Speech-Editing-Toolkit-stable\\checkpoints\\spec_denoiser_ada_fine_tuning\\model_ckpt_steps_568250.ckpt'.\n"
     ]
    }
   ],
   "source": [
    "#a sanity check to make sure no other weights are being messed with \n",
    "model_spk_id_ada_trained = GaussianDiffusion(\n",
    "            phone_encoder=ph_encoder,\n",
    "            out_dims=hparams['audio_num_mel_bins'], denoise_fn=DIFF_DECODERS[hparams['diff_decoder_type']](hparams),\n",
    "            timesteps=hparams['timesteps'], time_scale=hparams['timescale'],\n",
    "            loss_type=hparams['diff_loss_type'],\n",
    "            spec_min=hparams['spec_min'], spec_max=hparams['spec_max'],\n",
    "        )\n",
    "load_ckpt(model_spk_id_ada_trained, 'C:\\\\Users\\\\bezem\\\\Documents\\\\erdos_deep_learning\\\\Speech-Editing-Toolkit-stable-unedited\\\\Speech-Editing-Toolkit-stable\\\\checkpoints\\\\spec_denoiser_ada_fine_tuning', 'model',strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_problem_params=0\n",
    "for param in model_spk_id_ada_trained.named_parameters():\n",
    "    matching_params=[parameter for parameter in model.named_parameters() if param[0]==parameter[0]]\n",
    "    if len(matching_params)>1:\n",
    "        print('problem!')\n",
    "    elif len(matching_params)==1:\n",
    "        if any(value not in param[1] for value in matching_params[0][1]) and (param[0] not in all_weights_to_fine_tune_names):\n",
    "            num_problem_params+=1\n",
    "            print('Problem!')\n",
    "            print(param[0])\n",
    "            print(matching_params[0][0])\n",
    "            print('Original:')\n",
    "            print(matching_params[0][1])\n",
    "            print('Changed:')\n",
    "            print(param[1])\n",
    "    else:\n",
    "        print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denoise_fn.input_projection.weight\n",
      "denoise_fn.input_projection.bias\n",
      "denoise_fn.mlp.0.weight\n",
      "denoise_fn.mlp.0.bias\n",
      "denoise_fn.mlp.2.weight\n",
      "denoise_fn.mlp.2.bias\n",
      "denoise_fn.residual_layers.0.dilated_conv.weight\n",
      "denoise_fn.residual_layers.0.dilated_conv.bias\n",
      "denoise_fn.residual_layers.0.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.0.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.0.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.0.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.0.output_projection.weight\n",
      "denoise_fn.residual_layers.0.output_projection.bias\n",
      "denoise_fn.residual_layers.1.dilated_conv.weight\n",
      "denoise_fn.residual_layers.1.dilated_conv.bias\n",
      "denoise_fn.residual_layers.1.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.1.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.1.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.1.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.1.output_projection.weight\n",
      "denoise_fn.residual_layers.1.output_projection.bias\n",
      "denoise_fn.residual_layers.2.dilated_conv.weight\n",
      "denoise_fn.residual_layers.2.dilated_conv.bias\n",
      "denoise_fn.residual_layers.2.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.2.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.2.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.2.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.2.output_projection.weight\n",
      "denoise_fn.residual_layers.2.output_projection.bias\n",
      "denoise_fn.residual_layers.3.dilated_conv.weight\n",
      "denoise_fn.residual_layers.3.dilated_conv.bias\n",
      "denoise_fn.residual_layers.3.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.3.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.3.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.3.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.3.output_projection.weight\n",
      "denoise_fn.residual_layers.3.output_projection.bias\n",
      "denoise_fn.residual_layers.4.dilated_conv.weight\n",
      "denoise_fn.residual_layers.4.dilated_conv.bias\n",
      "denoise_fn.residual_layers.4.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.4.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.4.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.4.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.4.output_projection.weight\n",
      "denoise_fn.residual_layers.4.output_projection.bias\n",
      "denoise_fn.residual_layers.5.dilated_conv.weight\n",
      "denoise_fn.residual_layers.5.dilated_conv.bias\n",
      "denoise_fn.residual_layers.5.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.5.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.5.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.5.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.5.output_projection.weight\n",
      "denoise_fn.residual_layers.5.output_projection.bias\n",
      "denoise_fn.residual_layers.6.dilated_conv.weight\n",
      "denoise_fn.residual_layers.6.dilated_conv.bias\n",
      "denoise_fn.residual_layers.6.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.6.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.6.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.6.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.6.output_projection.weight\n",
      "denoise_fn.residual_layers.6.output_projection.bias\n",
      "denoise_fn.residual_layers.7.dilated_conv.weight\n",
      "denoise_fn.residual_layers.7.dilated_conv.bias\n",
      "denoise_fn.residual_layers.7.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.7.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.7.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.7.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.7.output_projection.weight\n",
      "denoise_fn.residual_layers.7.output_projection.bias\n",
      "denoise_fn.residual_layers.8.dilated_conv.weight\n",
      "denoise_fn.residual_layers.8.dilated_conv.bias\n",
      "denoise_fn.residual_layers.8.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.8.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.8.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.8.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.8.output_projection.weight\n",
      "denoise_fn.residual_layers.8.output_projection.bias\n",
      "denoise_fn.residual_layers.9.dilated_conv.weight\n",
      "denoise_fn.residual_layers.9.dilated_conv.bias\n",
      "denoise_fn.residual_layers.9.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.9.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.9.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.9.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.9.output_projection.weight\n",
      "denoise_fn.residual_layers.9.output_projection.bias\n",
      "denoise_fn.residual_layers.10.dilated_conv.weight\n",
      "denoise_fn.residual_layers.10.dilated_conv.bias\n",
      "denoise_fn.residual_layers.10.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.10.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.10.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.10.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.10.output_projection.weight\n",
      "denoise_fn.residual_layers.10.output_projection.bias\n",
      "denoise_fn.residual_layers.11.dilated_conv.weight\n",
      "denoise_fn.residual_layers.11.dilated_conv.bias\n",
      "denoise_fn.residual_layers.11.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.11.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.11.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.11.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.11.output_projection.weight\n",
      "denoise_fn.residual_layers.11.output_projection.bias\n",
      "denoise_fn.residual_layers.12.dilated_conv.weight\n",
      "denoise_fn.residual_layers.12.dilated_conv.bias\n",
      "denoise_fn.residual_layers.12.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.12.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.12.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.12.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.12.output_projection.weight\n",
      "denoise_fn.residual_layers.12.output_projection.bias\n",
      "denoise_fn.residual_layers.13.dilated_conv.weight\n",
      "denoise_fn.residual_layers.13.dilated_conv.bias\n",
      "denoise_fn.residual_layers.13.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.13.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.13.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.13.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.13.output_projection.weight\n",
      "denoise_fn.residual_layers.13.output_projection.bias\n",
      "denoise_fn.residual_layers.14.dilated_conv.weight\n",
      "denoise_fn.residual_layers.14.dilated_conv.bias\n",
      "denoise_fn.residual_layers.14.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.14.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.14.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.14.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.14.output_projection.weight\n",
      "denoise_fn.residual_layers.14.output_projection.bias\n",
      "denoise_fn.residual_layers.15.dilated_conv.weight\n",
      "denoise_fn.residual_layers.15.dilated_conv.bias\n",
      "denoise_fn.residual_layers.15.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.15.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.15.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.15.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.15.output_projection.weight\n",
      "denoise_fn.residual_layers.15.output_projection.bias\n",
      "denoise_fn.residual_layers.16.dilated_conv.weight\n",
      "denoise_fn.residual_layers.16.dilated_conv.bias\n",
      "denoise_fn.residual_layers.16.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.16.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.16.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.16.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.16.output_projection.weight\n",
      "denoise_fn.residual_layers.16.output_projection.bias\n",
      "denoise_fn.residual_layers.17.dilated_conv.weight\n",
      "denoise_fn.residual_layers.17.dilated_conv.bias\n",
      "denoise_fn.residual_layers.17.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.17.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.17.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.17.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.17.output_projection.weight\n",
      "denoise_fn.residual_layers.17.output_projection.bias\n",
      "denoise_fn.residual_layers.18.dilated_conv.weight\n",
      "denoise_fn.residual_layers.18.dilated_conv.bias\n",
      "denoise_fn.residual_layers.18.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.18.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.18.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.18.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.18.output_projection.weight\n",
      "denoise_fn.residual_layers.18.output_projection.bias\n",
      "denoise_fn.residual_layers.19.dilated_conv.weight\n",
      "denoise_fn.residual_layers.19.dilated_conv.bias\n",
      "denoise_fn.residual_layers.19.diffusion_projection.weight\n",
      "denoise_fn.residual_layers.19.diffusion_projection.bias\n",
      "denoise_fn.residual_layers.19.conditioner_projection.weight\n",
      "denoise_fn.residual_layers.19.conditioner_projection.bias\n",
      "denoise_fn.residual_layers.19.output_projection.weight\n",
      "denoise_fn.residual_layers.19.output_projection.bias\n",
      "denoise_fn.skip_projection.weight\n",
      "denoise_fn.skip_projection.bias\n",
      "denoise_fn.output_projection.weight\n",
      "denoise_fn.output_projection.bias\n",
      "fs.encoder.res_blocks.0.blocks.0.0.weight\n",
      "fs.encoder.res_blocks.0.blocks.0.0.bias\n",
      "fs.encoder.res_blocks.0.blocks.0.1.weight\n",
      "fs.encoder.res_blocks.0.blocks.0.1.bias\n",
      "fs.encoder.res_blocks.0.blocks.0.4.weight\n",
      "fs.encoder.res_blocks.0.blocks.0.4.bias\n",
      "fs.encoder.res_blocks.0.blocks.1.0.weight\n",
      "fs.encoder.res_blocks.0.blocks.1.0.bias\n",
      "fs.encoder.res_blocks.0.blocks.1.1.weight\n",
      "fs.encoder.res_blocks.0.blocks.1.1.bias\n",
      "fs.encoder.res_blocks.0.blocks.1.4.weight\n",
      "fs.encoder.res_blocks.0.blocks.1.4.bias\n",
      "fs.encoder.res_blocks.1.blocks.0.0.weight\n",
      "fs.encoder.res_blocks.1.blocks.0.0.bias\n",
      "fs.encoder.res_blocks.1.blocks.0.1.weight\n",
      "fs.encoder.res_blocks.1.blocks.0.1.bias\n",
      "fs.encoder.res_blocks.1.blocks.0.4.weight\n",
      "fs.encoder.res_blocks.1.blocks.0.4.bias\n",
      "fs.encoder.res_blocks.1.blocks.1.0.weight\n",
      "fs.encoder.res_blocks.1.blocks.1.0.bias\n",
      "fs.encoder.res_blocks.1.blocks.1.1.weight\n",
      "fs.encoder.res_blocks.1.blocks.1.1.bias\n",
      "fs.encoder.res_blocks.1.blocks.1.4.weight\n",
      "fs.encoder.res_blocks.1.blocks.1.4.bias\n",
      "fs.encoder.res_blocks.2.blocks.0.0.weight\n",
      "fs.encoder.res_blocks.2.blocks.0.0.bias\n",
      "fs.encoder.res_blocks.2.blocks.0.1.weight\n",
      "fs.encoder.res_blocks.2.blocks.0.1.bias\n",
      "fs.encoder.res_blocks.2.blocks.0.4.weight\n",
      "fs.encoder.res_blocks.2.blocks.0.4.bias\n",
      "fs.encoder.res_blocks.2.blocks.1.0.weight\n",
      "fs.encoder.res_blocks.2.blocks.1.0.bias\n",
      "fs.encoder.res_blocks.2.blocks.1.1.weight\n",
      "fs.encoder.res_blocks.2.blocks.1.1.bias\n",
      "fs.encoder.res_blocks.2.blocks.1.4.weight\n",
      "fs.encoder.res_blocks.2.blocks.1.4.bias\n",
      "fs.encoder.res_blocks.3.blocks.0.0.weight\n",
      "fs.encoder.res_blocks.3.blocks.0.0.bias\n",
      "fs.encoder.res_blocks.3.blocks.0.1.weight\n",
      "fs.encoder.res_blocks.3.blocks.0.1.bias\n",
      "fs.encoder.res_blocks.3.blocks.0.4.weight\n",
      "fs.encoder.res_blocks.3.blocks.0.4.bias\n",
      "fs.encoder.res_blocks.3.blocks.1.0.weight\n",
      "fs.encoder.res_blocks.3.blocks.1.0.bias\n",
      "fs.encoder.res_blocks.3.blocks.1.1.weight\n",
      "fs.encoder.res_blocks.3.blocks.1.1.bias\n",
      "fs.encoder.res_blocks.3.blocks.1.4.weight\n",
      "fs.encoder.res_blocks.3.blocks.1.4.bias\n",
      "fs.encoder.last_norm.weight\n",
      "fs.encoder.last_norm.bias\n",
      "fs.encoder.post_net1.weight\n",
      "fs.encoder.post_net1.bias\n",
      "fs.encoder.embed_tokens.weight\n",
      "fs.decoder.res_blocks.0.blocks.0.0.weight\n",
      "fs.decoder.res_blocks.0.blocks.0.0.bias\n",
      "fs.decoder.res_blocks.0.blocks.0.1.weight\n",
      "fs.decoder.res_blocks.0.blocks.0.1.bias\n",
      "fs.decoder.res_blocks.0.blocks.0.4.weight\n",
      "fs.decoder.res_blocks.0.blocks.0.4.bias\n",
      "fs.decoder.res_blocks.0.blocks.1.0.weight\n",
      "fs.decoder.res_blocks.0.blocks.1.0.bias\n",
      "fs.decoder.res_blocks.0.blocks.1.1.weight\n",
      "fs.decoder.res_blocks.0.blocks.1.1.bias\n",
      "fs.decoder.res_blocks.0.blocks.1.4.weight\n",
      "fs.decoder.res_blocks.0.blocks.1.4.bias\n",
      "fs.decoder.res_blocks.1.blocks.0.0.weight\n",
      "fs.decoder.res_blocks.1.blocks.0.0.bias\n",
      "fs.decoder.res_blocks.1.blocks.0.1.weight\n",
      "fs.decoder.res_blocks.1.blocks.0.1.bias\n",
      "fs.decoder.res_blocks.1.blocks.0.4.weight\n",
      "fs.decoder.res_blocks.1.blocks.0.4.bias\n",
      "fs.decoder.res_blocks.1.blocks.1.0.weight\n",
      "fs.decoder.res_blocks.1.blocks.1.0.bias\n",
      "fs.decoder.res_blocks.1.blocks.1.1.weight\n",
      "fs.decoder.res_blocks.1.blocks.1.1.bias\n",
      "fs.decoder.res_blocks.1.blocks.1.4.weight\n",
      "fs.decoder.res_blocks.1.blocks.1.4.bias\n",
      "fs.decoder.res_blocks.2.blocks.0.0.weight\n",
      "fs.decoder.res_blocks.2.blocks.0.0.bias\n",
      "fs.decoder.res_blocks.2.blocks.0.1.weight\n",
      "fs.decoder.res_blocks.2.blocks.0.1.bias\n",
      "fs.decoder.res_blocks.2.blocks.0.4.weight\n",
      "fs.decoder.res_blocks.2.blocks.0.4.bias\n",
      "fs.decoder.res_blocks.2.blocks.1.0.weight\n",
      "fs.decoder.res_blocks.2.blocks.1.0.bias\n",
      "fs.decoder.res_blocks.2.blocks.1.1.weight\n",
      "fs.decoder.res_blocks.2.blocks.1.1.bias\n",
      "fs.decoder.res_blocks.2.blocks.1.4.weight\n",
      "fs.decoder.res_blocks.2.blocks.1.4.bias\n",
      "fs.decoder.res_blocks.3.blocks.0.0.weight\n",
      "fs.decoder.res_blocks.3.blocks.0.0.bias\n",
      "fs.decoder.res_blocks.3.blocks.0.1.weight\n",
      "fs.decoder.res_blocks.3.blocks.0.1.bias\n",
      "fs.decoder.res_blocks.3.blocks.0.4.weight\n",
      "fs.decoder.res_blocks.3.blocks.0.4.bias\n",
      "fs.decoder.res_blocks.3.blocks.1.0.weight\n",
      "fs.decoder.res_blocks.3.blocks.1.0.bias\n",
      "fs.decoder.res_blocks.3.blocks.1.1.weight\n",
      "fs.decoder.res_blocks.3.blocks.1.1.bias\n",
      "fs.decoder.res_blocks.3.blocks.1.4.weight\n",
      "fs.decoder.res_blocks.3.blocks.1.4.bias\n",
      "fs.decoder.last_norm.weight\n",
      "fs.decoder.last_norm.bias\n",
      "fs.decoder.post_net1.weight\n",
      "fs.decoder.post_net1.bias\n",
      "fs.mel_out.weight\n",
      "fs.mel_out.bias\n",
      "fs.spk_embed_proj.weight\n",
      "fs.spk_embed_proj.bias\n",
      "fs.dur_embed.weight\n",
      "fs.dur_predictor.conv.0.0.weight\n",
      "fs.dur_predictor.conv.0.0.bias\n",
      "fs.dur_predictor.conv.0.2.weight\n",
      "fs.dur_predictor.conv.0.2.bias\n",
      "fs.dur_predictor.conv.1.0.weight\n",
      "fs.dur_predictor.conv.1.0.bias\n",
      "fs.dur_predictor.conv.1.2.weight\n",
      "fs.dur_predictor.conv.1.2.bias\n",
      "fs.dur_predictor.conv.2.0.weight\n",
      "fs.dur_predictor.conv.2.0.bias\n",
      "fs.dur_predictor.conv.2.2.weight\n",
      "fs.dur_predictor.conv.2.2.bias\n",
      "fs.dur_predictor.linear.0.weight\n",
      "fs.dur_predictor.linear.0.bias\n",
      "fs.pitch_embed.weight\n",
      "fs.pitch_predictor.conv.0.0.weight\n",
      "fs.pitch_predictor.conv.0.0.bias\n",
      "fs.pitch_predictor.conv.0.2.weight\n",
      "fs.pitch_predictor.conv.0.2.bias\n",
      "fs.pitch_predictor.conv.1.0.weight\n",
      "fs.pitch_predictor.conv.1.0.bias\n",
      "fs.pitch_predictor.conv.1.2.weight\n",
      "fs.pitch_predictor.conv.1.2.bias\n",
      "fs.pitch_predictor.conv.2.0.weight\n",
      "fs.pitch_predictor.conv.2.0.bias\n",
      "fs.pitch_predictor.conv.2.2.weight\n",
      "fs.pitch_predictor.conv.2.2.bias\n",
      "fs.pitch_predictor.conv.3.0.weight\n",
      "fs.pitch_predictor.conv.3.0.bias\n",
      "fs.pitch_predictor.conv.3.2.weight\n",
      "fs.pitch_predictor.conv.3.2.bias\n",
      "fs.pitch_predictor.conv.4.0.weight\n",
      "fs.pitch_predictor.conv.4.0.bias\n",
      "fs.pitch_predictor.conv.4.2.weight\n",
      "fs.pitch_predictor.conv.4.2.bias\n",
      "fs.pitch_predictor.linear.weight\n",
      "fs.pitch_predictor.linear.bias\n",
      "mel_encoder.encoder.0.weight\n",
      "mel_encoder.encoder.0.bias\n",
      "mel_encoder.encoder.2.weight\n",
      "mel_encoder.encoder.2.bias\n",
      "mel_encoder.fc_out.weight\n",
      "mel_encoder.fc_out.bias\n"
     ]
    }
   ],
   "source": [
    "for param in model_spk_id_ada_trained.named_parameters():\n",
    "    print(param[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing how to load in only some of the weights\n",
    "model_spk_id_ada_trained = GaussianDiffusion(\n",
    "            phone_encoder=ph_encoder,\n",
    "            out_dims=hparams['audio_num_mel_bins'], denoise_fn=DIFF_DECODERS[hparams['diff_decoder_type']](hparams),\n",
    "            timesteps=hparams['timesteps'], time_scale=hparams['timescale'],\n",
    "            loss_type=hparams['diff_loss_type'],\n",
    "            spec_min=hparams['spec_min'], spec_max=hparams['spec_max'],\n",
    "        )\n",
    "from utils.commons.ckpt_utils import get_last_checkpoint, get_all_ckpts\n",
    "checkpoint, name = get_last_checkpoint('C:\\\\Users\\\\bezem\\\\Documents\\\\erdos_deep_learning\\\\Speech-Editing-Toolkit-stable-unedited\\\\Speech-Editing-Toolkit-stable\\\\checkpoints\\\\spec_denoiser_ada_fine_tuning', None)\n",
    "checkpoints, name = get_last_checkpoint('C:\\\\Users\\\\bezem\\\\Documents\\\\erdos_deep_learning\\\\Speech-Editing-Toolkit-stable-unedited\\\\Speech-Editing-Toolkit-stable\\\\checkpoints\\\\spec_denoiser_ada_fine_tuning', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also perform an abalation study to see which of these is contributing the most to the low validation loss for the adaspeech style fine tuning. See help for the new  fine_tune_weight_sel on how to recreate this. Possible combinations are:\n",
    "\n",
    "Without spk_id:\n",
    "\n",
    "e \n",
    "\n",
    "l \n",
    "\n",
    "p\n",
    "\n",
    "s\n",
    "\n",
    "el\n",
    "\n",
    "ep\n",
    "\n",
    "es\n",
    "\n",
    "lp\n",
    "\n",
    "ls\n",
    "\n",
    "ps\n",
    "\n",
    "elp\n",
    "\n",
    "els\n",
    "\n",
    "eps\n",
    "\n",
    "lps\n",
    "\n",
    "elps \n",
    "\n",
    "with spk_id:\n",
    "e\n",
    "\n",
    "l\n",
    "\n",
    "p\n",
    "\n",
    "s\n",
    "\n",
    "el\n",
    "\n",
    "ep\n",
    "\n",
    "es\n",
    "\n",
    "lp\n",
    "\n",
    "ls\n",
    "\n",
    "ps\n",
    "\n",
    "elp\n",
    "\n",
    "els\n",
    "\n",
    "eps\n",
    "\n",
    "lps\n",
    "\n",
    "elps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aligner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
