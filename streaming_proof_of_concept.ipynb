{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bezem\\miniconda3\\envs\\aligner\\lib\\site-packages\\pyannote.audio-3.1.1-py3.10.egg\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from streamreader import stream\n",
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import gc\n",
    "import numpy as np\n",
    "#from silero_vad import load_silero_vad, get_speech_timestamps\n",
    "from colorama import Fore, Back, Style\n",
    "import os\n",
    "from  scipy.io import wavfile \n",
    "CURR_DIR = os.getcwd()\n",
    "\n",
    "from inference.tts.spec_denoiser import SpecDenoiserInfer\n",
    "from utils.commons.hparams import set_hparams\n",
    "import torchaudio\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import pyaudio\n",
    "import sys\n",
    "\n",
    "    \n",
    "    \n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import cv2\n",
    "import pyrubberband as pyrb\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from g2p_en.expand import normalize_numbers\n",
    "\n",
    "import subprocess\n",
    "from yt_downloader import download_and_convert_from_yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the following cells, but not camera and mic cells for input via youtube video. Warning: The downloading and conversion process may take several minutes depending on the video. You do not need to rerun the second cell below if you are using the same video as previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_link='https://www.youtube.com/watch?v=3F_Ygt55UdU' #put a link to your video here. The full web address.\n",
    "#video_file_name='kennedy' #the saved audio will be video_file_name_audio.wav and the video will be video_file_name_video.mp4. No need to but a file extension.\n",
    "video_link='https://www.youtube.com/watch?v=3EEoVbr5ido'\n",
    "video_file_name='housewives'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3EEoVbr5ido\n",
      "[youtube] 3EEoVbr5ido: Downloading webpage\n",
      "[youtube] 3EEoVbr5ido: Downloading ios player API JSON\n",
      "[youtube] 3EEoVbr5ido: Downloading web creator player API JSON\n",
      "[youtube] 3EEoVbr5ido: Downloading m3u8 information\n",
      "[info] 3EEoVbr5ido: Downloading 1 format(s): 136+140\n",
      "[download] Destination: housewives_video_temp.f136.mp4\n",
      "[download] 100% of    8.02MiB in 00:00:02 at 2.75MiB/s   \n",
      "[download] Destination: housewives_video_temp.f140.m4a\n",
      "[download] 100% of    1.30MiB in 00:00:00 at 2.12MiB/s   \n",
      "[Merger] Merging formats into \"housewives_video_temp.mp4\"\n",
      "Deleting original file housewives_video_temp.f136.mp4 (pass -k to keep)\n",
      "Deleting original file housewives_video_temp.f140.m4a (pass -k to keep)\n",
      "[VideoConvertor] Not converting media file \"housewives_video_temp.mp4\"; already is in target format mp4\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3EEoVbr5ido\n",
      "[youtube] 3EEoVbr5ido: Downloading webpage\n",
      "[youtube] 3EEoVbr5ido: Downloading ios player API JSON\n",
      "[youtube] 3EEoVbr5ido: Downloading web creator player API JSON\n",
      "[youtube] 3EEoVbr5ido: Downloading m3u8 information\n",
      "[info] 3EEoVbr5ido: Downloading 1 format(s): 140\n",
      "[download] Destination: housewives_audio_temp\n",
      "[download] 100% of    1.30MiB in 00:00:00 at 2.38MiB/s   \n",
      "[FixupM4a] Correcting container of \"housewives_audio_temp\"\n",
      "[ExtractAudio] Destination: housewives_audio_temp.wav\n",
      "Deleting original file housewives_audio_temp (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "download_and_convert_from_yt(video_link,video_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also use another downloading and conversion process. The video file should be 30fps and the audio should be a wav with 16000 sample rate\n",
    "inp_device = None  # None for a file\n",
    "audio_src = video_file_name+'_audio.wav' #'Kennedy_Blades.wav' #file name or device ID\n",
    "play_video = True\n",
    "video_source = video_file_name+'_video.mp4'#'kennedybladesvideo.mp4'\n",
    "decrease_video_delay_frames=0 #this should only need to be used when using the microphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the following cells for input via camera and microphone. Replace audio_src with your desired input device's alternative name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dshow @ 0000019873D23540] \"NexiGo N60 FHD Webcam\" (video)\n",
      "\n",
      "[dshow @ 0000019873D23540]   Alternative name \"@device_pnp_\\\\?\\usb#vid_3443&pid_60bb&mi_00#6&10710fd5&0&0000#{65e8773d-8f56-11d0-a3b9-00a0c9223196}\\global\"\n",
      "\n",
      "[dshow @ 0000019873D23540] \"OBS Virtual Camera\" (none)\n",
      "\n",
      "[dshow @ 0000019873D23540]   Alternative name \"@device_sw_{860BB310-5D01-11D0-BD3B-00A0C911CE86}\\{A3FCE0F5-3493-419F-958A-ABA1250EC20B}\"\n",
      "\n",
      "[dshow @ 0000019873D23540] \"Microphone (4- USB Audio CODEC )\" (audio)\n",
      "\n",
      "[dshow @ 0000019873D23540]   Alternative name \"@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\\wave_{1AA5AD52-5585-40E4-B78C-6D510C065237}\"\n",
      "\n",
      "[dshow @ 0000019873D23540] \"Microphone (NexiGo N60 FHD Webcam Audio)\" (audio)\n",
      "\n",
      "[dshow @ 0000019873D23540]   Alternative name \"@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\\wave_{24B34041-81F3-4869-BB55-5EAB761760AB}\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Capture the output from both stdout and stderr\n",
    "popen = subprocess.Popen('ffmpeg -list_devices true -f dshow -i dummy', stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "# Wait for the process to complete\n",
    "popen.wait()\n",
    "\n",
    "# Read lines from stderr\n",
    "lines = popen.stderr.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if line.decode('utf-8').startswith('[dshow'):\n",
    "        print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_device= 'dshow'#for windows, use 'dshow' for microphone - see https://pytorch.org/audio/main/tutorials/device_asr.html\n",
    "#audio_src = \"audio=@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\\wave_{FC2FBEC3-13F8-4BC1-A4A7-B1B99AD72DAC}\", # Francesca \n",
    "audio_src=\"audio=@device_cm_{33D9A762-90C8-11D0-BD43-00A0C911CE86}\\wave_{1AA5AD52-5585-40E4-B78C-6D510C065237}\" # Zack\n",
    "play_video = True\n",
    "video_source = 0 #0 for camera\n",
    "decrease_video_delay_frames=42 #even though the delay time for the audio is fairly accurate, it is difficult to tell how much delay to add to the video because of hardware differences, ect. The user can increase this to decrease the number of frames the video is delayed by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User input and hyperparameters - Most people will only need to change the first of these. After running the correct cells above, you will run all of the cells after this to open the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whether to save all audio and the initial dictionary (can be updated in gui but takes a long time before will take effect)\n",
    "SAVE_ALL_AUDIO = False\n",
    "\n",
    "#do not put apostrophes or hyphens in the dictionary!\n",
    "#key_phrases_dict_orig = {'silver':'gold', 'second':'last', 'battle':'fight', 'um':'haha','work':'play','girls':'dogs','womens':'cats','role':'deep learning','gold':'bronze','medal':'donkey','wrestling':'speech editing competitions','watching':'smelling','watched':'smelled'}\n",
    "#key_phrases_dict_orig={'shirt': 'model', 'shirts':'models', 'pants': 'speech','pant':'speech','i am so tired': 'thank you','im so tired':'thank you'}\n",
    "key_phrases_dict_orig={'fitness':'home grown veggie','video':'dinner','videos':'dinners','reviews':'potatoes','produce':'cook','distribution':'delicious','deal':'meal','distributed':'served','producer':'chef','production':'greenhouse','computer':'donkeys','on that':'some leeks','bethany francos':'gordon ramsays','numbers':'carrots','body':'garden','number':'carrots','receipts':'recipes','companies':'restaurants'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user needs to set up these\n",
    "SAVE_AUDIO_DIR = CURR_DIR + \"\\\\saved_audio\"\n",
    "if not os.path.exists(SAVE_AUDIO_DIR):\n",
    "    os.mkdir(SAVE_AUDIO_DIR)\n",
    "# Set this to true if you want to save all segments of audio and not just the flagged ones\n",
    "SAVE_ALL_AUDIO = False\n",
    "binary_data_directory='.\\\\data\\\\processed\\\\binary\\\\libritts'\n",
    "Espeak_dll_directory = 'C:\\Program Files\\eSpeak NG\\libespeak-ng.dll'\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "EspeakWrapper.set_library(Espeak_dll_directory) \n",
    "whisperX_phoneme_model_directory='..\\\\whisperX-main\\\\facebook'\n",
    "whisperX_transcription_model_directory=None     \n",
    "our_model_ckpt_path='checkpoints/spec_denoiser/model_ckpt_steps_568000.ckpt'\n",
    "data_queue = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some more hyperparameters the user may want to set. Be careful with most of these\n",
    "DO_RUBBERBAND = True \n",
    "#USE_LOOPING_ITERATIONS=True\n",
    "#NUM_ITER=40\n",
    "silero_sensitivity = 0.7 #higher = more likely to detect silence\n",
    "req_num_pauses = 2\n",
    "min_segs_to_keep = 1\n",
    "req_end_long = True\n",
    "#not sure if these are correct (other than sample_rate)\n",
    "sample_rate = 16000\n",
    "segment_length = 15000\n",
    "SILERO_MIN_LENGTH_LONG_SILENCE = 80\n",
    "if video_source==0:\n",
    "    video_frame_rate=20\n",
    "else:\n",
    "    video_frame_rate=30\n",
    "#force the audio to go through even if not enough silences are formed if more than this many chunks are appended \n",
    "MAX_ALLOWED_CHUNKS=3\n",
    "#the max amount of time you expect it to take to transcribe and run inference\n",
    "if video_source==0:\n",
    "    processing_buffer=.1\n",
    "else:\n",
    "    processing_buffer=.1\n",
    "#the maximum amount of audio that can ever be waited for is 2*MAX_ALLOWED_CHUNKS*segment_length/sample_rate, so we should ensure a delay of this plus some small processing buffer\n",
    "desired_delay_time=2*MAX_ALLOWED_CHUNKS*segment_length/sample_rate+processing_buffer\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot mel with alignment tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/audio/main/generated/torchaudio.pipelines.Wav2Vec2ASRBundle.html\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "\n",
    "model_trellis = bundle.get_model().to(DEVICE)\n",
    "labels_trellis = bundle.get_labels()\n",
    "dictionary = {c: i for i, c in enumerate(labels_trellis)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel_with_align(waveform, transcript, output_path):\n",
    "\n",
    "    # Edit transcript into correct format\n",
    "    transcript=normalize_numbers(transcript)\n",
    "    transcript = transcript.upper()\n",
    "    for punc in ['.', '?', '!', '...', ',', ':', ';']:\n",
    "        transcript = transcript.replace(punc, \"\")\n",
    "    transcript = transcript.replace(\" \", \"|\")\n",
    "    if not transcript.startswith(\"|\"):\n",
    "        transcript = \"|\" + transcript\n",
    "    if not transcript.endswith(\"|\"):\n",
    "        transcript = transcript + \"|\"\n",
    "\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        emissions, _ = model_trellis(waveform.to(DEVICE))\n",
    "        emissions = torch.log_softmax(emissions, dim=-1)\n",
    "\n",
    "    emission = emissions[0].cpu().detach()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tokens = [dictionary.get(c,0) for c in transcript]    \n",
    "\n",
    "    def get_trellis(emission, tokens, blank_id=0):\n",
    "        num_frame = emission.size(0)\n",
    "        num_tokens = len(tokens)\n",
    "\n",
    "        trellis = torch.zeros((num_frame, num_tokens))\n",
    "        trellis[1:, 0] = torch.cumsum(emission[1:, blank_id], 0)\n",
    "        trellis[0, 1:] = -float(\"inf\")\n",
    "        trellis[-num_tokens + 1 :, 0] = float(\"inf\")\n",
    "\n",
    "        for t in range(num_frame - 1):\n",
    "            trellis[t + 1, 1:] = torch.maximum(\n",
    "                # Score for staying at the same token\n",
    "                trellis[t, 1:] + emission[t, blank_id],\n",
    "                # Score for changing to the next token\n",
    "                trellis[t, :-1] + emission[t, tokens[1:]],\n",
    "            )\n",
    "        return trellis\n",
    "\n",
    "\n",
    "    trellis = get_trellis(emission, tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @dataclass\n",
    "    class Point:\n",
    "        token_index: int\n",
    "        time_index: int\n",
    "        score: float\n",
    "\n",
    "\n",
    "    def backtrack(trellis, emission, tokens, blank_id=0):\n",
    "        t, j = trellis.size(0) - 1, trellis.size(1) - 1\n",
    "\n",
    "        path = [Point(j, t, emission[t, blank_id].exp().item())]\n",
    "        while j > 0:\n",
    "            # Should not happen but just in case\n",
    "            assert t > 0\n",
    "\n",
    "            # 1. Figure out if the current position was stay or change\n",
    "            # Frame-wise score of stay vs change\n",
    "            p_stay = emission[t - 1, blank_id]\n",
    "            p_change = emission[t - 1, tokens[j]]\n",
    "\n",
    "            # Context-aware score for stay vs change\n",
    "            stayed = trellis[t - 1, j] + p_stay\n",
    "            changed = trellis[t - 1, j - 1] + p_change\n",
    "\n",
    "            # Update position\n",
    "            t -= 1\n",
    "            if changed > stayed:\n",
    "                j -= 1\n",
    "\n",
    "            # Store the path with frame-wise probability.\n",
    "            prob = (p_change if changed > stayed else p_stay).exp().item()\n",
    "            path.append(Point(j, t, prob))\n",
    "        # Now j == 0, which means, it reached the SoS.\n",
    "        # Fill up the rest for the sake of visualization\n",
    "        while t > 0:\n",
    "            prob = emission[t - 1, blank_id].exp().item()\n",
    "            path.append(Point(j, t - 1, prob))\n",
    "            t -= 1\n",
    "\n",
    "        return path[::-1]\n",
    "\n",
    "    path = backtrack(trellis, emission, tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Merge the labels\n",
    "    @dataclass\n",
    "    class Segment:\n",
    "        label: str\n",
    "        start: int\n",
    "        end: int\n",
    "        score: float\n",
    "\n",
    "        def __repr__(self):\n",
    "            return f\"{self.label}\\t({self.score:4.2f}): [{self.start:5d}, {self.end:5d})\"\n",
    "\n",
    "        @property\n",
    "        def length(self):\n",
    "            return self.end - self.start\n",
    "\n",
    "    def merge_repeats(path):\n",
    "        i1, i2 = 0, 0\n",
    "        segments = []\n",
    "        while i1 < len(path):\n",
    "            while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "                i2 += 1\n",
    "            score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
    "            segments.append(\n",
    "                Segment(\n",
    "                    transcript[path[i1].token_index],\n",
    "                    path[i1].time_index,\n",
    "                    path[i2 - 1].time_index + 1,\n",
    "                    score,\n",
    "                )\n",
    "            )\n",
    "            i1 = i2\n",
    "        return segments\n",
    "    \n",
    "    segments = merge_repeats(path)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Merge words\n",
    "    def merge_words(segments, separator=\"|\"):\n",
    "        words = []\n",
    "        i1, i2 = 0, 0\n",
    "        while i1 < len(segments):\n",
    "            if i2 >= len(segments) or segments[i2].label == separator:\n",
    "                if i1 != i2:\n",
    "                    segs = segments[i1:i2]\n",
    "                    word = \"\".join([seg.label for seg in segs])\n",
    "                    score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
    "                    words.append(Segment(word, segments[i1].start, segments[i2 - 1].end, score))\n",
    "                i1 = i2 + 1\n",
    "                i2 = i1\n",
    "            else:\n",
    "                i2 += 1\n",
    "        return words\n",
    "\n",
    "    word_segments = merge_words(segments)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_alignments(trellis, segments, word_segments, waveform, sample_rate=bundle.sample_rate):\n",
    "        \n",
    "\n",
    "        fig2, ax2 = plt.subplots()\n",
    "\n",
    "        \n",
    "        # The original waveform\n",
    "        ratio = waveform.size(0) / sample_rate / trellis.size(0)\n",
    "        ax2.specgram(waveform, Fs=sample_rate)\n",
    "        for word in word_segments:\n",
    "            x0 = ratio * word.start\n",
    "            x1 = ratio * word.end\n",
    "            ax2.axvspan(x0, x1, facecolor=\"none\", edgecolor=\"white\", hatch=\"/\")\n",
    "            #ax2.annotate(f\"{word.score:.2f}\", (x0, sample_rate * 0.51), annotation_clip=False)\n",
    "        i=0\n",
    "        for seg in segments:\n",
    "            if seg.label != \"|\":\n",
    "                label_shift=i%4\n",
    "                if label_shift==2:\n",
    "                    label_shift=0\n",
    "                elif label_shift==3:\n",
    "                    label_shift=-1\n",
    "                i+=1\n",
    "                ax2.annotate(seg.label, (seg.start * ratio, sample_rate * (0.55+.02*label_shift)), annotation_clip=False)\n",
    "        ax2.set_xlabel(\"time [second]\")\n",
    "        ax2.set_yticks([])\n",
    "        fig2.tight_layout()\n",
    "        #plt.ioff()\n",
    "        fig2.savefig(output_path)\n",
    "        plt.close(fig2) \n",
    "\n",
    "\n",
    "    plot_alignments(\n",
    "        trellis,\n",
    "        segments,\n",
    "        word_segments,\n",
    "        waveform[0]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tkinter GUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GUIViewerApp:\n",
    "    def __init__(self, root):\n",
    "        #print(\"__init__ GUI\")\n",
    "        global key_phrases_dict_orig\n",
    "        self.root = root\n",
    "        self.root.title(\"Transcription and mel spectograms live viewer\")\n",
    "        \n",
    "\n",
    "        self.video_start = False\n",
    "        self.video_end = False\n",
    "        self.first_update = True\n",
    "        self.start_time = 0.0\n",
    "        self.video_start_time = 0.0\n",
    "        self.stream_delay = 0.0\n",
    "        self.update_dict_flag = False\n",
    "        self.audio_froze=False\n",
    "        self.first_video_frame=True\n",
    "        self.num_video_frames=1\n",
    "        self.initial_audio_recieved_time=0\n",
    "\n",
    "        # Event to stop threads\n",
    "        self.stop_event = threading.Event()\n",
    "\n",
    "        # Add a Close button\n",
    "        self.close_button = tk.Button(root, text=\"Close\", command=self.close_window)\n",
    "        self.close_button.pack(side='top')\n",
    "\n",
    "        # Pause/Resume button\n",
    "        self.is_paused = False\n",
    "        self.pause_button = tk.Button(root, text=\"Pause after current (background) loop\", command=self.toggle_pause_resume)\n",
    "        self.pause_button.pack()\n",
    "\n",
    "        # Label for transcript\n",
    "        self.transcript_label = tk.Label(root, text=\"...\", font=(\"Arial\", 14))\n",
    "        self.transcript_label.pack(pady=10)\n",
    "\n",
    "        # Canvas for spectrogram\n",
    "        self.spectrogram_canvas = tk.Label(root)\n",
    "        self.spectrogram_canvas.pack(side='left')\n",
    "\n",
    "        # Canvas for video\n",
    "        if play_video:\n",
    "            self.video_canvas = tk.Label(root)\n",
    "            self.video_canvas.pack(side='left')\n",
    "        else:\n",
    "            self.spectrogram_canvas.pack()\n",
    "\n",
    "        self.running = True\n",
    "        self.rows = []\n",
    "\n",
    "        # Create a frame to hold the labels and buttons (fixed part)\n",
    "        self.fixed_frame = tk.Frame(root)\n",
    "        self.fixed_frame.pack(side=\"top\", fill=\"x\", padx=10, pady=10)\n",
    "\n",
    "        # Buttons to add/remove rows\n",
    "        self.add_button = tk.Button(self.fixed_frame, text=\"Add Row\", command=self.add_row)\n",
    "        self.add_button.grid(row=1, column=0, padx=5, pady=10)\n",
    "\n",
    "        self.remove_button = tk.Button(self.fixed_frame, text=\"Remove Row\", command=self.remove_row)\n",
    "        self.remove_button.grid(row=1, column=1, padx=5, pady=10)\n",
    "\n",
    "        self.update_button = tk.Button(self.fixed_frame, text=\"Update Dictionary\", command=self.update_dict)\n",
    "        self.update_button.grid(row=1, column=2, padx=5, pady=10)\n",
    "\n",
    "        # Add labels for the two columns\n",
    "        self.label1 = tk.Label(self.fixed_frame, text=\"Original Phrase |\", font=(\"Arial\", 10, \"bold\"))\n",
    "        self.label2 = tk.Label(self.fixed_frame, text=\"| Replacement Phrase\", font=(\"Arial\", 10, \"bold\"))\n",
    "\n",
    "        self.label1.grid(row=0, column=0, padx=5, pady=5)\n",
    "        self.label2.grid(row=0, column=1, padx=5, pady=5)\n",
    "\n",
    "        # Create a canvas and a vertical scrollbar for scrolling the rows (scrollable part)\n",
    "        self.row_canvas = tk.Canvas(root, height=200, width=200)\n",
    "        self.scrollbar = tk.Scrollbar(root, orient=\"vertical\", command=self.row_canvas.yview)\n",
    "        self.row_frame = tk.Frame(self.row_canvas)\n",
    "\n",
    "        self.row_frame.bind(\n",
    "            \"<Configure>\",\n",
    "            lambda e: self.row_canvas.configure(scrollregion=self.row_canvas.bbox(\"all\"))\n",
    "        )\n",
    "\n",
    "        self.row_canvas.create_window((0, 0), window=self.row_frame, anchor=\"nw\")\n",
    "        self.row_canvas.configure(yscrollcommand=self.scrollbar.set)\n",
    "        self.row_canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        self.scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "        self.row_canvas.bind_all(\"<MouseWheel>\", self._on_mouse_wheel)\n",
    "\n",
    "        # Add rows based on the initial dictionary\n",
    "        for key, value in key_phrases_dict_orig.items():\n",
    "            self.add_row(key, value)\n",
    "\n",
    "        # Start threads\n",
    "        self.start_threads()\n",
    "\n",
    "    def resize_image(self,image,scale):\n",
    "        new_width = self.root.winfo_width()\n",
    "        old_width=image.width\n",
    "        old_height=image.height\n",
    "        ratio=old_height/old_width\n",
    "        new_height=new_width*ratio\n",
    "        self.vid_width=int(round(scale*new_width))\n",
    "        image = image.resize((self.vid_width,int(round(scale*new_height))), Image.Resampling.LANCZOS)\n",
    "        return image\n",
    "    \n",
    "    def toggle_pause_resume(self):\n",
    "        if self.is_paused:\n",
    "            self.pause_button.config(text=\"Pause after current (background) loop\")\n",
    "            self.is_paused = False\n",
    "        else:\n",
    "            self.pause_button.config(text=\"Resume (background) looping\")\n",
    "            self.is_paused = True\n",
    "            \n",
    "\n",
    "\n",
    "    def start_threads(self):\n",
    "        #print(\"GUI start_threads\")\n",
    "        # Start the main task in a separate thread\n",
    "        self.main_thread = threading.Thread(target=main_function, args=(audio_src, inp_device, self,), daemon=True)\n",
    "        self.main_thread.start()\n",
    "\n",
    "        # Start worker thread\n",
    "        self.worker_thread = threading.Thread(target=worker_task, args=(self,), daemon=True)\n",
    "        self.worker_thread.start()\n",
    "\n",
    "        if play_video:\n",
    "            self.video_thread = threading.Thread(target=video_task, args=(self,), daemon=True)\n",
    "            self.video_thread.start()\n",
    "            \n",
    "    def refresh_worker_threads(self):\n",
    "        self.worker_thread.join(timeout=.1)\n",
    "        #print(\"worker_thread joined\")\n",
    "        if play_video:\n",
    "            self.video_thread.join(timeout=.1)\n",
    "            #print(\"video_thread joined\")\n",
    "        self.worker_thread = threading.Thread(target=worker_task, args=(self,), daemon=True)\n",
    "        self.worker_thread.start()\n",
    "\n",
    "        if play_video:\n",
    "            self.video_thread = threading.Thread(target=video_task, args=(self,), daemon=True)\n",
    "            self.video_thread.start()\n",
    "        \n",
    "\n",
    "    def stop_threads(self):\n",
    "        #print(\"GUI stop_threads\")\n",
    "        # Signal threads to stop\n",
    "        self.stop_event.set()\n",
    "\n",
    "        # Join threads to ensure they have stopped\n",
    "        self.main_thread.join(timeout=2)\n",
    "        #print(\"main_thread joined\")\n",
    "        self.worker_thread.join(timeout=2)\n",
    "        #print(\"worker_thread joined\")\n",
    "        if play_video:\n",
    "            self.video_thread.join(timeout=2)\n",
    "            #print(\"video_thread joined\")\n",
    "\n",
    "      \n",
    "\n",
    "    def add_row(self, key=\"\", value=\"\"):\n",
    "        row = len(self.rows)\n",
    "        entry1 = tk.Entry(self.row_frame, width=20)\n",
    "        entry2 = tk.Entry(self.row_frame, width=20)\n",
    "\n",
    "        entry1.grid(row=row, column=0, padx=5, pady=5)\n",
    "        entry2.grid(row=row, column=1, padx=5, pady=5)\n",
    "\n",
    "        entry1.insert(0, key)\n",
    "        entry2.insert(0, value)\n",
    "\n",
    "        self.rows.append((entry1, entry2))\n",
    "\n",
    "        self.row_frame.update_idletasks()\n",
    "        self.row_canvas.config(scrollregion=self.row_canvas.bbox(\"all\"))\n",
    "\n",
    "    def remove_row(self):\n",
    "        if self.rows:\n",
    "            entry1, entry2 = self.rows.pop()\n",
    "            entry1.grid_forget()\n",
    "            entry2.grid_forget()\n",
    "\n",
    "            self.row_frame.update_idletasks()\n",
    "            self.row_canvas.config(scrollregion=self.row_canvas.bbox(\"all\"))\n",
    "\n",
    "    def update_dict(self):\n",
    "        self.update_dict_flag = True\n",
    "\n",
    "    def _on_mouse_wheel(self, event):\n",
    "        self.row_canvas.yview_scroll(int(-1 * (event.delta / 120)), \"units\")\n",
    "\n",
    "    def update_gui(self, transcript, mel_path, initial_audio_recieved_time=0):\n",
    "\n",
    "        if video_source==0:\n",
    "            self.transcript_label.config(text=f'{transcript}\\n transcript at: {round(time.time()-self.start_time,2)}s, delay from live = {round(self.stream_delay,3)}s')\n",
    "        else:\n",
    "            self.transcript_label.config(text=f'{transcript}\\n transcript at: {round(time.time()-self.start_time,2)}s, delay from live = {round(self.stream_delay+3*segment_length/sample_rate,3)}s')\n",
    "\n",
    "        #image = self.resize_image(Image.open(mel_path),.4)\n",
    "        image = Image.open(mel_path)\n",
    "        self.image_reference = ImageTk.PhotoImage(image)\n",
    "        self.spectrogram_canvas.config(image=self.image_reference)\n",
    "        self.spectrogram_canvas.image = self.image_reference\n",
    "\n",
    "        self.root.after(0, self.root.update)\n",
    "\n",
    "    def video_player(self, frame):\n",
    "        self.video_canvas.config(image=frame)\n",
    "        self.video_canvas.image = frame\n",
    "        self.root.after(0, self.root.frame)\n",
    "\n",
    "    \n",
    "    \n",
    "    def clear_queue(self):\n",
    "        #print(\"GUI clear_queue\")\n",
    "        try:\n",
    "            while not data_queue.empty():\n",
    "                data_queue.get_nowait()  # Remove and discard the item\n",
    "                data_queue.task_done()\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "\n",
    "    \n",
    "        \n",
    "    def close_window(self):\n",
    "        print(\"**** Closing the GUI ****\")\n",
    "        self.running = False\n",
    "        self.stop_threads()\n",
    "        self.clear_queue()\n",
    "        self.root.quit()\n",
    "        self.root.destroy()\n",
    "        try:\n",
    "            sys.exit(0)  # Exit the program completely (this will throw an ExitError, but it's innocuous)\n",
    "        except SystemExit:\n",
    "            pass\n",
    "\n",
    "    def no_op(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live speech editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\bezem/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "# Load the Silero VAD model and utilities\n",
    "silero_model, silero_utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False)\n",
    "(get_speech_ts, _, _, _, _) = silero_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split wav audio on silence using silero. Can get labels for speech/silence, and timestamps\n",
    "def split_on_silence_with_padding(audio_segment, silero_model,  min_silence_duration_ms = 500, sampling_rate = 16000):\n",
    "\n",
    "    wav = audio_segment\n",
    "\n",
    "    # Detect non-silent (speech) segments\n",
    "    speech_timestamps = get_speech_ts(wav, silero_model, sampling_rate = sampling_rate,  min_silence_duration_ms = 5,  window_size_samples = 512,threshold=silero_sensitivity)\n",
    "    \"\"\" \n",
    "    Other parameters that could be passed to Silero to improve things:\n",
    "    threshold: float = 0.5, \n",
    "    sampling_rate: int = 16000, \n",
    "    min_speech_duration_ms: int = 250, \n",
    "    min_silence_duration_ms: int = 100, \n",
    "    window_size_samples: int = 1536, \n",
    "    speech_pad_ms: int = 30, \n",
    "    return_seconds: bool = False, \n",
    "    visualize_probs: bool = False): \n",
    "    see https://github.com/snakers4/silero-vad/discussions/201\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_nonsilence = len(speech_timestamps)\n",
    "\n",
    "    if len(speech_timestamps) == 0:\n",
    "        return [number_of_nonsilence, [wav, 'silence', False, [0, len(wav)]]]\n",
    "    \n",
    "    # Prepare variables for splitting\n",
    "    chunks = []\n",
    "    prev_end = 0\n",
    "    threshold_samples = int((min_silence_duration_ms / 1000) * sampling_rate)\n",
    "    half_threshold_samples = threshold_samples // 2\n",
    "\n",
    "    for index, segment in enumerate(speech_timestamps):\n",
    "        curr_start = segment['start']\n",
    "        curr_end = segment['end']\n",
    "        flag_add_future_silence = False\n",
    "\n",
    "        # First let's deal with the endpoint of the segment\n",
    "        if (index < len(speech_timestamps) - 1) and (len(speech_timestamps) >= 2):\n",
    "            future_start = speech_timestamps[index+1]['start']        \n",
    "        else:\n",
    "            future_start = len(wav)\n",
    "        future_silence_duration_samples = future_start - curr_end\n",
    "        if future_silence_duration_samples > threshold_samples:\n",
    "            extra_future_silence_samples = future_silence_duration_samples - threshold_samples\n",
    "            temp_prev_end = curr_end + half_threshold_samples\n",
    "            temp_next_prev_end = temp_prev_end + extra_future_silence_samples\n",
    "            flag_add_future_silence = True\n",
    "        else:\n",
    "            temp_prev_end = (curr_end + future_start) // 2 # Take midpoint between current end and future start as padding\n",
    "   \n",
    "        # Now let's deal with the startpoint of the segment\n",
    "        silence_duration_samples = curr_start - prev_end\n",
    "        \n",
    "        # Apart from the very first segment, for all the other segments the distance from the previous segment is always less than threshold / 2\n",
    "        # because of the way we deal with the future endpoints\n",
    "        if silence_duration_samples > threshold_samples:\n",
    "            extra_silence_duration_samples = silence_duration_samples - threshold_samples\n",
    "\n",
    "            if extra_silence_duration_samples > 0:\n",
    "                # This is going to be a silent chunk\n",
    "                chunks.append([wav[prev_end: curr_start - half_threshold_samples], 'silence', False, [prev_end, curr_start - half_threshold_samples]])\n",
    "\n",
    "            # Now append current segment\n",
    "            chunks.append([wav[curr_start - half_threshold_samples: temp_prev_end], 'speech', flag_add_future_silence, [curr_start - half_threshold_samples, temp_prev_end]])\n",
    "            # set the new prev_end\n",
    "            prev_end = temp_prev_end\n",
    "            \n",
    "            if flag_add_future_silence:\n",
    "                # Add future silence\n",
    "                chunks.append([wav[temp_prev_end: temp_next_prev_end], 'silence', False, [temp_prev_end, temp_next_prev_end]])\n",
    "                # set the new prev_end \n",
    "                prev_end = temp_next_prev_end\n",
    "            \n",
    "        else:\n",
    "            chunks.append([wav[prev_end: temp_prev_end], 'speech', flag_add_future_silence, [prev_end, temp_prev_end]])\n",
    "            # set the new prev_end\n",
    "            prev_end = temp_prev_end\n",
    "            \n",
    "            if flag_add_future_silence:\n",
    "                # Add future silence\n",
    "                chunks.append([wav[temp_prev_end: temp_next_prev_end], 'silence', False, [temp_prev_end, temp_next_prev_end]])\n",
    "                # set the new prev_end \n",
    "                prev_end = temp_next_prev_end\n",
    "    \n",
    "    # Deal with the very last segment (which should be a silence)\n",
    "    if prev_end < len(wav):\n",
    "        chunks.append([wav[prev_end: ], 'last_segment_short_silence', False, [prev_end, len(wav)]])\n",
    "\n",
    "    return [number_of_nonsilence, chunks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all the various models separately from inference\n",
    "\n",
    "(Maybe put a button in the gui that does this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building whisperX transcription model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building our model...\n",
      "LOAD DIFFUSION MODEL TIME: 1.2777225971221924\n",
      "| load 'model_gen' from 'pretrained/hifigan_hifitts\\model_ckpt_steps_2168000.ckpt'.\n",
      "Build Vocoder Time 2.562967538833618\n",
      "Vocoder Device cuda\n",
      "Loaded the voice encoder model on cuda in 0.08 seconds.\n",
      "WHISPERX LOAD TIME = 6.229824542999268\n",
      "Warming up models...\n",
      "Mask loc buffer set to 10 frames, but there are only 14 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' This is a Librivox recording.',\n",
       " 'chunks': [{'text': ' This', 'timestamp': (0.0, 0.26)},\n",
       "  {'text': ' is', 'timestamp': (0.26, 1.52)},\n",
       "  {'text': ' a', 'timestamp': (1.52, 1.74)},\n",
       "  {'text': ' Librivox', 'timestamp': (1.74, 2.34)},\n",
       "  {'text': ' recording.', 'timestamp': (2.34, 2.76)}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Building whisperX transcription model...\")\n",
    "torch_dtype = torch.float16 if DEVICE=='cuda' else torch.float32\n",
    "#https://huggingface.co/models?pipeline_tag=text-to-speech&p=1&sort=trending\n",
    "model_id = \"distil-whisper/distil-large-v3\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "model_id, torch_dtype = torch_dtype, low_cpu_mem_usage = True, use_safetensors = True,cache_dir=whisperX_transcription_model_directory)\n",
    "model.to(DEVICE)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "whisper = pipeline(\n",
    "\"automatic-speech-recognition\",\n",
    "model = model,\n",
    "tokenizer = processor.tokenizer,\n",
    "feature_extractor = processor.feature_extractor,\n",
    "max_new_tokens = 128,\n",
    "torch_dtype = torch_dtype,\n",
    "device = DEVICE)\n",
    "\n",
    "\n",
    "hparams=set_hparams(exp_name ='spec_denoiser')\n",
    "\n",
    "print('Building our model...')\n",
    "our_model = SpecDenoiserInfer(hparams,binary_data_directory,our_model_ckpt_path,whisperX_phoneme_model_directory,device=DEVICE)\n",
    "\n",
    "print('Warming up models...')\n",
    "our_model.example_run([{'item_name':'','text':'this is a libri vox recording','edited_text':'this is a funny joke shows.','wav_fn_orig':'inference/audio_backup/1_space.wav','edited_region':'[4,6]','region':'[4,6]','mfa_textgrid':''}],use_MFA=False,use_librosa=False,save_wav_bool=False,disp_wav=False,mask_loc_buffer=10)\n",
    "sample_audio,rate= torchaudio.load('inference/audio_backup/1_space.wav')\n",
    "sample_audio = torchaudio.functional.resample(sample_audio, orig_freq=rate, new_freq=sample_rate)[0].squeeze()\n",
    "get_speech_ts(sample_audio, silero_model, sampling_rate = sample_rate,  min_silence_duration_ms = 5,  window_size_samples = 512,threshold=silero_sensitivity)\n",
    "whisper(sample_audio.to('cpu').numpy(),\n",
    "                chunk_length_s = 30,\n",
    "                stride_length_s = 5,\n",
    "                batch_size = 1,return_timestamps='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(inp_device, src, app):\n",
    "    global key_phrases_dict_orig\n",
    "    global key_phrases_dict\n",
    "    global key_phrases\n",
    "\n",
    "    \n",
    "    long_pause_punc_list = ['. ', '? ', '! ', '... ']\n",
    "    pause_punc_list = ['. ', '? ', '! ', '... ', ', ', ': ', '; ']\n",
    "    extended_pause_punc_list = pause_punc_list + [\" \"]\n",
    "    \n",
    "    # Dictionary of the phrases to be replaced (the key is the phrase to be replaced, while the value is the replacement)\n",
    "    # TODO: Initial uppercases/lowercases?\n",
    "\n",
    "    print(f'Segment Length in seconds: {segment_length/sample_rate}')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    @torch.no_grad\n",
    "    def infer(app):\n",
    "        global key_phrases_dict_orig\n",
    "        global key_phrases_dict\n",
    "        global key_phrases\n",
    "        import time\n",
    "        \n",
    "        # Counter for keeping track of saved wav files for playback (need to save separate wav files, otherwise permission errors)\n",
    "        COUNTER_FOR_SD = 0        \n",
    "        \n",
    "           \n",
    "        #how many key phrases have been found in total\n",
    "        num_key_phrase = 0\n",
    "\n",
    "\n",
    "        #will hold the current transcript\n",
    "        transcript = ''\n",
    "        cur_transcript = ''\n",
    "\n",
    "        count_num_audios_saved = 0\n",
    "\n",
    "        \n",
    "        transcription_inference_times=[0]\n",
    "        our_model_inference_times=[0]\n",
    "        total_iter_times = [] # This is not used here, so we can remove it.\n",
    "        num_its_before_transcription_locked = []\n",
    "        num_its_before_transcription_counter = 0\n",
    "        total_num_chunks = 0\n",
    "            \n",
    "        total_audio = np.empty([1])\n",
    "\n",
    "        CHUNKS_SO_FAR = torch.empty(0)\n",
    "        flag_start_timing_its_before_transcription = True\n",
    "        flag_at_least_one_transcription = False\n",
    "\n",
    "        #only used if looping audio. The loop will have length NUM_ITER*segment_length/sample_rate\n",
    "        #iteration=0\n",
    "\n",
    "        print(\"**** Start of streaming ****\")\n",
    "        print(\"Key phrases dict:\", key_phrases_dict_orig)\n",
    "\n",
    "        #if (not play_video) and USE_LOOPING_ITERATIONS and inp_device==None:\n",
    "        #    print(f'Will loop audio with loop length {NUM_ITER*segment_length/sample_rate}')\n",
    "\n",
    "        #while app.running and (iteration<NUM_ITER):\n",
    "        while app.running:\n",
    "            # Check periodically to exit the loop if stop_event is set\n",
    "            if app.stop_event.wait(timeout=0.0001):\n",
    "                break\n",
    "            \n",
    "            #if (not play_video) and USE_LOOPING_ITERATIONS and inp_device==None:\n",
    "                #iteration+=1\n",
    "\n",
    "            #get the current chunk of audio\n",
    "            if total_num_chunks==0 and video_source==0:\n",
    "                chunk,initial_audio_recieved_time = q.get()\n",
    "                #initial_audio_recieved_time=time.time()\n",
    "                initial_audio_recieved_time-=chunk.shape[0]/sample_rate\n",
    "            else:\n",
    "                chunk,_ = q.get()\n",
    "            \n",
    "\n",
    "            #for some reason on the first get we wait for several audio chunks to come through. If its the first run, just burn through to the end\n",
    "            #if i==0:\n",
    "            #    while not q.qsize()==0:\n",
    "            #        chunk,chunk_time=q.get()     \n",
    "            #    print(time.time()-chunk_time)  \n",
    "            if not video_source==0:\n",
    "                initial_audio_recieved_time=time.time()\n",
    "            \n",
    "            CHUNKS_SO_FAR = torch.cat((CHUNKS_SO_FAR, chunk[:,0]), 0)\n",
    "            num_its_before_transcription_counter  += 1\n",
    "            total_num_chunks+=1\n",
    "\n",
    "            os.makedirs(\"./non_silent_chunks\", exist_ok=True)   \n",
    "            \n",
    "            # Use silero to compute the number of speech bits in the CHUNKS_SO_FAR\n",
    "            number_of_nonsilence, split_by_silence_chunks_list = split_on_silence_with_padding(CHUNKS_SO_FAR, silero_model, SILERO_MIN_LENGTH_LONG_SILENCE, 16000)\n",
    "\n",
    "\n",
    "            # If there are at least two speech bits or if there is only a speech bit followed by a long silence, then lock the first speech bit for transcription with Whisper\n",
    "            flag_send_to_whisper = False\n",
    "            if number_of_nonsilence > 0:\n",
    "                # Find the first speech bit\n",
    "                flag_found_first_speech_segment = False\n",
    "                index_split_regions = 0\n",
    "                preceding_silence = torch.empty(0)\n",
    "                while not flag_found_first_speech_segment:\n",
    "                    if split_by_silence_chunks_list[index_split_regions][1] != 'speech':\n",
    "                        preceding_silence = torch.cat((preceding_silence, split_by_silence_chunks_list[index_split_regions][0]))\n",
    "                    if split_by_silence_chunks_list[index_split_regions][1] == 'speech':\n",
    "                        flag_found_first_speech_segment = True\n",
    "                        segment_to_send_to_whisper = split_by_silence_chunks_list[index_split_regions][0]\n",
    "                    index_split_regions += 1\n",
    "                # Check if the first speech bit can be sent to Whisper\n",
    "                # i.e. either there are at least two speech bits or \n",
    "                # there is a speech bit followed by a long silence\n",
    "                if (number_of_nonsilence > req_num_pauses) or (split_by_silence_chunks_list[index_split_regions-1][2] == True):\n",
    "                    flag_send_to_whisper = True\n",
    "                    # Reset CHUNKS_SO_FAR to the remaining regions after the first speech region\n",
    "                    CHUNKS_SO_FAR = torch.empty(0)\n",
    "                    while index_split_regions in range(len(split_by_silence_chunks_list)):\n",
    "                        CHUNKS_SO_FAR = torch.cat((CHUNKS_SO_FAR, split_by_silence_chunks_list[index_split_regions][0]), 0)\n",
    "                        index_split_regions += 1\n",
    "                elif num_its_before_transcription_counter>=MAX_ALLOWED_CHUNKS:\n",
    "                    flag_send_to_whisper = True\n",
    "                    # Reset CHUNKS_SO_FAR to the remaining regions after the first speech region\n",
    "                    CHUNKS_SO_FAR = torch.empty(0)\n",
    "                    while index_split_regions in range(len(split_by_silence_chunks_list)):\n",
    "                        CHUNKS_SO_FAR = torch.cat((CHUNKS_SO_FAR, split_by_silence_chunks_list[index_split_regions][0]), 0)\n",
    "                        index_split_regions += 1\n",
    "                    #print('Warning - Not enough pauses detected for optimal inference')\n",
    "                    \n",
    "\n",
    "            #this is the case of all silence\n",
    "            elif num_its_before_transcription_counter>=MAX_ALLOWED_CHUNKS:\n",
    "                segment_to_send_to_whisper=CHUNKS_SO_FAR\n",
    "                total_audio=np.concatenate((total_audio, CHUNKS_SO_FAR.numpy()),axis=0)\n",
    "                #######################################################################################################\n",
    "                wavfile.write(f'{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}.wav', 16000, CHUNKS_SO_FAR.numpy())\n",
    "                processed_segment_path = f'{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}.wav'\n",
    "                audio_segment, _ = torchaudio.load(processed_segment_path)\n",
    "                mel_path = f'{SAVE_AUDIO_DIR}/mel_flagged_{COUNTER_FOR_SD}.png'\n",
    "                plot_mel_with_align(audio_segment, cur_transcript, mel_path)\n",
    "                data_queue.put(['', mel_path, CHUNKS_SO_FAR.numpy(),initial_audio_recieved_time,0])\n",
    "                COUNTER_FOR_SD += 1\n",
    "                count_num_audios_saved +=1\n",
    "                num_its_before_transcription_counter=0\n",
    "                CHUNKS_SO_FAR = torch.empty(0)\n",
    "                \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            if flag_start_timing_its_before_transcription:\n",
    "                time_its_before_transcription = time.time()\n",
    "                flag_start_timing_its_before_transcription = False\n",
    "\n",
    "\n",
    "            # If the current segment can be passed to whisper for transcription:\n",
    "            if flag_send_to_whisper:\n",
    "                # Get the new dictionary with all the punctuated variations of keys and values\n",
    "                key_phrases_dict = generate_punctuated_key_phrases_dict(key_phrases_dict_orig, extended_pause_punc_list)\n",
    "                key_phrases = list(key_phrases_dict.keys())\n",
    "\n",
    "                def update_key_phrases_dict():\n",
    "                    global key_phrases_dict_orig\n",
    "                    global key_phrases_dict\n",
    "                    global key_phrases\n",
    "                    \n",
    "                    key_phrases_dict_orig.clear()  # Clear the dictionary before updating\n",
    "                    \n",
    "                    for entry1, entry2 in app.rows:\n",
    "                        key = entry1.get().strip().replace(\"'\",'').replace(\"-\",' ')\n",
    "                        value = entry2.get().strip().replace(\"'\",'').replace(\"-\",' ')\n",
    "                        if key:  # Only add non-empty keys\n",
    "                            key_phrases_dict_orig[key] = value                    \n",
    "                    \n",
    "                    key_phrases_dict = generate_punctuated_key_phrases_dict(key_phrases_dict_orig, extended_pause_punc_list)\n",
    "                    key_phrases = list(key_phrases_dict.keys())\n",
    "                    \n",
    "                    print(\"Updated dictionary:\", key_phrases_dict_orig)  # Print to console for demonstration                \n",
    "                \n",
    "                if app.update_dict_flag:\n",
    "                        update_key_phrases_dict()\n",
    "                        app.update_dict_flag=False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                flag_at_least_one_transcription = True\n",
    "                \n",
    "                num_its_before_transcription_locked.append(num_its_before_transcription_counter)\n",
    "                num_its_before_transcription_counter = 0\n",
    "                total_iter_times.append(time.time() - time_its_before_transcription)\n",
    "                flag_start_timing_its_before_transcription = True\n",
    "               \n",
    "\n",
    "          \n",
    "                # Pass the locked audiosegment to Whisper for transcription\n",
    "                transcription_inference_time_start=time.time()\n",
    "                transcript = whisper(segment_to_send_to_whisper.numpy(),\n",
    "                        chunk_length_s = 30,\n",
    "                        stride_length_s = 5,\n",
    "                        batch_size = 1,\n",
    "                        return_timestamps='word')                                     \n",
    "            \n",
    "                transcription_inference_times.append(time.time()-transcription_inference_time_start)\n",
    "                cur_transcript = ' '+' '.join(transcript['text'].lower().replace(\"'\",\"\").replace(\"-\",' ').split()) + ' '\n",
    "\n",
    "\n",
    "                # Look for flagged words in the transcript\n",
    "                if any([phrase in cur_transcript for phrase in key_phrases]):\n",
    "                    wavfile.write(f'{SAVE_AUDIO_DIR}/flagged_{num_key_phrase}.wav', sample_rate, segment_to_send_to_whisper.numpy())\n",
    "                    \n",
    "                    # Run our speech editing model\n",
    "                    our_model_inference_time_start=time.time()\n",
    "                    dataset_info = prep_inp_for_replacement_handle_multiple_key_phrases(cur_transcript, f'{SAVE_AUDIO_DIR}/flagged_{num_key_phrase}.wav')\n",
    "                    try:\n",
    "                        result_wavs = our_model.example_run(dataset_info, False, False, False, False, 5)\n",
    "                    except RuntimeError as e:\n",
    "                        print(e)\n",
    "                        print(cur_transcript)\n",
    "                        print(dataset_info)\n",
    "                        resampled_inferred_audio=segment_to_send_to_whisper\n",
    "\n",
    "                    our_model_inference_times.append(time.time()-our_model_inference_time_start)\n",
    "\n",
    "                    wavfile.write(f'{SAVE_AUDIO_DIR}/flagged_edited_{num_key_phrase}.wav', sample_rate, result_wavs[0][1].astype(np.float32))\n",
    "\n",
    "                    num_flagged_phrases = sum(x != 0 for x in [phrase in cur_transcript.lower() for phrase in key_phrases])\n",
    "                    num_key_phrase += num_flagged_phrases\n",
    "\n",
    "                    if SAVE_ALL_AUDIO:\n",
    "                        # Note: here the original sampling rate is 22050 because it's the output of our whisperX model (which works at that sampling rate)\n",
    "                        total_audio=np.concatenate((total_audio,\n",
    "                                                    preceding_silence.numpy(),\n",
    "                                                    torchaudio.functional.resample(torch.tensor(result_wavs[0][1]),orig_freq=22050,new_freq=sample_rate).to('cpu').numpy()),axis=0)\n",
    "\n",
    "                    #######################################################################################################   \n",
    "                    resampled_inferred_audio=torchaudio.functional.resample(torch.tensor(result_wavs[0][1]),orig_freq=22050,new_freq=sample_rate).to('cpu').numpy()                                \n",
    "                    wavfile.write(f'{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}_inferred.wav', sample_rate, resampled_inferred_audio.astype(np.float32))                                                              \n",
    "                    processed_segment_path = f'{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}_inferred.wav'\n",
    "                    audio_segment, _ = torchaudio.load(processed_segment_path)\n",
    "                    mel_path = f'{SAVE_AUDIO_DIR}/mel_flagged_{COUNTER_FOR_SD}.png'\n",
    "                    plot_mel_with_align(audio_segment, dataset_info[0]['edited_text'], mel_path)\n",
    "                    \n",
    "            \n",
    "                    desired_len_change=len(resampled_inferred_audio)-len(segment_to_send_to_whisper.numpy())\n",
    "\n",
    "                    \n",
    "                    data_queue.put([dataset_info[0]['edited_text_with_marked_words'], mel_path, np.concatenate((preceding_silence.numpy(), resampled_inferred_audio), 0),initial_audio_recieved_time,desired_len_change])                                \n",
    "                    COUNTER_FOR_SD += 1\n",
    "                    #######################################################################################################\n",
    "\n",
    "                    count_num_audios_saved +=1\n",
    "                \n",
    "                else: # No flagged words\n",
    "                    total_audio=np.concatenate((total_audio, \n",
    "                                                preceding_silence.numpy(),\n",
    "                                                segment_to_send_to_whisper.numpy()),axis=0)\n",
    "\n",
    "                    #######################################################################################################\n",
    "                    wavfile.write(f'{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}.wav', 16000, segment_to_send_to_whisper.numpy())\n",
    "                    processed_segment_path = f'{SAVE_AUDIO_DIR}/temp_{COUNTER_FOR_SD}.wav'\n",
    "                    audio_segment, _ = torchaudio.load(processed_segment_path)\n",
    "                    mel_path = f'{SAVE_AUDIO_DIR}/mel_flagged_{COUNTER_FOR_SD}.png'\n",
    "                    plot_mel_with_align(audio_segment, cur_transcript, mel_path)\n",
    "                    data_queue.put([cur_transcript, mel_path, np.concatenate((preceding_silence.numpy(), segment_to_send_to_whisper.numpy()), 0),initial_audio_recieved_time,0])\n",
    "                                                \n",
    "                    COUNTER_FOR_SD += 1\n",
    "                    #######################################################################################################\n",
    "\n",
    "                    count_num_audios_saved +=1      \n",
    "               \n",
    "\n",
    "               \n",
    "        if SAVE_ALL_AUDIO:\n",
    "            wavfile.write(f'{SAVE_AUDIO_DIR}/complete.wav', sample_rate, total_audio.astype(np.float32))\n",
    "\n",
    "        if not flag_at_least_one_transcription:\n",
    "            total_iter_times.append(time.time() - time_its_before_transcription)        \n",
    "\n",
    "        \n",
    "        print(f'Average Transcription Inference Time: {np.mean(transcription_inference_times)}. Maximum: {np.max(transcription_inference_times)} Transcription performed on {len(transcription_inference_times)-1} of {total_num_chunks} iterations')\n",
    "        print(f'Average Our Model Inference Time: {np.mean(our_model_inference_times)}. Maximum: {np.max(our_model_inference_times)} .Replacement performed on {len(our_model_inference_times)-1} of {total_num_chunks} iterations')\n",
    "        print(f'Average Total Iteration Time: {np.mean(total_iter_times)}. Maximum: {np.max(total_iter_times)}.')\n",
    "        try:\n",
    "            print(f'Average Number of Iterations before transcription: {np.mean(num_its_before_transcription_locked)}. Maximum: {np.max(num_its_before_transcription_locked)}. Number of locked segments: {len(num_its_before_transcription_locked)}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(f'Total Audio Shape: {np.shape(total_audio)}')\n",
    "\n",
    "        print(\"**** Iterations finished! ****\")\n",
    "        \n",
    "    def generate_punctuated_key_phrases_dict(key_phrases_dict, extended_pause_punc_list):\n",
    "        punct_key_phrases_dict = {}\n",
    "        key_phrases = list(key_phrases_dict.keys())\n",
    "        for phrase in key_phrases:\n",
    "            flag_starts_with_space = False\n",
    "            if not phrase.startswith(\" \"):\n",
    "                flag_starts_with_space = True\n",
    "                \n",
    "            for p in extended_pause_punc_list:\n",
    "                if not phrase.endswith(p):\n",
    "                    if not flag_starts_with_space:\n",
    "                        punct_key_phrases_dict[phrase + p] = key_phrases_dict[phrase] + p\n",
    "                    else:\n",
    "                        punct_key_phrases_dict[\" \" + phrase + p] = \" \" + key_phrases_dict[phrase] + p\n",
    "        return punct_key_phrases_dict\n",
    "\n",
    "\n",
    "    def prep_inp_for_replacement_handle_multiple_key_phrases(transcipt,file_name): #,silero_timestamps):\n",
    "        global key_phrases_dict\n",
    "        global key_phrases\n",
    "        #right now if there are multiple key phrases in the transcipt, this tells our model to infer those phrases and everything in between\n",
    "        #a smarter way to do this would be to make it so that our model supports editing multiple regions\n",
    "        dataset_info=[{}]\n",
    "        dataset_info[0]['item_name'] = ''  #this should just be used for naming the output file\n",
    "        for punc in pause_punc_list:\n",
    "            transcipt=transcipt.replace(punc.strip(),'')\n",
    "        transcript_words=transcipt.lower().split()\n",
    "        transcript_words=[word.replace(\"'\",\"\").replace(\"-\",' ') for word in transcript_words]\n",
    "        dataset_info[0]['text'] = ' '.join(transcript_words) #a transcription of the original text\n",
    "        \n",
    "        dataset_info[0]['wav_fn_orig'] = file_name #location of the .wav file to perform inference on         \n",
    "        key_phrase_words=[phrase.split() for phrase in key_phrases]\n",
    "        #right now if multiple key phrases exist in the transcript, we consider the last one appearing in key_phrases_dict as the one to be replaced, and replace the first occurance of it\n",
    "        num_ves = 0\n",
    "        phrase_info_dicts = []\n",
    "        change_in_region_length = 0\n",
    "        for phrase in key_phrase_words:\n",
    "            try:\n",
    "                word_reg_start=transcript_words.index(phrase[0])\n",
    "                word_reg_end=max(loc for loc, val in enumerate(transcript_words) if val == phrase[-1])\n",
    "                key_phrase=' '+' '.join(phrase)+' '\n",
    "                phrase_info_dicts.append({'phrase':key_phrase,'start':word_reg_start,'end':word_reg_end})\n",
    "                key = ' ' +' '.join(phrase)+ ' '\n",
    "                num_phrase_appearances=dataset_info[0]['text'].count(' '.join(phrase))\n",
    "                change_in_region_length+=num_phrase_appearances*(len(key_phrases_dict[key].split())-len(phrase))\n",
    "            except ValueError:\n",
    "                num_ves+=1\n",
    "        if num_ves>=len(key_phrase_words):\n",
    "            print('Error: Attempting to replace a word that does not exist in the transcript')\n",
    "            return 1          \n",
    "        dataset_info[0]['edited_text'] =' '+' '.join(transcript_words)+ ' '\n",
    "        dataset_info[0]['edited_text_with_marked_words'] = ' '+' '.join(transcript_words)+ ' '\n",
    "        for phrase in key_phrases:\n",
    "            replacement_phrase = key_phrases_dict[phrase]     \n",
    "            dataset_info[0]['edited_text_with_marked_words'] = dataset_info[0]['edited_text_with_marked_words'].replace(phrase, f\" |{replacement_phrase.strip()}| \")\n",
    "            dataset_info[0]['edited_text'] = dataset_info[0]['edited_text'].replace(phrase, replacement_phrase)        \n",
    "        for punc in pause_punc_list:\n",
    "            dataset_info[0]['edited_text']=dataset_info[0]['edited_text'].replace(punc.strip(),'')\n",
    "            dataset_info[0]['edited_text_with_marked_words'] = dataset_info[0]['edited_text_with_marked_words'].replace(punc.strip(),'')\n",
    "        dataset_info[0]['edited_text']=dataset_info[0]['edited_text'].strip()\n",
    "        dataset_info[0]['edited_text_with_marked_words'] = dataset_info[0]['edited_text_with_marked_words'].strip()\n",
    "        phrase_info_dicts= sorted(phrase_info_dicts, key=lambda d: d['start']) #this could potentially fail, it assumes that the transcript and phrases are all so that the first occurance of the first word in the phrase is contained in the full phrase\n",
    "        word_reg_start=phrase_info_dicts[0]['start']\n",
    "        word_reg_end=phrase_info_dicts[-1]['end']\n",
    "        dataset_info[0]['region'] = f'[{word_reg_start+1},{word_reg_end+1}]' #the region to edit (counting the words that will be changed starting from 1)\n",
    "        dataset_info[0]['edited_region'] = f'[{word_reg_start+1},{word_reg_end+1+change_in_region_length}]' #word counts in the full edited text of the region which is to be inferred starting from one \n",
    "        dataset_info[0]['mfa_textgrid'] = '' #we still need to set this to some value even if we are not using MFA    \n",
    "        \n",
    "\n",
    "        if not os.path.exists('./transcripts'):\n",
    "            os.makedirs('./transcripts')\n",
    "        import json\n",
    "        with open(f\"./transcripts/dataset_info.txt\", \"a\") as f:\n",
    "            f.write(json.dumps(dataset_info[0]))\n",
    "            f.write('\\n')\n",
    "        return dataset_info\n",
    "   \n",
    "\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    manager=ctx.Manager() #for some reason this fixes an issue I was having with multiprocessing https://discuss.pytorch.org/t/using-torch-tensor-over-multiprocessing-queue-process-fails/2847\n",
    "    q = manager.Queue()\n",
    "    p = ctx.Process(target = stream, args = (q, inp_device, src, segment_length, sample_rate,))\n",
    "\n",
    "    #print(\"p started\")\n",
    "    p.start()\n",
    "    infer(app)\n",
    "    #print(\"infer finished\")\n",
    "    p.join(timeout=1)\n",
    "    #print(\"p joined\")\n",
    "\n",
    "\n",
    "def main_function(audio_src,inp_device,app):\n",
    "    number_of_loops = 1\n",
    "\n",
    "    while not app.stop_event.is_set():\n",
    "        # Check if the GUI has been paused\n",
    "        if not app.is_paused:\n",
    "            print(\"****************\")\n",
    "            print(\"Loop number\", number_of_loops)\n",
    "            print(\"****************\")\n",
    "            gc.collect()\n",
    "            \n",
    "            data_queue = queue.Queue()\n",
    "            app.first_update=True\n",
    "            app.first_video_frame=True\n",
    "            app.num_video_frames=1\n",
    "            main(\n",
    "                src = audio_src,\n",
    "                inp_device = inp_device,\n",
    "                app=app     \n",
    "            )\n",
    "            \n",
    "            number_of_loops += 1\n",
    "        else:\n",
    "            time.sleep(0.0001)\n",
    "        \n",
    "        # Check periodically to exit the loop if stop_event is set\n",
    "        if app.stop_event.wait(timeout=0.1):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_task(app):\n",
    "\n",
    "    FORMAT = pyaudio.paFloat32\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "\n",
    "    pya = pyaudio.PyAudio()\n",
    "\n",
    "    py_stream = pya.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    output=True)\n",
    "    \n",
    "    app.first_update=True\n",
    "    app.audio_froze=False\n",
    "    while  not app.stop_event.is_set():\n",
    "        while not data_queue.empty():\n",
    "            transcript, mel_path, audio_chunk,initial_audio_recieved_time,desired_len_change = data_queue.get()\n",
    "            if app.first_update:\n",
    "                if (time.time()-initial_audio_recieved_time)<desired_delay_time:\n",
    "                    time.sleep(desired_delay_time-(time.time()-initial_audio_recieved_time))\n",
    "            if app.audio_froze:\n",
    "                desired_len_change+=int(round((time.time()-empty_queue_time_start)*sample_rate))\n",
    "                \n",
    "            #print(\"THREAD: TRANSCRIPT:\", transcript)\n",
    "            #app.root.after(10, app.update_gui, transcript, mel_path)\n",
    "            if app.running:\n",
    "\n",
    "                #if we had to freeze the app, rubberband the next audio\n",
    "                \n",
    "                # Check if the root window is still active\n",
    "                if app.root.winfo_exists():\n",
    "                    # Schedule the GUI update on the main thread\n",
    "                    try:\n",
    "                        #app.update_gui(transcript, mel_path, initial_audio_recieved_time)\n",
    "                        app.root.after(0, app.update_gui, transcript, mel_path, initial_audio_recieved_time)\n",
    "                    except RuntimeError:\n",
    "                    # The root window may have been destroyed in the meantime\n",
    "                        break\n",
    "\n",
    "\n",
    "                    \n",
    "                if desired_len_change!=0 and DO_RUBBERBAND:\n",
    "                    desired_new_len=len(audio_chunk)-desired_len_change\n",
    "                    rb_ratio=len(audio_chunk)/desired_new_len\n",
    "                    if rb_ratio<0:\n",
    "                        app.running=False\n",
    "                        print('Could not shorten the next segment enough to catch up: Terminating program')\n",
    "                        break\n",
    "                    else:\n",
    "                        audio_chunk=pyrb.time_stretch(audio_chunk, sample_rate, rb_ratio).astype(np.float32)\n",
    "\n",
    "                if app.first_update:\n",
    "                    app.initial_audio_recieved_time=initial_audio_recieved_time\n",
    "                    app.start_time = time.time()\n",
    "                    app.stream_delay = app.start_time - initial_audio_recieved_time\n",
    "                    app.first_update = False\n",
    "                    if play_video:\n",
    "                        app.video_start = True\n",
    "                py_stream.write(audio_chunk.tobytes())\n",
    "                #py_stream.write(audio_chunk)\n",
    "                data_queue.task_done()\n",
    "            else:\n",
    "                break \n",
    "        if not app.running:\n",
    "                break \n",
    "        if data_queue.empty() and not app.first_update:\n",
    "            empty_queue_time_start=time.time()\n",
    "            app.audio_froze=True\n",
    "            print('Audio froze!')\n",
    "        while data_queue.empty():\n",
    "            time.sleep(.0001)\n",
    "            if not app.running:\n",
    "                break \n",
    "        # Check periodically to exit the loop if stop_event is set\n",
    "        if app.stop_event.wait(timeout=0.1):\n",
    "            break\n",
    "\n",
    "                 \n",
    "    py_stream.stop_stream()\n",
    "    py_stream.close()\n",
    "    pya.terminate()\n",
    "\n",
    "    app.video_end=True        \n",
    "    print(\"If it didn't close automatically, please manually close the GUI\")\n",
    "    try:\n",
    "        while not data_queue.empty():\n",
    "            data_queue.get_nowait()  # Remove and discard the item\n",
    "            data_queue.task_done()   # Mark the task as done\n",
    "    except data_queue.empty():\n",
    "        pass  # If the queue is already empty, just pass\n",
    "    #app.root.after(1, app.root.destroy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_task(app):\n",
    "    global segment_length,sample_rate,video_frame_rate,decrease_video_delay_frames\n",
    "    # Create a VideoCapture object and read from input file\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False):\n",
    "        print(\"Error opening video file\")\n",
    "    # used to record the time when we processed last frame \n",
    "    \n",
    "    buffer=0\n",
    "    app.first_video_frame=True\n",
    "    app.num_video_frames=1\n",
    "    fps=0\n",
    "\n",
    "\n",
    "    if video_source==0:\n",
    "        delayed_rets=[]\n",
    "        delayed_frames=[]\n",
    "        new_frame_time=time.time()\n",
    "        intial_cap_time=new_frame_time\n",
    "        prev_frame_time=new_frame_time\n",
    "        num_delayed_frames=1\n",
    "\n",
    "\n",
    "    # Read until video is completed\n",
    "    while not app.stop_event.is_set():\n",
    "        while video_source==0 and not app.video_start:\n",
    "            ret, frame = cap.read()\n",
    "            new_frame_time = time.time()\n",
    "            while (new_frame_time-prev_frame_time +time.time()-intial_cap_time<num_delayed_frames/video_frame_rate+buffer):\n",
    "                new_frame_time = time.time()\n",
    "                cv2.waitKey(1)\n",
    "            delayed_rets.append(ret)\n",
    "            delayed_frames.append(frame)\n",
    "            prev_frame_time=new_frame_time\n",
    "            num_delayed_frames+=1\n",
    "          \n",
    "        if app.video_start:\n",
    "            if app.first_video_frame:\n",
    "                prev_frame_time=app.start_time\n",
    "                new_frame_time=app.start_time\n",
    "                app.first_video_frame=False\n",
    "                if video_source==0:\n",
    "                    delayed_rets=delayed_rets[-int(round(video_frame_rate*app.stream_delay)-decrease_video_delay_frames):]\n",
    "                    delayed_frames=delayed_frames[-int(round(video_frame_rate*app.stream_delay)-decrease_video_delay_frames):]\n",
    "\n",
    "                \n",
    "                \n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if video_source==0:\n",
    "                delayed_rets.append(ret)\n",
    "                delayed_frames.append(frame)\n",
    "                ret=delayed_rets.pop(0)\n",
    "                frame=delayed_frames.pop(0)\n",
    "            \n",
    "\n",
    "            \n",
    "            while (new_frame_time-prev_frame_time +time.time()-app.start_time<app.num_video_frames/video_frame_rate+buffer) and not app.video_end:\n",
    "                new_frame_time = time.time()\n",
    "                cv2.waitKey(1)\n",
    "                \n",
    "            \n",
    "            \n",
    "            # Capture frame-by-frame      \n",
    "            # if video finished or no Video Input \n",
    "            if not ret: \n",
    "                break    \n",
    "            # Our operations on the frame come here \n",
    "            gray = frame\n",
    "\n",
    "            # resizing the frame size according to our need \n",
    "            (h, w) = gray.shape[:2]\n",
    "\n",
    "            # Desired width\n",
    "            #new_width = app.vid_width\n",
    "\n",
    "            new_width = 640\n",
    "\n",
    "            # Calculate the aspect ratio\n",
    "            aspect_ratio = h / w\n",
    "            new_height = int(new_width * aspect_ratio)\n",
    "            gray = cv2.resize(gray, (new_width, new_height))     \n",
    "            # font which we will be using to display FPS \n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "            # time when we finish processing for this frame \n",
    "            #      \n",
    "            # Calculating the fps     \n",
    "            # fps will be number of frame processed in given time frame \n",
    "            # since their will be most of time error of 0.001 second \n",
    "            # we will be subtracting it to get more accurate result  \n",
    "            if new_frame_time!=prev_frame_time:\n",
    "                fps = 1/(.000001+new_frame_time-prev_frame_time) \n",
    "            # converting the fps into integer \n",
    "            fps = int(fps)     \n",
    "            # converting the fps to string so that we can display it on frame \n",
    "            # by using putText function \n",
    "            fps = str(fps)     \n",
    "            # putting the FPS count on the frame \n",
    "            cv2.putText(gray,'fps: '+ fps, (2, 30), font, .4, (100, 255, 0) , 1, cv2.LINE_AA)     \n",
    "            #put_total_time on the frame\n",
    "            cv2.putText(gray, 'time elapsed: '+str(round(time.time()-app.start_time,1))+'s', (2, 50), font, .4, (0, 0, 255), 1, cv2.LINE_AA)   \n",
    "            \n",
    "            \n",
    "            prev_frame_time =new_frame_time\n",
    "            app.num_video_frames+=1\n",
    "\n",
    "            #convert to forward for Tk\n",
    "            gray=cv2.cvtColor(gray, cv2.COLOR_BGR2RGB) \n",
    "            gray=Image.fromarray(gray)\n",
    "            gray = ImageTk.PhotoImage(gray)\n",
    "            \n",
    "\n",
    "            #send frame to the display\n",
    "            \n",
    "            app.video_player(gray)\n",
    "            \n",
    "\n",
    "        # Check periodically to exit the loop if stop_event is set\n",
    "        if app.stop_event.wait(timeout=0.0001):\n",
    "            break\n",
    "\n",
    "        if app.video_end or (not app.running):\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "    # When everything done, release\n",
    "    # the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************\n",
      "Loop number 1\n",
      "****************\n",
      "Segment Length in seconds: 0.9375\n",
      "**** Start of streaming ****\n",
      "Key phrases dict: {'shirt': 'model', 'shirts': 'models', 'pants': 'speech', 'pant': 'speech', 'i am so tired': 'thank you', 'im so tired': 'thank you'}\n",
      "Mask loc buffer set to 5 frames, but there are only 2 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "Mask loc buffer set to 5 frames, but there are only 2 frames of silence after the last edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "Mask loc buffer set to 5 frames, but there are only 8 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "Mask loc buffer set to 5 frames, but there are only 2 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "Mask loc buffer set to 5 frames, but there are only 4 frames of silence after the last edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the left side of the spec is not during an interval of silence.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "Mask loc buffer set to 5 frames, but there are only 2 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "Silent inferred phonemes predicted, changing word regions!\n",
      "Silent inferred phonemes predicted, changing word regions!\n",
      "Silent inferred phonemes predicted, changing word regions!\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "Mask loc buffer set to 5 frames, but there are only 8 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "Mask loc buffer set to 5 frames, but there are only 2 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "Silent inferred phonemes predicted, changing word regions!\n",
      "WARNING: The masking location for the left side of the spec is not during an interval of silence.\n",
      "Mask loc buffer set to 5 frames, but there are only 2 frames of silence after the last edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "Mask loc buffer set to 5 frames, but there are only 4 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "Mask loc buffer set to 5 frames, but there are only 6 frames of silence before the first edited word. Using silence midpoint instead.\n",
      "WARNING: The masking location for the right side of the spec is not during an interval of silence. This is not a problem if you are replacing the last word in the original audio.\n",
      "**** Closing the GUI ****\n",
      "Average Transcription Inference Time: 0.7462594677025164. Maximum: 0.9285459518432617 Transcription performed on 70 of 219 iterations\n",
      "Average Our Model Inference Time: 0.3960331757863363. Maximum: 0.5604002475738525 .Replacement performed on 14 of 219 iterations\n",
      "Average Total Iteration Time: 1.6580409356525967. Maximum: 37.62663793563843.\n",
      "Average Number of Iterations before transcription: 2.442857142857143. Maximum: 3. Number of locked segments: 70\n",
      "Total Audio Shape: (2734813,)\n",
      "**** Iterations finished! ****\n",
      "If it didn't close automatically, please manually close the GUI\n",
      "**** End of streaming ****\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Make sure the data_queue is empty\n",
    "    clear_output()\n",
    "    gc.collect()\n",
    "    tk_root = tk.Tk()\n",
    "    tk_app = GUIViewerApp(tk_root)\n",
    "    tk_root.mainloop()\n",
    "    print(\"**** End of streaming ****\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aligner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
